[["index.html", "Workshop 9: Multivariate Analyses in R QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 9: Multivariate Analyses in R QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2023-04-24 21:02:44 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by several QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best for the community Show courtesy and respect towards other community members 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors Originally developed by Bérenger Bourgeois, Xavier Giroux-Bougard, Amanda Winegardner, Emmanuelle Chrétien, Monica Granados and Pedro Henrique Pereira Braga. Material in the R script was originally derived from Borcard, Gillet &amp; Legendre (2011). Numerical Ecology with R. Springer New York. Since 2014, several QCBS members contributed to consistently and collaboratively develop and update this workshop, as part of the Learning and Development Award from the Québec Centre for Biodiversity Science. They were: 2025 - 2024 - 2023 2022 - 2021 - 2020 2019 - 2018 - 2017 2016 - 2015 - 2014 Pedro Henrique P. Braga Pedro Henrique P. Braga Gabriel Muñoz Monica Granados Katherine Hébert Marie Hélène-Brice Emmanuelle Chrétien Mi Lin Pedro Henrique P. Braga Bérenger Bourgeois Linley Sherin Amanda Winegardner Xavier Giroux-Bougard Vincent Fugère Zofia Taranu 0.3 Contributing Under construction. The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives Learn the basics of multivariate analysis to reveal patterns in community composition data Use R to perform an unconstrained ordination Learn about similarity and dissimilarity coefficients and transformations to perform multivariate analysis Use R to create dendrograms Learn the following methods: Clustering analysis Principal Component Analysis (PCA) Principal Coordinate Analysis (PCoA) Non-Metric MultiDimensional Scaling (NMDS) "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop To prepare for this workshop, you must do the following steps: Download the data required for this workshop: DoubsEnv data DoubsSpe data Their data can also be retrieved from the ade4 package: library(ade4) data(doubs) spe &lt;- doubs$fish env &lt;- doubs$env Alternatively, from the codep package: library(codep) data(Doubs) spe &lt;- Doubs.fish env &lt;- Doubs.env Download the script containing the coldiss() function: R script R script containing the coldiss() function You must also use these packages: ape ade4 codep gclus vegan GGally PlaneGeometry remotes MASS list.of.packages &lt;- c(&quot;ape&quot;, &quot;ade4&quot;, &quot;codep&quot;, &quot;gclus&quot;, &quot;vegan&quot;, &quot;GGally&quot;, &quot;PlaneGeometry&quot;, &quot;remotes&quot;, &quot;matlib&quot;, &quot;MASS&quot;) new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[, &quot;Package&quot;])] if (length(new.packages) &gt; 0) { install.packages(new.packages, dependencies = TRUE) print(paste0(&quot;The following package was installed:&quot;, new.packages)) } else if (length(new.packages) == 0) { print(&quot;All packages were already installed previously&quot;) } ## [1] &quot;The following package was installed:ape&quot; ## [2] &quot;The following package was installed:ade4&quot; ## [3] &quot;The following package was installed:codep&quot; ## [4] &quot;The following package was installed:gclus&quot; ## [5] &quot;The following package was installed:vegan&quot; ## [6] &quot;The following package was installed:GGally&quot; ## [7] &quot;The following package was installed:PlaneGeometry&quot; ## [8] &quot;The following package was installed:remotes&quot; ## [9] &quot;The following package was installed:matlib&quot; # Load all required libraries at once invisible(lapply(list.of.packages, library, character.only = TRUE, quietly = TRUE)) # source(file.choose()) # use coldiss.R which you have # downloaded to your own directory "],["recap-univariate-analyses.html", "Chapter 3 Recap: Univariate analyses", " Chapter 3 Recap: Univariate analyses We have learned a multitude of analyses that allowed us to interpret ecological data while depicting the effects of one or multiple variables in one response variable. We can recall the: General Linear Models, from which we used the functions: lm(); anova(); t.test(); lmer(). Generalized Linear Models, where we learned how to apply using: glm() and glmer() with several family() link functions. Generalized Additive Models, with the: gam() function. These models allowed us to ask questions such as: What are the effects of precipitation and temperature on species richness? How does the abundance of microbes change between hosts? Do co-occurring fish become more aggressive after being induced to fear? However, one may be interested in making inferences from ecological data containing more than one outcome or dependent variable. This interest may be driven by hypothesis testing and modelling, but also be entirely exploratory. "],["intro-multivariate-analyses.html", "Chapter 4 Intro: Multivariate analyses", " Chapter 4 Intro: Multivariate analyses For instance, our research question might be: How does the bacterial composition on maple leaves changes along the elevational gradient? What is the compositional dissimilarity of the bat communities? How closely-related spider local communities are in relation to their composition? In all these cases, the outcome is composed of several variables, e.g. usually a sample-by-species or sample-by-environment matrix. "],["setting-up-our-goals.html", "Chapter 5 Setting up our goals", " Chapter 5 Setting up our goals We will now dive into multivariate statistics, a tool set that will allow us to address questions requiring the simultaneous observation or analysis of more than one outcome variable. We will explore certain methods, such as: Association (or dis-similarity) measures and matrices; Classification (or cluster) analysis; Unconstrained ordination; Constrained (or canonical) ordination (in Workshop 10). Before everything, we will do a little review on matrix algebra. "],["matrix-algebra-very-briefly.html", "Chapter 6 Matrix algebra, very briefly 6.1 Data sets are matrices 6.2 Association matrices", " Chapter 6 Matrix algebra, very briefly Matrix algebra is well-suited for ecology, because most (if not all) data sets we work with are in a matrix format. 6.1 Data sets are matrices Ecological data tables are obtained as object-observations or sampling units, and are often recorded as this: Objects \\(y_1\\) \\(y_2\\) \\(\\dots\\) \\(y_n\\) \\(x_1\\) \\(y_{1,1}\\) \\(y_{1,2}\\) \\(\\dots\\) \\(y_{1,n}\\) \\(x_2\\) \\(y_{2,1}\\) \\(y_{2,2}\\) \\(\\dots\\) \\(y_{2,n}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\ddots\\) \\(\\vdots\\) \\(x_m\\) \\(y_{m,1}\\) \\(y_{m,2}\\) \\(\\dots\\) \\(y_{m,n}\\) where \\(x_m\\) is the sampling unit \\(m\\); and \\(y_n\\) is the ecological descripor that can be, for example, species present in a sampling unit, locality, or a chemical variable. The same ecological data table can be represented in matrix notation like this: \\[Y = [y_{m,n}] = \\begin{bmatrix} y_{1,1} &amp; y_{1,2} &amp; \\cdots &amp; y_{1,n} \\\\ y_{2,1} &amp; y_{2,2} &amp; \\cdots &amp; y_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ y_{m,1} &amp; y_{m,2} &amp; \\cdots &amp; y_{m,n} \\end{bmatrix}\\] where lowercase letters indicate elements, and the subscript letters indicate the position of these elements in the matrix (and in the table!). Moreover, any subset of a matrix can be recognized. We can subset a row matrix, as below: \\[\\begin{bmatrix} y_{1,1} &amp; y_{1,2} &amp; \\cdots &amp; y_{1,n} \\\\ \\end{bmatrix}\\] We can also subset a column matrix, as below: \\[\\begin{bmatrix} y_{1,1} \\\\ y_{2,2} \\\\ \\vdots \\\\ y_{m,2} \\end{bmatrix}\\] 6.2 Association matrices Two important matrices can be derived from the ecological data matrix: the association matrix among objects and the association matrix among descriptors. Using the data from our matrix \\(Y\\), \\[ Y = \\begin{array}{cc} \\begin{array}{ccc} x_1 \\rightarrow\\\\ x_2 \\rightarrow\\\\ \\vdots \\\\ x_m \\rightarrow\\\\ \\end{array} &amp; \\begin{bmatrix} y_{1,1} &amp; y_{1,2} &amp; \\cdots &amp; y_{1,n} \\\\ y_{2,1} &amp; y_{2,2} &amp; \\cdots &amp; y_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ y_{m,1} &amp; y_{m,2} &amp; \\cdots &amp; y_{m,n} \\end{bmatrix} \\end{array} \\] one can examine the relationship between the first two objects: \\[x_1 \\rightarrow \\begin{bmatrix} y_{1,1} &amp; y_{1,2} &amp; \\cdots &amp; y_{1,n} \\\\ \\end{bmatrix} \\] \\[x_2 \\rightarrow \\begin{bmatrix} y_{2,1} &amp; y_{2,2} &amp; \\cdots &amp; y_{2,n} \\\\ \\end{bmatrix} \\] and obtain \\(a_{1,2}\\). We can populate the association matrix \\(A_{n,n}\\) with the relationships between all objects from \\(Y\\): \\[A_{n,n} = \\begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \\cdots &amp; a_{1,n} \\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n,1} &amp; a_{n,2} &amp; \\cdots &amp; a_{n,n} \\end{bmatrix}\\] Because \\(A_{n,n}\\) has the same number of rows and columns, it is denoted a square matrix. Therefore, \\(A_{n,n}\\) has \\(n^2\\) elements. We can also obtain the relationship between the first two descriptors of \\(Y\\), \\(y_1\\) and \\(y_2\\): \\[\\begin{bmatrix} y_{1,2} \\\\ y_{2,2} \\\\ \\vdots \\\\ y_{m,2} \\end{bmatrix}\\] \\[\\begin{bmatrix} y_{1,1} \\\\ y_{2,1} \\\\ \\vdots \\\\ y_{m,1} \\end{bmatrix}\\] and store it in \\(a_{1,2}\\). We can populate the association matrix \\(A_{m,m}\\) with the relationships between all descriptors from \\(Y\\): \\[A_{m,m} = \\begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \\cdots &amp; a_{1,m} \\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m,1} &amp; a_{m,2} &amp; \\cdots &amp; a_{m,m} \\end{bmatrix}\\] This \\(A_{m,m}\\) is a square matrix, and it has \\(m^2\\) elements. These matrices, \\(A_{n,n}\\) and \\(A_{m,m}\\), are the basis of Q-mode and R-mode analyses in ecology. R-mode constitutes of analyzing the association between descriptors or species, while Q-mode analyzes the association between OTUs, objects or sites. "],["exploring-the-dataset.html", "Chapter 7 Exploring the dataset 7.1 Doubs river fish communities 7.2 Doubs river environmental data", " Chapter 7 Exploring the dataset We will use two main data sets in the first part of this workshop. They come from Verneaux’s PhD thesis (1973), where he proposed to use fish species to characterize ecological zones along European rivers and streams. He collected data at 30 localities along the Doubs river, which runs near the France-Switzerland border, in the Jura Mountains. He showed that fish communities were biological indicators of these water bodies. Their data is split in three matrices: The abundance of 27 fish species across the communities (DoubsSpe.csv and hereon, the spe object); The environmental variables recorded at each site (DoubsEnv.csv and hereon, the env object); and, The geographical coordinates of each site. Verneaux, J. (1973) Cours d’eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs. Essai de biotypologie. Thèse d’état, Besançon. 1–257. 7.1 Doubs river fish communities You can download these datasets from r.qcbs.ca/workshops/r-workshop-09. We can load their data from the data/ directory in this workshop: spe &lt;- read.csv(&quot;data/doubsspe.csv&quot;, row.names = 1) env &lt;- read.csv(&quot;data/doubsenv.csv&quot;, row.names = 1) Their data can also be retrieved from the ade4 package: library(ade4) data(doubs) spe &lt;- doubs$fish env &lt;- doubs$env Alternatively, from the codep package: library(codep) data(Doubs) spe &lt;- Doubs.fish env &lt;- Doubs.env We can then explore the objects containing our newly loaded data. Let us peek into the spe data: head(spe)[, 1:8] ## CHA TRU VAI LOC OMB BLA HOT TOX ## 1 0 3 0 0 0 0 0 0 ## 2 0 5 4 3 0 0 0 0 ## 3 0 5 5 5 0 0 0 0 ## 4 0 4 5 5 0 0 0 0 ## 5 0 2 3 2 0 0 0 0 ## 6 0 3 4 5 0 0 0 0 We can also use the str() function, which we learned in Workshops 1 and 2: str(spe) ## &#39;data.frame&#39;: 30 obs. of 27 variables: ## $ CHA: int 0 0 0 0 0 0 0 0 0 0 ... ## $ TRU: int 3 5 5 4 2 3 5 0 0 1 ... ## $ VAI: int 0 4 5 5 3 4 4 0 1 4 ... ## $ LOC: int 0 3 5 5 2 5 5 0 3 4 ... ## $ OMB: int 0 0 0 0 0 0 0 0 0 0 ... ## $ BLA: int 0 0 0 0 0 0 0 0 0 0 ... ## $ HOT: int 0 0 0 0 0 0 0 0 0 0 ... ## $ TOX: int 0 0 0 0 0 0 0 0 0 0 ... ## $ VAN: int 0 0 0 0 5 1 1 0 0 2 ... ## $ CHE: int 0 0 0 1 2 2 1 0 5 2 ... ## $ BAR: int 0 0 0 0 0 0 0 0 0 0 ... ## $ SPI: int 0 0 0 0 0 0 0 0 0 0 ... ## $ GOU: int 0 0 0 1 2 1 0 0 0 1 ... ## $ BRO: int 0 0 1 2 4 1 0 0 0 0 ... ## $ PER: int 0 0 0 2 4 1 0 0 0 0 ... ## $ BOU: int 0 0 0 0 0 0 0 0 0 0 ... ## $ PSO: int 0 0 0 0 0 0 0 0 0 0 ... ## $ ROT: int 0 0 0 0 2 0 0 0 0 0 ... ## $ CAR: int 0 0 0 0 0 0 0 0 0 0 ... ## $ TAN: int 0 0 0 1 3 2 0 0 1 0 ... ## $ BCO: int 0 0 0 0 0 0 0 0 0 0 ... ## $ PCH: int 0 0 0 0 0 0 0 0 0 0 ... ## $ GRE: int 0 0 0 0 0 0 0 0 0 0 ... ## $ GAR: int 0 0 0 0 5 1 0 0 4 0 ... ## $ BBO: int 0 0 0 0 0 0 0 0 0 0 ... ## $ ABL: int 0 0 0 0 0 0 0 0 0 0 ... ## $ ANG: int 0 0 0 0 0 0 0 0 0 0 ... You can also try some of these! # Try some of these! names(spe) # names of objects dim(spe) # dimensions str(spe) # structure of objects summary(spe) # summary statistics head(spe) # first 6 rows 7.2 Doubs river environmental data str(env) ## &#39;data.frame&#39;: 30 obs. of 11 variables: ## $ das: num 0.3 2.2 10.2 18.5 21.5 32.4 36.8 49.1 70.5 99 ... ## $ alt: int 934 932 914 854 849 846 841 792 752 617 ... ## $ pen: num 48 3 3.7 3.2 2.3 3.2 6.6 2.5 1.2 9.9 ... ## $ deb: num 0.84 1 1.8 2.53 2.64 2.86 4 1.3 4.8 10 ... ## $ pH : num 7.9 8 8.3 8 8.1 7.9 8.1 8.1 8 7.7 ... ## $ dur: int 45 40 52 72 84 60 88 94 90 82 ... ## $ pho: num 0.01 0.02 0.05 0.1 0.38 0.2 0.07 0.2 0.3 0.06 ... ## $ nit: num 0.2 0.2 0.22 0.21 0.52 0.15 0.15 0.41 0.82 0.75 ... ## $ amm: num 0 0.1 0.05 0 0.2 0 0 0.12 0.12 0.01 ... ## $ oxy: num 12.2 10.3 10.5 11 8 10.2 11.1 7 7.2 10 ... ## $ dbo: num 2.7 1.9 3.5 1.3 6.2 5.3 2.2 8.1 5.2 4.3 ... It contains the following variables: Variable Description das Distance from the source [km] alt Altitude [m a.s.l.] pen Slope [per thousand] deb Mean min. discharge [m3s-1] pH pH of water dur Ca conc. (hardness) [mgL-1] pho K conc. [mgL-1] nit N conc. [mgL-1] amn NH₄⁺ conc. [mgL-1] oxy Diss. oxygen [mgL-1] dbo Biol. oxygen demand [mgL-1] You can also use summary() to obtain summary statistics from the variables in env: summary(env) # summary statistics "],["dist.html", "Chapter 8 dist()", " Chapter 8 dist() Community resemblance is almost always assessed on the basis of species composition data in the form of a site-by-species data table \\(Y_{m,n}\\). We can obtain an association matrix \\(A_{m,m}\\) in the form of pairwise distances or dissimilarities \\(D_{m,m}\\) (or similarities \\(S_{m,m}\\)) and then analyse those distances. Association matrices between objects or among descriptors allow for calculations of similarity or distances between objects or descriptors (Legendre and Legendre 2012). In R, we can compute distance or dissimilarity matrices using stats::dist(). For simplicity, let us do this without specifying arguments: dist(spe) Run dist(spe) from your end, and you should observe that the output from `dist(spe) is a lower triangular matrix representing pairwise associations between the columns of your original matrix. Let us see what the commands below show us: class(dist(spe)) ## [1] &quot;dist&quot; The output from dist() is a dist class object by default. This object is composed of a vector that contains the lower triangle of the distance matrix, distributed across columns. You can coerce it into a matrix with as.matrix(), as seen below: as.matrix(dist(spe)) Notably, you can coerce a matrix that contains distances (\\(D_{m,m}\\)) using as.dist(). You can also explore the structure and dimensions of our dist-class object and distance matrix: str(dist(spe)) dim(as.matrix(dist(spe))) ## [1] 30 30 "],["types-of-distance-coefficients.html", "Chapter 9 Types of distance coefficients 9.1 Metric distances 9.2 Semimetric distances 9.3 Nonmetric distances 9.4 Representing distance matrices", " Chapter 9 Types of distance coefficients There are three groups of distance coefficients: metrics, semimetrics, and nonmetrics . 9.1 Metric distances The first group consists of metrics, and its coefficients satisfy the following properties: minimum 0: if species \\(a\\) is equal to species \\(b\\), then \\(D(a,b)=0\\); ✅ positiveness: if \\(a \\neq b\\), then \\(D(a,b) &gt; 0\\); ✅ symmetry: \\(D(a,b) = D(b,a)\\); ✅ triangle inequality: \\(D(a,b) + D(b,c) \\geq D(a,c)\\). The sum of two sides of a triangle drawn in the Euclidean space is equal or greater than the third side. ✅ We can spot all these properties below: as.matrix(dist(spe))[1:6, 1:6] ## 1 2 3 4 5 6 ## 1 0.000000 5.385165 7.416198 7.874008 10.816654 7.348469 ## 2 5.385165 0.000000 2.449490 4.123106 10.677078 4.582576 ## 3 7.416198 2.449490 0.000000 3.000000 10.862780 4.123106 ## 4 7.874008 4.123106 3.000000 0.000000 9.219544 2.828427 ## 5 10.816654 10.677078 10.862780 9.219544 0.000000 8.185353 ## 6 7.348469 4.582576 4.123106 2.828427 8.185353 0.000000 9.1.1 Euclidean distances The most common metric distance measure is the Euclidean distance. Euclidean distance is a measure of the distance between two points in Euclidean space. In two dimensions, the Euclidean distance between two points (x1, y1) and (x2, y2) can be calculated using the Pythagorean theorem: \\[D_{1} (x_1,x_2) = \\sqrt{\\sum_{j=1}^p(y_{1j} - y_{2j})^2}\\] Euclidean distance is a commonly used measure in multivariate analyses because it provides a straightforward and intuitive way to measure the distance or similarity between observations in a multidimensional space. Using stats::dist(), we can compute it with: spe.D.Euclid &lt;- dist(x = spe, method = &quot;euclidean&quot;) And, we can test whether a distance is Euclidean using: is.euclid(spe.D.Euclid) ## [1] TRUE 9.1.2 Challenge #1 Your turn! Using the dist() function, compute the Euclidean distance matrix \\(D_{hmm}\\) for the species abundances by site matrix \\(Y_{hmm}\\) below: Sites \\(y_1\\) \\(y_2\\) \\(y_3\\) \\(s_1\\) 0 4 8 \\(s_2\\) 0 1 1 \\(s_3\\) 1 0 0 Y.hmm &lt;- data.frame(y1 = c(0, 0, 1), y2 = c(4, 1, 0), y3 = c(8, 1, 0)) After this, look into the numbers, think critically about them! Solution: You should have something similar to this: Y.hmm.DistEu &lt;- dist(x = Y.hmm, method = &quot;euclidean&quot;) as.matrix(Y.hmm.DistEu) ## 1 2 3 ## 1 0.000000 7.615773 9.000000 ## 2 7.615773 0.000000 1.732051 ## 3 9.000000 1.732051 0.000000 Now, look into the composition and the distances between sites \\(s_2\\) and \\(s_3\\) and between \\(s_1\\) and \\(s_2\\). What is going on? The Euclidean distance between sites \\(s_2\\) and \\(s_3\\), which have no species in common, is smaller than the distance between \\(s_1\\) and \\(s_2\\), which share species \\(y_2\\) and \\(y_3\\) (!). From an ecological perspective, this is a problematic assessment of the relationship among sites. This issue is known as the double-zero problem, i.e. double zeroes are treated in the same way as double presences, so that the double zeros shrink the distance between two sites. Euclidean distances ( \\(D_1\\) ) should thus not be used to compare sites based on species abundances. 9.1.3 Chord distances Orlóci (1967) proposed the Chord distance to analyse community composition. Chord distance, also known as angular distance or great-circle distance, is a measure of the distance between two points on a sphere, such as the Earth. It consists of: 1. Normalizing the data, i.e. scaling site vectors to length 1 by dividing species abundances in a given sample by the square-rooted sum of square abundances in all samples as \\[y&#39;_{Uj}=y_{Uj}/\\sum^s_{j=1}{y^2_{Uj}}\\] 2. Calculating the Euclidean distances on this normalized data: \\[D_{3} (x_1,x_2) = \\sqrt{\\sum_{j=1}^p(y&#39;_{1j} - y&#39;_{2j})^2}\\] We can use vegan::vegdist() for this one: spe.D.Ch &lt;- vegdist(spe, method = &quot;chord&quot;) ## Warning in vegdist(spe, method = &quot;chord&quot;): you have empty rows: their dissimilarities may be ## meaningless in method &quot;chord&quot; ## Warning in vegdist(spe, method = &quot;chord&quot;): missing values in results as.matrix(spe.D.Ch)[1:3, 1:3] ## 1 2 3 ## 1 0.0000000 0.7653669 0.9235374 ## 2 0.7653669 0.0000000 0.2309609 ## 3 0.9235374 0.2309609 0.0000000 When two sites share the same species in the same proportions of the number of individuals the value of \\(D_3\\) is \\(0\\), and when no species are shared, its value is \\(\\sqrt{2}\\). What happens if we compute Chord distances in the same site-by-species matrix \\(Y_{hmm}\\)? Let us try the Chord distances in the same matrix we used for Challenge #1: Y.hmm.DistCh &lt;- vegdist(Y.hmm, method = &quot;chord&quot;) as.matrix(Y.hmm.DistCh) ## 1 2 3 ## 1 0.0000000 0.3203645 1.414214 ## 2 0.3203645 0.0000000 1.414214 ## 3 1.4142136 1.4142136 0.000000 Now, let us compare with what we obtained when we used Euclidean distances: as.matrix(Y.hmm.DistEu) ## 1 2 3 ## 1 0.000000 7.615773 9.000000 ## 2 7.615773 0.000000 1.732051 ## 3 9.000000 1.732051 0.000000 See again how our matrix looks: Y.hmm ## y1 y2 y3 ## 1 0 4 8 ## 2 0 1 1 ## 3 1 0 0 So, adding any number of double zeroes to a pair of sites does not change the value of \\(D_3\\). Hence, Chord distances can be used to compare sites described by species abundances! 9.1.4 Jaccard’s coefficient Another popular association coefficient is the Jaccard similarity coefficient (1900). The Jaccard similarity coefficient was originally proposed by the French mathematician Paul Jaccard in 1901, in the context of ecology. Jaccard was interested in comparing the species composition of different plant communities, and proposed the Jaccard index as a measure of similarity between two communities based on their species richness. Jaccard’s similarity coefficient is only appropriate for binary data, and its distance coefficient is defined with the size of the intersection divided by the size of the union of the sample sets. \\[D_{7}(x_1,x_2) = 1 - \\frac{\\vert x_1 \\cap x_2 \\vert}{\\vert x_1 \\cup x_2 \\vert} = 1 - \\frac{\\vert x_1 \\cap x_2 \\vert}{\\vert x_1 \\vert + \\vert x_2 \\vert - \\vert x_1 \\cap x_2 \\vert} = 1-\\frac{a}{a+b+c}\\] where, \\(a\\) is the number of species shared between \\(x_1\\) and \\(x_2\\) that are coded \\(1\\); \\(b\\) is the number of occurrences where \\(x_1\\) and \\(x_2\\) are known to be different; \\(c\\) is the number of common absences between \\(x_1\\) and \\(x_2\\), i.e. both \\(0\\). For example, for sites \\(x_1\\) and \\(x_2\\): \\(x_1,x_2\\) \\(y_1\\) \\(y_2\\) \\(y_3\\) \\(y_4\\) \\(y_5\\) \\(x_1\\) 0 1 0 1 0 \\(x_2\\) 0 1 1 1 1 So, we can calculate \\(a\\), \\(b\\), and \\(c\\): - \\(a\\) = 1 + 1 = 2 \\(b\\) = 1 + 1 = 2 \\(c\\) = 1 And, then our distance coefficient: \\[D_{7}(x_1,x_2) = 1-\\frac{2}{2+2+1}= 0.6\\] In R, you can use use the vegan::vegdist() function to calculate the Jaccard’s coefficient: spe.D.Jac &lt;- vegdist(spe, method = &quot;jaccard&quot;, binary = TRUE) ## Warning in vegdist(spe, method = &quot;jaccard&quot;, binary = TRUE): you have empty rows: their dissimilarities may be ## meaningless in method &quot;jaccard&quot; 9.2 Semimetric distances The second group consists of semimetrics, and they violate the triangle inequality property: minimum 0: if species \\(a\\) is equal to species \\(b\\), then \\(D(a,b)=0\\); ✅ positiveness: if \\(a \\neq b\\), then \\(D(a,b) &gt; 0\\); ✅ symmetry: \\(D(a,b) = D(b,a)\\); ✅ triangle inequality: \\({D(a,b) + D(b,c) \\geq or &lt; D(a,c)}\\). The sum of two sides of a triangle drawn in the Euclidean space is not equal or greater than the third side. ❌ 9.2.1 Sørensen’s coefficient All parameters in Jaccard’s similarity coefficient have equal weights. \\[D_{7}(x_1,x_2)=1-\\frac{a}{a+b+c}\\] However, you may want to consider that a presence of a species is more informative than its absence. The distance corresponding to Sørensen’s similarity coefficient (1948) gives weight to double presences: \\[D_{13}(x_1,x_2)=1-\\frac{2a}{2a+b+c}=\\frac{b+c}{2a+b+c}\\] where, \\(a\\) is the number of species shared between \\(x_1\\) and \\(x_2\\) that are coded \\(1\\); \\(b\\) is the number of occurrences where \\(x_1\\) and \\(x_2\\) are known to be different; \\(c\\) is the number of common absences between \\(x_1\\) and \\(x_2\\), i.e. both \\(0\\). In R, you can also use the vegan::vegdist() function to calculate the Sørensen’s coefficient: spe.D.Sor &lt;- vegdist(spe, method = &quot;bray&quot;, binary = TRUE) ## Warning in vegdist(spe, method = &quot;bray&quot;, binary = TRUE): you have empty rows: their dissimilarities may be ## meaningless in method &quot;bray&quot; Because both Jaccard’s and Sørensen’s are only appropriate for presence-absence data, you must binary-transform abundance data using binary = TRUE in vegdist(). 9.2.2 Bray-Curtis’ coefficient The Bray-Curtis dissimilarity coefficient is a modified version of the Sørensen’s index and allows for species abundances: \\[D_{14}(x_1,x_2)=\\frac{\\sum{\\vert y_{1j}-y_{2j}\\vert}}{\\sum{( y_{1j}+y_{2j})}}=\\] \\[D_{14}(x_1,x_2)=1 - \\frac{2W}{A+B}\\] where, \\(W\\) is the sum of the lowest abundances in each species found between sites \\(x_1\\) and \\(x_2\\); \\(A\\) is the sum of all abundances in \\(x_1\\); and, \\(B\\) is the sum of all abundances in \\(x_2\\). For example, for sites \\(x_1\\) and \\(x_2\\): \\(x_1,x_2\\) \\(y_1\\) \\(y_2\\) \\(y_3\\) \\(y_4\\) \\(y_5\\) \\(x_1\\) 2 1 0 5 2 \\(x_2\\) 5 1 3 1 1 So: \\(W = 2 + 1 + 0 + 1 + 1 = 5\\) \\(A = 2 + 1 + 0 + 5 + 0 = 8\\) \\(B = 5 + 1 + 3 + 1 + 2 = 12\\) \\[D_{14}(x_1,x_2) = 1-\\frac{2 \\times 5}{8+12} = 0.5\\] To calculate the Bray-Curtis dissimilarity coefficient, which can account for abundances, you need to set binary = FALSE. spe.db.pa &lt;- vegdist(spe, method = &quot;bray&quot;, binary = FALSE) ## Warning in vegdist(spe, method = &quot;bray&quot;, binary = FALSE): you have empty rows: their dissimilarities may be ## meaningless in method &quot;bray&quot; spe.db &lt;- as.matrix(spe.db.pa) 9.3 Nonmetric distances Non-metric distances do not satisfy the metric properties of symmetry, triangle inequality, and identity of indiscernibles: minimum 0: if species \\(a\\) is equal to species \\(b\\), then \\(D(a,b)=0\\); ✅ positiveness: if \\(a \\neq b\\), then \\(D(a,b) &gt; or &lt; 0\\); ❌ symmetry: \\(D(a,b) = D(b,a)\\); ✅ triangle inequality: \\({D(a,b) + D(b,c) \\geq or &lt; D(a,c)}\\). The sum of two sides of a triangle drawn in the Euclidean space is not equal or greater than the third side. ❌ 9.3.1 Mahalanobis distance The Mahalanobis distance between a point \\(x\\) and a group of points with mean \\(\\mu\\) and covariance \\(\\Sigma\\) is defined as: \\[ D_{M}(x, \\mu)=\\sqrt{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)} \\] where \\(T\\) denotes the transpose, and \\(\\Sigma^{-1}\\) is the inverse (or generalized inverse) of the covariance matrix \\(\\Sigma\\). The Mahalanobis distance is a measure of the distance between a point and a group of points, taking into account the covariance structure of the data. The inverse of the covariance matrix may not exist in some cases, such as when the variables are linearly dependent or when there are more variables than observations. In these cases, we can use the generalized inverse of the covariance matrix instead of the inverse to calculate the Mahalanobis distance. This generalized inverse can be calculated using various methods, such as the Moore-Penrose pseudoinverse or the singular value decomposition. # Create a matrix x &lt;- matrix(rnorm(100 * 3), ncol = 3) # Compute the covariance matrix and its generalized inverse cov_mat &lt;- cov(x) cov_inv &lt;- MASS::ginv(cov_mat) # Compute the Mahalanobis distance using the generalized # inverse mah_dist &lt;- mahalanobis(x, colMeans(x), cov_inv) # Print the Mahalanobis distance mah_dist ## [1] 5.3061344 0.4564071 0.5675535 2.6662348 2.6104018 2.8003909 ## [7] 0.7387431 1.6389466 2.0946018 2.4312276 4.1154981 0.6594897 ## [13] 1.5999119 1.6972242 1.3342990 1.9901599 1.7357188 3.3316827 ## [19] 0.5151656 3.9368640 5.2791708 5.1663413 2.8405269 0.7353339 ## [25] 1.9673492 0.3209718 6.9408993 3.3828410 5.1428101 0.5353325 ## [31] 8.5139202 3.0309815 0.7104318 0.9032237 2.3282240 3.0346249 ## [37] 5.4598914 0.1983710 4.1290330 4.2982019 2.5967636 0.9382021 ## [43] 0.8371468 4.0157851 1.8016870 4.9426385 4.7946697 0.2282839 ## [49] 2.0152140 2.5913860 0.5940376 2.6276484 4.2863809 1.4615368 ## [55] 3.1716714 7.5784045 4.5591584 0.9571423 1.6410469 4.5505595 ## [61] 1.3234568 3.8684175 1.6439035 2.8403670 2.3683293 2.4394518 ## [67] 0.1457964 0.5843688 2.2891974 2.2350074 2.3389038 1.7066545 ## [73] 11.6688333 0.4188703 5.8838899 2.7220653 0.9200645 1.9423320 ## [79] 1.8693955 0.8986885 0.1474006 1.0547710 1.0659005 5.6092933 ## [85] 3.0226432 3.1002519 3.8087669 1.3096003 0.6523915 1.5947396 ## [91] 10.6049817 0.9768489 7.7914822 1.5312161 1.4748627 1.4869509 ## [97] 4.9946700 1.6155105 2.7046948 1.1386726 9.4 Representing distance matrices We can create graphical depictions of association matrices using the coldiss() function: # coldiss() function Color plots of a dissimilarity matrix, # without and with ordering License: GPL-2 Author: Francois # Gillet, 23 August 2012 &quot;coldiss&quot; &lt;- function(D, nc = 4, byrank = TRUE, diag = FALSE) { require(gclus) if (max(D) &gt; 1) D &lt;- D/max(D) if (byrank) { spe.color &lt;- dmat.color(1 - D, cm.colors(nc)) } else { spe.color &lt;- dmat.color(1 - D, byrank = FALSE, cm.colors(nc)) } spe.o &lt;- order.single(1 - D) speo.color &lt;- spe.color[spe.o, spe.o] op &lt;- par(mfrow = c(1, 2), pty = &quot;s&quot;) if (diag) { plotcolors(spe.color, rlabels = attributes(D)$Labels, main = &quot;Dissimilarity Matrix&quot;, dlabels = attributes(D)$Labels) plotcolors(speo.color, rlabels = attributes(D)$Labels[spe.o], main = &quot;Ordered Dissimilarity Matrix&quot;, dlabels = attributes(D)$Labels[spe.o]) } else { plotcolors(spe.color, rlabels = attributes(D)$Labels, main = &quot;Dissimilarity Matrix&quot;) plotcolors(speo.color, rlabels = attributes(D)$Labels[spe.o], main = &quot;Ordered Dissimilarity Matrix&quot;) } par(op) } # Usage: coldiss(D = dissimilarity.matrix, nc = 4, byrank = # TRUE, diag = FALSE) If D is not a dissimilarity matrix # (max(D) &gt; 1), then D is divided by max(D) nc # \\t\\t\\t\\t\\t\\t\\tnumber of colours (classes) byrank= # TRUE\\t\\tequal-sized classes byrank= FALSE\\t\\tequal-length # intervals diag = TRUE\\t\\t\\tprint object labels also on # the diagonal # Example: coldiss(spe.dj, nc=9, byrank=F, diag=T) coldiss(spe.D.Jac) You can also use ggplot2::ggplot() to represent your matrix using geom_tile(): # obtain the order of the rows and columns order_spe.D.Jac &lt;- hclust(spe.D.Jac, method = &quot;complete&quot;)$order # reorder the matrix to produce an figure ordered by # similarities order_spe.D.Jac_matrix &lt;- as.matrix(spe.D.Jac)[order_spe.D.Jac, order_spe.D.Jac] # converts to data frame molten_spe.D.Jac &lt;- reshape2::melt(as.matrix(order_spe.D.Jac_matrix)) # create ggplot object ggplot(data = molten_spe.D.Jac, aes(x = Var1, y = Var2, fill = value)) + geom_tile() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + theme_minimal() "],["transformations.html", "Chapter 10 Transformations 10.1 Presence-absence transformation 10.2 Species profiles transformation 10.3 Hellinger transformation 10.4 Z-score standardization", " Chapter 10 Transformations Communities sampled over homogeneous or short environmental conditions can have species compositions with few zeroes, so that Euclidean distances could be enough to characterize them. Nevertheless, this is rarely the reality. Species may be highly frequent when conditions are favourable, or may be absent from many sites. Sometimes, this skewness may introduce spurious problems to our analyses. We may then have to transform our composition data to appropriately analyze it. In R, we can rely on vegan::decostand() for many types of transformations. Take a look into the help of this function to see the available options: ?decostand() 10.1 Presence-absence transformation We can change the argument method to \"pa\" in vegdist() to transform our abundance data into presence-absence data: If \\(y_{ij} \\geq 1\\), then, \\(y&#39;_{ij} = 1\\). Let us recall our spe data set: spe[1:6, 1:6] ## CHA TRU VAI LOC OMB BLA ## 1 0 3 0 0 0 0 ## 2 0 5 4 3 0 0 ## 3 0 5 5 5 0 0 ## 4 0 4 5 5 0 0 ## 5 0 2 3 2 0 0 ## 6 0 3 4 5 0 0 Let us transform spe abundances to presence-absences: spe.pa &lt;- decostand(spe, method = &quot;pa&quot;) spe.pa[1:6, 1:6] ## CHA TRU VAI LOC OMB BLA ## 1 0 1 0 0 0 0 ## 2 0 1 1 1 0 0 ## 3 0 1 1 1 0 0 ## 4 0 1 1 1 0 0 ## 5 0 1 1 1 0 0 ## 6 0 1 1 1 0 0 10.2 Species profiles transformation Sometimes, one wants to remove the effects of highly abundant units. We can transform the data into profiles of relative species abundances through the following equation: \\[y&#39;_{ij} = \\frac{y_{ij}}{y_{i+}}\\] where, \\(yi+\\) indicates the sample total count over all \\(j=1,…,m\\) species, for the \\(i\\)-th sample. In decostand(), we can use the method with \"total\": spe.total &lt;- decostand(spe, method = &quot;total&quot;) spe.total[1:5, 1:6] ## CHA TRU VAI LOC OMB BLA ## 1 0 1.00000000 0.00000000 0.00000000 0 0 ## 2 0 0.41666667 0.33333333 0.25000000 0 0 ## 3 0 0.31250000 0.31250000 0.31250000 0 0 ## 4 0 0.19047619 0.23809524 0.23809524 0 0 ## 5 0 0.05882353 0.08823529 0.05882353 0 0 10.3 Hellinger transformation We can take the square-root of the species profile transformation and obtain the Hellinger transformation, which has very good mathematical properties and allows us to reduce the effects of \\(y_{ij}\\) values that are extremely large. \\[y&#39;_{ij} = \\sqrt{\\frac{y_{ij}}{y_{i+}}}\\] In decostand(), we can use the method with \"hellinger\": spe.total &lt;- decostand(spe, method = &quot;hellinger&quot;) spe.total[1:5, 1:6] ## CHA TRU VAI LOC OMB BLA ## 1 0 1.0000000 0.0000000 0.0000000 0 0 ## 2 0 0.6454972 0.5773503 0.5000000 0 0 ## 3 0 0.5590170 0.5590170 0.5590170 0 0 ## 4 0 0.4364358 0.4879500 0.4879500 0 0 ## 5 0 0.2425356 0.2970443 0.2425356 0 0 10.4 Z-score standardization Z-score standardization, also known as standard score normalization, is a technique used to transform a distribution of data to a standard normal distribution with a mean of 0 and a standard deviation of 1. It involves subtracting the mean of the data and dividing by the standard deviation. Standardizing environmental variables is crucial as you cannot compare the effects of variables with different units: ## `?`(decostand) env.z &lt;- decostand(env, method = &quot;standardize&quot;) This centres and scales the variables to make your downstream analysis more appropriate: apply(env.z, 2, mean) ## das alt pen deb pH ## 1.000429e-16 1.814232e-18 -1.659010e-17 1.233099e-17 -4.096709e-15 ## dur pho nit amm oxy ## 3.348595e-16 1.327063e-17 -8.925898e-17 -4.289646e-17 -2.886092e-16 ## dbo ## 7.656545e-17 apply(env.z, 2, sd) ## das alt pen deb pH dur pho nit amm oxy dbo ## 1 1 1 1 1 1 1 1 1 1 1 We will see more details about this transformation in the next sections! 10.4.0.1 Little review Association - “general term to describe any measure or coefficient to quantify the resemblance or difference between objects or descriptors. In an analysis between descriptors, zero means no association.” (Legendre and Legendre 2012). Similarity - a measure that is “maximum (S=1) when two objects are identical and minimum when two objects are completely different.” (Legendre and Legendre 2012). Distance (also called dissimilarity) - a measure that is “maximum (D=1) when two objects are completely different”. (Legendre and Legendre 2012). Distance or dissimilarity (D) = 1-S Choosing an association measure depends on your data, but also on what you know, ecologically about your data. Here are some commonly used dissimilarity (distance) measures (recreated from Gotelli and Ellison 2004): Measure name Property Description Euclidean Metric Distance between two points in 2D space. Manhattan Metric Distance between two points, where the distance is the sum of differences of their Cartesian coordinates, i.e. if you were to make a right able between the points. Chord Metric This distance is generally used to assess differences due to genetic drift. Mahalanobis Metric Distance between a point and a set distribution, where the distance is the number of standard deviations of the point from the mean of the distribution. Chi-square Metric Similar to Euclidean. Bray-Curtis Semi-metric Dissimilarity between two samples (or sites) where the sum of lower values for species present in both samples are divided by the sum of the species counted in each sample. Jaccard Metric Description Sorensen’s Semi-metric Bray-Curtis is 1 - Sorensen 10.4.1 Other association metrics Quantitative environmental data Let us look at associations between environmental variables (also known as Q mode analysis): `?`(dist) # euclidean distance matrix of the standardized # environmental variables env.de &lt;- dist(env.z, method = &quot;euclidean&quot;) windows() # Creates a separate graphical window coldiss(env.de, diag = TRUE) We can then look at the dependence between environmental variables (also known as R mode analysis): (env.pearson &lt;- cor(env)) # Computing Pearson&#39;s r among variables round(env.pearson, 2) # Rounds the coefficients to 2 decimal points (env.ken &lt;- cor(env, method = &quot;kendall&quot;)) # Kendall&#39;s tau rank correlation round(env.ken, 2) The Pearson correlation measures the linear correlation between two variables. The Kendall tau is a rank correlation which means that it quantifies the relationship between two descriptors or variables when the data are ordered within each variable. In some cases, there may be mixed types of environmental variables. Q mode can still be used to find associations between these environmental variables. We’ll do this by first creating an example dataframe: var.g1 &lt;- rnorm(30, 0, 1) var.g2 &lt;- runif(30, 0, 5) var.g3 &lt;- gl(3, 10) var.g4 &lt;- gl(2, 5, 30) (dat2 &lt;- data.frame(var.g1, var.g2, var.g3, var.g4)) str(dat2) summary(dat2) A dissimilarity matrix can be generated for these mixed variables using the Gower dissimilarity matrix: `?`(daisy #This function can handle NAs in the data ) (dat2.dg &lt;- daisy(dat2, metric = &quot;gower&quot;)) coldiss(dat2.dg) Challenge 1 - Advanced Calculate the Bray-Curtis and the Gower dissimilarity of species abundance CHA, TRU and VAI for sites 1, 2 and 3 (using the “spe” and “env” dataframes) without using the decostand() function. Challenge 1 - Advanced Solution &lt;hidden&gt; Subset the species data so that only sites 1, 2 are included and only the species CHA, TRU and VAI. spe.challenge &lt;- spe[1:3, 1:3] #”[1:3,” refers to rows 1 to 3 while “,1:3]” refers to the first 3 species columns (in #this case the three variables of interest) Determine total species abundance for each site of interest (sum of the 3 rows). This will be for the denominator in the above equation. (Abund.s1 &lt;- sum(spe.challenge[1, ])) (Abund.s2 &lt;- sum(spe.challenge[2, ])) (Abund.s3 &lt;- sum(spe.challenge[3, ])) # () around code will cause output to print right away in # console Now calculate the difference in species abundances for each pair of sites. For example, what is the difference between the abundance of CHA and TRU in site 1? You need to calculate the following differences: CHA and TRU site 1 CHA and VAI site 1 TRU and VAI site 1 CHA and TRU site 2 CHA and VAI site 2 TRU and VAI site 2 CHA and TRU site 3 CHA and VAI site 3 TRU and VAI site 3 Spec.s1s2 &lt;- 0 Spec.s1s3 &lt;- 0 Spec.s2s3 &lt;- 0 for (i in 1:3) { Spec.s1s2 &lt;- Spec.s1s2 + abs(sum(spe.challenge[1, i] - spe.challenge[2, i])) Spec.s1s3 &lt;- Spec.s1s3 + abs(sum(spe.challenge[1, i] - spe.challenge[3, i])) Spec.s2s3 &lt;- Spec.s2s3 + abs(sum(spe.challenge[2, i] - spe.challenge[3, i])) } Now take the differences you have calculated as the numerator in the equation for Bray-Curtis dissimilarity and the total species abundance that you already calculated as the denominator. (db.s1s2 &lt;- Spec.s1s2/(Abund.s1 + Abund.s2)) #Site 1 compared to site 2 (db.s1s3 &lt;- Spec.s1s3/(Abund.s1 + Abund.s3)) #Site 1 compared to site 3 (db.s2s3 &lt;- Spec.s2s3/(Abund.s2 + Abund.s3)) #Site 2 compared to site 3 You should find values of 0.5 for site 1 to site 2, 0.538 for site 1 to site 3 and 0.053 for site 2 to 3. Check your manual results with what you would find using the function vegdist() with the Bray-Curtis method: (spe.db.challenge &lt;- vegdist(spe.challenge, method = &quot;bray&quot;)) A matrix looking like this is produced, which should be the same as your manual calculations: Site 1 Site 2 Site 2 0.5 -- Site 3 0.538 0.0526 For the Gower dissimilarity, proceed in the same way but use the appropriate equation: # Calculate the number of columns in your dataset M &lt;- ncol(spe.challenge) # Calculate the species abundance differences between pairs # of sites for each species Spe1.s1s2 &lt;- abs(spe.challenge[1, 1] - spe.challenge[2, 1]) Spe2.s1s2 &lt;- abs(spe.challenge[1, 2] - spe.challenge[2, 2]) Spe3.s1s2 &lt;- abs(spe.challenge[1, 3] - spe.challenge[2, 3]) Spe1.s1s3 &lt;- abs(spe.challenge[1, 1] - spe.challenge[3, 1]) Spe2.s1s3 &lt;- abs(spe.challenge[1, 2] - spe.challenge[3, 2]) Spe3.s1s3 &lt;- abs(spe.challenge[1, 3] - spe.challenge[3, 3]) Spe1.s2s3 &lt;- abs(spe.challenge[2, 1] - spe.challenge[3, 1]) Spe2.s2s3 &lt;- abs(spe.challenge[2, 2] - spe.challenge[3, 2]) Spe3.s2s3 &lt;- abs(spe.challenge[2, 3] - spe.challenge[3, 3]) # Calculate the range of each species abundance between # sites Range.spe1 &lt;- max(spe.challenge[, 1]) - min(spe.challenge[, 1]) Range.spe2 &lt;- max(spe.challenge[, 2]) - min(spe.challenge[, 2]) Range.spe3 &lt;- max(spe.challenge[, 3]) - min(spe.challenge[, 3]) # Calculate the Gower dissimilarity (dg.s1s2 &lt;- (1/M) * ((Spe2.s1s2/Range.spe2) + (Spe3.s1s2/Range.spe3))) (dg.s1s3 &lt;- (1/M) * ((Spe2.s1s3/Range.spe2) + (Spe3.s1s3/Range.spe3))) (dg.s2s3 &lt;- (1/M) * ((Spe2.s2s3/Range.spe2) + (Spe3.s2s3/Range.spe3))) # Compare your results (spe.db.challenge &lt;- vegdist(spe.challenge, method = &quot;gower&quot;)) "],["clustering.html", "Chapter 11 Clustering 11.1 Single linkage agglomerative clustering 11.2 Complete linkage agglomerative clustering 11.3 Unweighted Pair Group Method with Arithmetic Mean (UPGMA) 11.4 Weighted Pair Group Method with Arithmetic Mean (WPGMA) 11.5 Ward’s minimum variance 11.6 Deciding on cut-off points 11.7 Playing with real data: the Doubs fish species data", " Chapter 11 Clustering One application of association matrices is clustering. Clustering highlights structures in the data by partitioning either the objects or the descriptors. As a result, similar objects are combined into groups, allowing distinctions – or contrasts – between groups to be identified. One goal of ecologists could be to divide a set of sites into groups with respect to their environmental conditions or their community composition. Clustering results are often represented as dendrograms (trees), where objects agglomerate into groups. There are several families of clustering methods, but for the purpose of this workshop, we will present an overview of three hierarchical agglomerative clustering methods: single linkage, complete linkage, and Ward’s minimum variance clustering. See Chapter 8 of Legendre and Legendre (2012) for more details on the different families of clustering methods. In hierarchical methods, elements of lower clusters (or groups) become members of larger, higher ranking clusters, e.g. species, genus, family, order. Prior to clustering, one needs to create an association matrix among the objects. Distance matrix is the default choice of clustering functions in R. The association matrix is first sorted in increasing distance order (or decreasing similarities). Then, groups are formed hierarchically following rules specific to each method. R has several built-in functions for computing agglomerative clusters and visualizing the results. Some of the most commonly used functions are: hclust() function in the stats package: This function computes hierarchical clustering using a variety of linkage methods, including single, complete, average, and Ward’s minimum variance. The output is a dendrogram object that can be plotted using the plot() function. agnes() function in the cluster package: This function also computes hierarchical clustering using several linkage methods, but it can handle larger datasets than hclust(). The output is an object of class “agnes” that can be plotted using the plot() function. dendextend() package: This package provides several functions for manipulating and visualizing dendrograms, including color_branches(), rotate() and cutree() . ggdendro() function in the ggdendro package: This function creates a dendrogram plot using ggplot2 syntax and provides more customization options than the base plot() function. Below, we will learn several types of clustering algorithms, while applying the simplest function in R to produce them, hclust(). 11.1 Single linkage agglomerative clustering Single linkage agglomerative clustering is a hierarchical clustering algorithm that works by iteratively merging the two closest clusters based on the minimum distance between their closest members. The steps involved in it are: Start with assigning each observation to its own cluster. Compute the distance between all pairs of clusters using a chosen distance metric (e.g., Euclidean distance). Merge the two closest clusters into a single cluster. Recompute the distance between the new cluster and all remaining clusters. Repeat steps 3 and 4 until all observations belong to a single cluster, or until a pre-defined number of clusters has been reached. In single linkage agglomerative clustering, the distance between two clusters is defined as the minimum distance between any two points in the clusters. This is why it’s also called the “nearest neighbor” or “single linkage” clustering. One disadvantage of single linkage agglomerative clustering is that it can produce long, trailing clusters that do not represent well-defined groups, also known as chaining phenomenon. This can be overcome by using other linkage criteria such as complete linkage, average linkage, or Ward’s linkage. # generate some sample data set.seed(123) x &lt;- matrix(rnorm(20), ncol = 2) # perform single linkage agglomerative clustering hc &lt;- hclust(dist(x), method = &quot;single&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of Single Linkage Agglomerative Clustering&quot;, hang = -1) 11.2 Complete linkage agglomerative clustering Complete linkage agglomerative clustering is another hierarchical clustering algorithm that works by iteratively merging the two closest clusters based on the maximum distance between their furthest members. The steps involved in the complete linkage agglomerative clustering algorithm are: Start with assigning each observation to its own cluster. Compute the distance between all pairs of clusters using a chosen distance metric (e.g., Euclidean distance). Merge the two closest clusters into a single cluster. Recompute the distance between the new cluster and all remaining clusters. Repeat steps 3 and 4 until all observations belong to a single cluster, or until a pre-defined number of clusters has been reached. In complete linkage agglomerative clustering, the distance between two clusters is defined as the maximum distance between any two points in the clusters. This is why it’s also called the “furthest neighbor” or “complete linkage” clustering. Compared to single linkage agglomerative clustering, complete linkage tends to produce more compact, spherical clusters that are less prone to the chaining phenomenon. However, it’s more sensitive to outliers and can produce unbalanced clusters if there are extreme values or noise in the data. # perform complete linkage agglomerative clustering hc &lt;- hclust(dist(x), method = &quot;complete&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of Complete Linkage Agglomerative Clustering&quot;, hang = -1) 11.3 Unweighted Pair Group Method with Arithmetic Mean (UPGMA) Another hierarchical clustering algorithm that is commonly used in bioinformatics and evolutionary biology is the Unweighted Pair Group Method with Arithmetic Mean (UPGMA). The steps involved in the UPGMA algorithm: Begin by assigning each data point to its own cluster. Compute the pairwise distances between all clusters based on the distance metric of choice, such as Euclidean distance, Manhattan distance, or Pearson correlation. Find the two closest clusters based on the pairwise distances and merge them into a single cluster. The distance between the two clusters is calculated as the average of the pairwise distances between their members. Update the pairwise distances between the new cluster and all remaining clusters. The distance between the new cluster and any other cluster is calculated as the average of the pairwise distances between the members of the new cluster and the members of the other cluster. Repeat steps 3 and 4 until all data points belong to a single cluster. # perform Unweighted Pair Group Method with Arithmetic Mean # clustering hc &lt;- hclust(dist(x), method = &quot;average&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of \\nUnweighted Pair Group Method with Arithmetic Mean \\nAgglomerative Clustering&quot;, hang = -1) The UPGMA algorithm assumes a constant rate of evolution, and therefore it is often used to construct phylogenetic trees from genetic or molecular data. The output of UPGMA is a dendrogram, which shows the hierarchical structure of the clusters. A limitation of UPGMA is that it can be sensitive to outliers and can produce biased results if there are non-random patterns of missing data or convergent evolution. Additionally, it is an unweighted method, meaning that it assumes all data points are equally important, which may not always be the case. 11.4 Weighted Pair Group Method with Arithmetic Mean (WPGMA) The Weighted Pair Group Method with Arithmetic Mean (WPGMA) is similar to UPGMA, but but it takes into account the weights of the observations (e.g., when some observations are more important than others). The algorithm works as follows: Begin by assigning each data point to its own cluster. Compute the pairwise distances between all clusters based on the distance metric of choice, such as Euclidean distance, Manhattan distance, or Pearson correlation. Find the two closest clusters based on the pairwise distances and merge them into a single cluster. The distance between the two clusters is calculated as the average of the pairwise distances between their members, weighted by their respective weights. Update the pairwise distances between the new cluster and all remaining clusters. The distance between the new cluster and any other cluster is calculated as the average of the pairwise distances between the members of the new cluster and the members of the other cluster, weighted by their respective weights. Repeat steps 3 and 4 until all data points belong to a single cluster. # perform Weighted Pair Group Method with Arithmetic Mean # clustering hc &lt;- hclust(dist(x), method = &quot;mcquitty&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of \\nWeighted Pair Group Method with Arithmetic Mean \\nAgglomerative Clustering&quot;, hang = -1) 11.5 Ward’s minimum variance Ward’s minimum variance method is a hierarchical clustering algorithm that aims to minimize the variance within each cluster by merging clusters that minimize the increase in the total sum of squared distances. The algorithm works as follows: Begin by assigning each data point to its own cluster. Compute the distance between each pair of clusters using a chosen distance metric, such as Euclidean distance or Manhattan distance. Merge the two clusters that minimize the increase in the total sum of squared distances. The increase in the sum of squared distances is calculated as the sum of squared distances within each cluster plus the squared distance between the centroids of the two clusters multiplied by the number of data points in each cluster. Compute the distance between the new cluster and all remaining clusters using the chosen distance metric. Repeat steps 3 and 4 until all data points belong to a single cluster. Ward’s method is often preferred when the data contains clusters of different sizes and densities. The output of Ward’s method is a dendrogram that shows the hierarchical structure of the clusters. Ward’s method is sensitive to outliers and can produce biased results if there are non-random patterns of missing data or if the underlying assumptions of normality and equal variances are violated. Ward’s minimum variance method can be formulated in different ways, leading to different variants known as Ward D and Ward D2. Ward D uses the sum of squared distances as the criterion to minimize when merging clusters, which is equivalent to minimizing the increase in the sum of squared deviations from the mean of the combined cluster. # perform Ward&#39;s minimum variance clustering hc &lt;- hclust(dist(x), method = &quot;ward.D&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of \\nWard&#39;s minimum variance Agglomerative Clustering&quot;, hang = -1) On the other hand, Ward D2 uses the sum of squared differences from the centroid as the criterion to minimize when merging clusters, which is equivalent to minimizing the increase in the sum of squared deviations from the centroid of the combined cluster. # perform Ward&#39;s minimum variance clustering hc &lt;- hclust(dist(x), method = &quot;ward.D2&quot;) # plot the dendrogram plot(hc, main = &quot;Dendrogram of \\nWard&#39;s D2 minimum variance Agglomerative Clustering&quot;, hang = -1) In other words, Ward D2 considers the distance between the centroids of the clusters being merged, while Ward D considers the distance between the individual data points and the mean of the merged cluster. Empirically, Ward D2 tends to produce more compact and spherical clusters, while Ward D may be more sensitive to outliers and can produce elongated or irregularly shaped clusters. However, the choice of Ward D or Ward D2 may depend on the specific characteristics of the data and the research question. 11.6 Deciding on cut-off points Deciding the cutoff point to consider cluster groups is an important step in hierarchical clustering analysis. The cutoff point determines the number of clusters to consider and can have a significant impact on the interpretation of the results. Some common methods for determining the cutoff point are: Dendrogram visual inspection: One approach is to visually inspect the dendrogram and identify a point where the branches start to become long and sparse. This point represents a natural breaking point in the hierarchy and can be used as the cutoff point. Elbow method: The elbow method involves plotting a measure of cluster quality, such as the within-cluster sum of squares, against the number of clusters. The point where the plot starts to level off is considered the elbow point and can be used as the cutoff point. Gap statistic: The gap statistic compares the within-cluster sum of squares for the actual data to the expected within-cluster sum of squares for a null reference distribution. The optimal number of clusters is the one with the largest gap statistic. Domain-specific knowledge: In some cases, domain-specific knowledge or prior research can provide guidance on the appropriate number of clusters to consider. 11.7 Playing with real data: the Doubs fish species data Let us compare the single and complete linkage clustering methods using the Doubs fish species data. Species data were already Hellinger-transformed. The cluster analysis requiring similarity or dissimilarity indices, the first step will be to generate the Hellinger distance indices. # Generates the distance matrix from Hellinger transformed # data spe.dhel &lt;- vegdist(spe.hel, method = &quot;euclidean&quot;) # See difference between the two matrices head(spe.hel) # Hellinger-transformed species data ## CHA TRU VAI LOC OMB BLA HOT TOX VAN CHE BAR SPI ## 1 0 1.0000000 0.0000000 0.0000000 0 0 0 0 0.0000000 0.0000000 0 0 ## 2 0 0.6454972 0.5773503 0.5000000 0 0 0 0 0.0000000 0.0000000 0 0 ## 3 0 0.5590170 0.5590170 0.5590170 0 0 0 0 0.0000000 0.0000000 0 0 ## 4 0 0.4364358 0.4879500 0.4879500 0 0 0 0 0.0000000 0.2182179 0 0 ## 5 0 0.2425356 0.2970443 0.2425356 0 0 0 0 0.3834825 0.2425356 0 0 ## 6 0 0.3779645 0.4364358 0.4879500 0 0 0 0 0.2182179 0.3086067 0 0 ## GOU BRO PER BOU PSO ROT CAR TAN BCO PCH GRE ## 1 0.0000000 0.0000000 0.0000000 0 0 0.0000000 0 0.0000000 0 0 0 ## 2 0.0000000 0.0000000 0.0000000 0 0 0.0000000 0 0.0000000 0 0 0 ## 3 0.0000000 0.2500000 0.0000000 0 0 0.0000000 0 0.0000000 0 0 0 ## 4 0.2182179 0.3086067 0.3086067 0 0 0.0000000 0 0.2182179 0 0 0 ## 5 0.2425356 0.3429972 0.3429972 0 0 0.2425356 0 0.2970443 0 0 0 ## 6 0.2182179 0.2182179 0.2182179 0 0 0.0000000 0 0.3086067 0 0 0 ## GAR BBO ABL ANG ## 1 0.0000000 0 0 0 ## 2 0.0000000 0 0 0 ## 3 0.0000000 0 0 0 ## 4 0.0000000 0 0 0 ## 5 0.3834825 0 0 0 ## 6 0.2182179 0 0 0 head(spe.dhel) # Hellinger distances among sites ## [1] 0.8420247 0.9391305 1.0616631 1.2308244 1.1153793 0.9391305 Use hclust() to compute both single and complete linkage clustering algorithms for this data: spe.dhel.single &lt;- hclust(spe.dhel, method = &quot;single&quot;) plot(spe.dhel.single) spe.dhel.complete &lt;- hclust(spe.dhel, method = &quot;complete&quot;) plot(spe.dhel.complete) Are there big differences between the two dendrograms? In single linkage clustering, chains of objects occur (e.g. 19, 29, 30, 20, 26, etc.), whereas more contrasted groups are formed in the complete linkage clustering. Again, it is possible to generate a Ward’s minimum variance clustering with hclust(). However, the dendogram shows squared distances by default. In order to compare this dendrogram to the single and complete linkage clustering results, one must calculate the square root of the distances. # Perform Ward minimum variance clustering spe.dhel.ward &lt;- hclust(spe.dhel, method = &quot;ward.D2&quot;) plot(spe.dhel.ward) # Re-plot the dendrogram by using the square roots of the # fusion levels spe.dhel.ward$height &lt;- sqrt(spe.dhel.ward$height) plot(spe.dhel.ward) plot(spe.dhel.ward, hang = -1) # hang=-1 aligns all objets on the same line One must be careful in the choice of an association measure and clustering method in order to correctly address a problem. What are you most interested in: gradients? contrasts between objects? Moreover, the results should be interpreted with respect to the properties of the method used. If more than one method seems suitable to an ecological question, computing them all and compare the results would be the way to go. As a reminder, clustering is not a statistical method, but further steps can be taken to identify interpretative clusters (e.g. where to cut the tree), or to compute clustering statistics. Clustering can also be combined to ordination in order to distinguish groups of sites. These go beyond the scope of this workshop, but see Borcard et al. 2011 for more details. We will now dive into ordination methods. Ordination is a method used to visualize and explore relationships among variables or observations in a multivariate dataset. The goal of ordination is to represent the data in a lower-dimensional space, such as a two-dimensional or three-dimensional plot, while preserving the overall structure of the data. Ordination can be used to explore patterns and relationships among variables or observations, to identify important variables or patterns in the data, or to visualize the results of other analyses. The first group of ordination methods we will explore are the unconstrained ordinations. "],["what-does-unconstrained-mean.html", "Chapter 12 What does “unconstrained” mean?", " Chapter 12 What does “unconstrained” mean? Unconstrained ordination methods are multivariate techniques used to visualize and explore relationships among variables or observations in a dataset without imposing any specific constraints on the relationships. These methods are called “unconstrained” because they do not require prior knowledge or assumptions about the structure of the data or the relationships among variables. Unconstrained ordination methods work by representing the variables or observations in a lower-dimensional space, such as a two-dimensional or three-dimensional plot, while preserving the overall structure of the data. Some common unconstrained ordination methods include principal component analysis (PCA), correspondence analysis (CA), and multidimensional scaling (MDS). These methods can be applied to a wide range of ecological data, including species abundance data, environmental data, and community similarity matrices. Overall, unconstrained ordination methods are widely used in ecology to explore patterns and relationships among biological communities and environmental variables. These methods are valuable tools for identifying key factors that influence community composition and structure, and for developing hypotheses about the underlying ecological processes that drive these patterns. "],["principal-component-analysis.html", "Chapter 13 Principal Component Analysis 13.1 Principal component analysis not in a nutshell 13.2 Principal component analysis using package functions 13.3 Principal component analysis on ecological data 13.4 Condensing data with principal component analysis 13.5 Scaling 13.6 Challenge #2", " Chapter 13 Principal Component Analysis Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while retaining most of its variability. It is a linear transformation method that converts the original set of variables into a new set of linearly uncorrelated variables, called principal components (PCs), which are sorted in decreasing order of variance. PCA was first introduced by Karl Pearson in 1901, who developed the mathematical foundation for the method. Later, Harold Hotelling (1933) provided a more detailed and modern interpretation of the PCA method. PCA has become one of the most commonly used techniques in data analysis due to its ability to identify hidden patterns and reduce the complexity of high-dimensional data. In essence, PCA aims to find the linear combinations of the original variables that account for the largest possible amount of variation in the dataset. The resulting principal components are orthogonal to each other, meaning that they are not correlated, and their order reflects their importance in explaining the variability of the data. To become comfortable with PCA, we will follow with detailed examples on how to perform it step-by-step, and then we will use functions from R packages to do it. 13.1 Principal component analysis not in a nutshell Suppose we have a dataset with \\(n\\) observations and \\(p\\) variables represented by an n x p matrix \\(X\\). The goal of PCA is to transform this dataset into a new set of \\(p\\) uncorrelated variables, called principal components (PCs), which capture the maximum amount of variance in the original data. 13.1.0.1 Load data In this workshop, we use the data(varechem) dataset, which contains measurements of chemical properties of 18 soil samples from a field experiment. We will select the first two variables: # Load the datasets package library(datasets) # Load the varechem dataset data(varechem) # Select data (data &lt;- varechem[, 1:2]) ## N P ## 18 19.8 42.1 ## 15 13.4 39.1 ## 24 20.2 67.7 ## 27 20.6 60.8 ## 23 23.8 54.5 ## 19 22.8 40.9 ## 22 26.6 36.7 ## 16 24.2 31.0 ## 28 29.8 73.5 ## 13 28.1 40.5 ## 14 21.8 38.1 ## 20 26.2 61.9 ## 25 22.8 50.6 ## 7 30.5 24.6 ## 5 33.1 22.7 ## 6 19.1 26.4 ## 3 31.1 32.3 ## 4 18.0 64.9 ## 2 22.3 47.4 ## 9 15.0 48.4 ## 12 16.0 32.7 ## 10 14.3 62.8 ## 11 16.7 55.8 ## 21 21.0 26.5 13.1.0.2 Standardize data We first need to standardize the data to have mean zero and unit variance: \\[ Z_{ij} = \\frac{X_{ij} - \\bar{X_j}}{s_j} \\] where Z is the standardized matrix, X is the original matrix, \\(\\bar{X_j}\\) is the mean of variable j, and \\(s_j\\) is the standard deviation of variable j. data_std &lt;- scale(data) 13.1.0.3 Compute the covariance matrix Next, we compute the covariance matrix of \\(Z\\): \\[ C = \\frac{1}{n-1}ZZ^T \\] where \\(C\\) is the covariance matrix and \\(T\\) denotes the transpose operation. The covariance matrix is a symmetric matrix that represents the pairwise covariances between the variables. The formula for the covariance between two variables \\(X\\) and \\(Y\\) is: \\[\\text{Cov}(X,Y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})\\] where \\(n\\) is the sample size, \\(X_i\\) and \\(Y_i\\) are the values of the variables for observation $i$, and \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means of the variables. cov_matrix &lt;- cov(data_std) 13.1.0.4 Perform the Eigendecomposition of the covariance matrix Then, we calculate the eigenvalues and eigenvectors of \\(C\\): \\[ Cv = \\lambda v \\] where \\(C\\) is the covariance matrix, \\(v\\) is the eigenvector and \\(\\lambda\\) is the corresponding eigenvalue. eigen_decomp &lt;- eigen(cov_matrix) Eigenvalues &lt;- eigen_decomp$values Eigenvectors &lt;- eigen_decomp$vectors The eigenvectors represent the directions in the \\(p\\)-dimensional space that capture the maximum amount of variance in the data, and the eigenvalues indicate the amount of variance captured by each eigenvector. 13.1.0.5 Project the standardized data onto the Eigenspace Finally, we project the standardized data matrix \\(Z\\) onto the new basis vectors to obtain the principal components. Here, we’ll calculate the principal component scores by multiplying the standardized data by the eigenvectors of all principal components: \\[ Y = ZV \\] where \\(Y\\) is the transformed data matrix, and \\(V\\) is the matrix of eigenvectors. F_PrComps &lt;- data_std %*% Eigenvectors head(F_PrComps) ## [,1] [,2] ## 18 0.1894957 0.4713816 ## 15 0.8662023 1.4319452 ## 24 1.3494546 -0.7909067 ## 27 0.9718543 -0.5156358 ## 23 0.2644868 -0.6269034 ## 19 -0.2510109 0.1444178 The score matrix, \\(F\\), (object F_PrComps) allows one to rotate the new data space, so it is represented in relation to the principal components. For instance, see the figure below: Figure 13.1: Relationship between N and P from the varechem dataset. The purple and orange diagonal lines represent the first and second principal components, respectively. Purple lines denote the residuals between the data point and the first principal component, while orange segmented lines denote the distance between points and the second principal componenet. Arrows indicate the Eigenvalues (direction) from the principal component analysis done on these two variables. Figure 13.2: Relationship between PC1 and PC2 from the principal component analysis done on the P and N variables from the varechem dataset. The purple and orange lines represent the first and second principal components, respectively. Purple lines denote the residuals between the data point and the first principal component, while orange segmented lines denote the distance between points and the second principal componenet. 13.2 Principal component analysis using package functions PCA can also be computed using the stats::prcomp(), stats::princomp(), vegan::rda(), and ade4::dudi.pca() functions. In a nutshell, this is what we have done: data(varechem) Y &lt;- varechem[, 1:2] Y_std &lt;- as.matrix(scale(Y)) Y_R &lt;- cov(Y_std) Eigenvalues &lt;- eigen(Y_R)$values Eigenvectors &lt;- eigen(Y_R)$vectors F_PrComps &lt;- Y_std %*% Eigenvectors head(F_PrComps) ## [,1] [,2] ## 18 0.1894957 0.4713816 ## 15 0.8662023 1.4319452 ## 24 1.3494546 -0.7909067 ## 27 0.9718543 -0.5156358 ## 23 0.2644868 -0.6269034 ## 19 -0.2510109 0.1444178 How, how does it compare to stats::prcomp()? PCA_prcomp &lt;- prcomp(Y, center = TRUE, scale = TRUE) # or PCA_prcomp &lt;- prcomp(Y_std) head(PCA_prcomp$x) ## PC1 PC2 ## 18 -0.1894957 -0.4713816 ## 15 -0.8662023 -1.4319452 ## 24 -1.3494546 0.7909067 ## 27 -0.9718543 0.5156358 ## 23 -0.2644868 0.6269034 ## 19 0.2510109 -0.1444178 And, how does it compare to stats::princomp()? PCA_princomp &lt;- princomp(Y_std) head(PCA_princomp$scores) ## Comp.1 Comp.2 ## 18 -0.1894957 -0.4713816 ## 15 -0.8662023 -1.4319452 ## 24 -1.3494546 0.7909067 ## 27 -0.9718543 0.5156358 ## 23 -0.2644868 0.6269034 ## 19 0.2510109 -0.1444178 And to vegan::rda()? PCA_vegan_rda &lt;- rda(Y_std) scores(PCA_vegan_rda, display = &quot;sites&quot;, scaling = 1, choices = seq_len(PCA_vegan_rda$CA$rank), const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ] ## PC1 PC2 ## 18 -0.1894957 -0.4713816 ## 15 -0.8662023 -1.4319452 ## 24 -1.3494546 0.7909067 ## 27 -0.9718543 0.5156358 ## 23 -0.2644868 0.6269034 vegan::rda() is a bit special. It uses alternative scalings. We will not cover them here, but you can study the vignette(\"decision-vegan\"). 13.3 Principal component analysis on ecological data We have implemented PCA on a two-variables dataset, for simplicity. Let us advance and apply it to our fish species dataset. For this, we will use the vegan::rda() function on the Hellinger-transformed fish data and summarise the results: spe.h.pca &lt;- rda(spe.hel) # summary(spe.h.pca) The first lines of summary.rda() tell us about the Total variance and Unconstrained variance in our model. ## [1] &quot;Partitioning of variance:&quot; &quot; Inertia Proportion&quot; ## [3] &quot;Total 0.5023 1&quot; &quot;Unconstrained 0.5023 1&quot; ## [1] &quot;Importance of components:&quot; ## [2] &quot; PC1 PC2 PC3 PC4 PC5 PC6 PC7&quot; ## [3] &quot;Eigenvalue 0.2491 0.06592 0.04615 0.03723 0.02125 0.01662 0.01477&quot; ## [4] &quot;Proportion Explained 0.4959 0.13122 0.09188 0.07412 0.04230 0.03309 0.02940&quot; ## [5] &quot;Cumulative Proportion 0.4959 0.62715 0.71903 0.79315 0.83544 0.86853 0.89794&quot; ## [6] &quot; PC14 PC15 PC16 PC17 PC18 PC19&quot; ## [7] &quot;Eigenvalue 0.002612 0.001505 0.001387 0.001037 0.0007815 0.0004749&quot; ## [8] &quot;Proportion Explained 0.005200 0.002996 0.002761 0.002065 0.0015557 0.0009454&quot; ## [9] &quot;Cumulative Proportion 0.987229 0.990225 0.992986 0.995051 0.9966069 0.9975523&quot; This is followed by the Eigenvalues, and their contribution to the variance. In fact, if we sum all our Eigenvalues, we will obtain the amount of uncostrained variance explained by the analysis! sum(spe.h.pca$CA$eig) ## [1] 0.5023429 The next information is related to the scaling, to the species scores, and to the site scores. ## [1] &quot;Eigenvalue 0.0004263 0.0002812 0.0002188 0.0001382 0.0000876&quot; ## [2] &quot;Proportion Explained 0.0008487 0.0005598 0.0004356 0.0002752 0.0001744&quot; ## [3] &quot;Cumulative Proportion 0.9984010 0.9989608 0.9993965 0.9996717 0.9998460&quot; ## [4] &quot; PC25 PC26 PC27&quot; ## [5] &quot;Proportion Explained 1.062e-04 2.938e-05 1.835e-05&quot; ## [6] &quot;Cumulative Proportion 1.000e+00 1.000e+00 1.000e+00&quot; ## [7] &quot;Scaling 2 for species and site scores&quot; ## [8] &quot;* Species are scaled proportional to eigenvalues&quot; ## [9] &quot;* Sites are unscaled: weighted dispersion equal on all dimensions&quot; ## [10] &quot;* General scaling constant of scores: 1.953663 &quot; ## [11] &quot;&quot; ## [12] &quot;&quot; ## [13] &quot;Species scores&quot; ## [14] &quot;BCO -0.20055 0.08332 -0.074787 -0.0504875 0.073890 0.0249842&quot; ## [15] &quot;PCH -0.14626 0.05268 -0.072012 -0.0432572 0.050318 0.0178776&quot; ## [16] &quot;GAR -0.35085 -0.09353 0.198664 0.0178669 0.023796 -0.0971362&quot; ## [17] &quot;BBO -0.24167 0.03598 -0.079528 -0.0339049 0.096690 0.0620979&quot; ## [18] &quot;ABL -0.42269 -0.22879 0.007158 0.1128353 0.006759 0.1248913&quot; ## [19] &quot;ANG -0.20521 0.11557 -0.072060 -0.0159902 0.072030 -0.0003801&quot; ## [20] &quot;&quot; ## [21] &quot;&quot; ## [22] &quot;Site scores (weighted sums of species scores)&quot; Species refer to your descriptors (i.e. the columns in your dataset), which here are the fish species. Scores refer to the position of every species along the principal components. ## [1] &quot;Cumulative Proportion 1.000e+00 1.000e+00 1.000e+00&quot; ## [2] &quot;Scaling 2 for species and site scores&quot; ## [3] &quot;* Species are scaled proportional to eigenvalues&quot; ## [4] &quot;* Sites are unscaled: weighted dispersion equal on all dimensions&quot; ## [5] &quot;* General scaling constant of scores: 1.953663 &quot; ## [6] &quot;&quot; ## [7] &quot;&quot; ## [8] &quot;Species scores&quot; ## [1] &quot;PCH -0.14626 0.05268 -0.072012 -0.0432572 0.050318 0.0178776&quot; ## [2] &quot;GAR -0.35085 -0.09353 0.198664 0.0178669 0.023796 -0.0971362&quot; ## [3] &quot;BBO -0.24167 0.03598 -0.079528 -0.0339049 0.096690 0.0620979&quot; ## [4] &quot;ABL -0.42269 -0.22879 0.007158 0.1128353 0.006759 0.1248913&quot; ## [5] &quot;ANG -0.20521 0.11557 -0.072060 -0.0159902 0.072030 -0.0003801&quot; ## [6] &quot;&quot; ## [7] &quot;&quot; ## [8] &quot;Site scores (weighted sums of species scores)&quot; Sites represent the rows in your dataset, which here are the different sites along the Doubs river. This information can be obtained with the score() function that we used before: scores(spe.h.pca, display = &quot;species&quot; or &quot;sites&quot;) 13.4 Condensing data with principal component analysis Here, we have 27 principal components. However, we can apply algorithms to select the lowest number of principal components that still account for a large variance in the data. 13.4.0.1 Kaiser-Guttman criterion We can select the principal components that capture more variance than the average explanation of all principal components. We do this by: Extracting the Eigenvalues associated to the principal components; Subsetting the Eigenvalues above the mean Eigenvalue: ev &lt;- spe.h.pca$CA$eig # ev[ev &gt; mean(ev)] n &lt;- length(ev) barplot(ev, main = &quot;&quot;, col = &quot;grey&quot;, las = 2) abline(h = mean(ev), col = &quot;red3&quot;, lwd = 2) legend(&quot;topright&quot;, &quot;Average eigenvalue&quot;, lwd = 2, col = &quot;red3&quot;, bty = &quot;n&quot;) 13.4.0.2 Broken-stick model The broken-stick model retains components that explain more variance than would be expected by randomly dividing the variance into \\(p\\) parts. head(bstick(spe.h.pca)) ## PC1 PC2 PC3 PC4 PC5 PC6 ## 0.07240169 0.05379640 0.04449375 0.03829199 0.03364067 0.02991961 screeplot(spe.h.pca, bstick = TRUE, type = &quot;lines&quot;) 13.5 Scaling All that is left is to discuss scaling and to visualize our results. Let us practice and compute a PCA on the standardized environmental variables for the same dataset. env.pca &lt;- rda(env.z) # summary(env.pca, scaling = 2) Determine our subset of Eigenvalues and their corresponding Eigenvectors: ev &lt;- env.pca$CA$eig ev[ev &gt; mean(ev)] ## PC1 PC2 PC3 ## 5.968749 2.163818 1.065164 The information computed by the PCA can be represented with biplots. We can produce a quick and dirty biplot of the PCA using the function plot() in base R. plot(spe.h.pca) biplot() from base R allows for a better interpretation. biplot(spe.h.pca) The arrows are plotted to show the directionality and angle of the descriptors in the ordination. Descriptors at 180 degrees of each other are negatively correlated; Descriptors at 90 degrees of each other have zero correlation; Descriptors at 0 degrees of each other are positively correlated. Type 2 scaling (default): distances among objects are not approximations of Euclidean distances; angles between descriptor (species) vectors reflect their correlations. biplot(spe.h.pca, scaling = 2) Type 1 scaling: attempts to preserve the Euclidean distance (in multidimensional space) among objects (sites): the angles among descriptor (species) vector are not meaningful. biplot(spe.h.pca, scaling = 1) 13.6 Challenge #2 Using everything you have learned, compute a PCA on the mite species abundance data data(mite) Be ready to discuss and answer: What are the most relevant principal components, i.e. subset them? Which groups of sites can you identify? Which groups of species are related to these groups of sites? Challenge 2 - Solution Your code likely looks like the following. Compute PCA on the Hellinger-transformed species data mite.spe.hel &lt;- decostand(mite, method = &quot;hellinger&quot;) mite.spe.h.pca &lt;- rda(mite.spe.hel) Apply the Kaiser-Guttman criterion ev &lt;- mite.spe.h.pca$CA$eig ev[ev &gt; mean(ev)] n &lt;- length(ev) barplot(ev, main = &quot;Eigenvalues&quot;, col = &quot;grey&quot;, las = 2) abline(h = mean(ev), col = &quot;red3&quot;, lwd = 2) legend(&quot;topright&quot;, &quot;Average eigenvalue&quot;, lwd = 2, col = &quot;red3&quot;, bty = &quot;n&quot;) biplot(mite.spe.h.pca, col = c(&quot;red3&quot;, &quot;grey15&quot;)) "],["correspondence-analysis.html", "Chapter 14 Correspondence Analysis", " Chapter 14 Correspondence Analysis One of the key assumptions made in PCA is that species are related to each other linearly and that they respond linearly to ecological gradients. This is not necessarily the case with a lot of ecological data (e.g. many species have unimodal species distributions). Using PCA on data with unimodal species distributions or a lot of zero values may lead to a phenomenon called the “horseshoe effect”, and can occur with long ecological gradients. As such, a CA or Correspondence Analysis may be a better option for this type of data, see Legendre and Legendre (2012) for further information. As CA preserves Chi2 distances (while PCA preserves Euclidean distances), this technique is indeed better suited to ordinate datasets containing unimodal species distributions, and has, for a long time, been one of the favourite tools for the analysis of species presence–absence or abundance data. In CA, the raw data are first transformed into a matrix Q of cell-by-cell contributions to the Pearson Chi2 statistic, and the resulting table is submitted to a singular value decomposition to compute its eigenvalues and eigenvectors. The result is an ordination, where it is the Chi2 distance that is preserved among sites instead of the Euclidean distance in PCA. The Chi2 distance is not influenced by the double zeros. Therefore, CA is a method adapted to the analysis of species abundance data without pre-transformation. Contrary to PCA, CA can also be applied to analyze both quantitative and binary data (such as species abundances or absence/presence). As in PCA, the Kaiser-Guttman criterion can be applied to determine the significant axes of a CA, and ordination axes can be extracted to be used in multiples regressions. Run a CA on species data: # Run the CA using the cca() function (NB: cca() is used # for both CA and CCA) spe.ca &lt;- cca(spe[-8, ]) # Identify the significant axes ev &lt;- spe.ca$CA$eig ev[ev &gt; mean(ev)] ## CA1 CA2 CA3 CA4 CA5 ## 0.60099264 0.14437089 0.10729384 0.08337321 0.05157826 n = length(ev) barplot(ev, main = &quot;Eigenvalues&quot;, col = &quot;grey&quot;, las = 2) abline(h = mean(ev), col = &quot;red&quot;) legend(&quot;topright&quot;, &quot;Average eigenvalue&quot;, lwd = 1, col = 2, bty = &quot;n&quot;) From this barplot, you can see that once you reach C6, the proportion of variance explained falls below the average proportion explained by the other components. If you take another look at the CA summary, you will notice that by the time you reach CA5, the cumulative proportion of variance explained by the principal components is 84.63%. summary(spe.h.pca) #overall results ## ## Call: ## rda(X = spe.hel) ## ## Partitioning of variance: ## Inertia Proportion ## Total 0.5023 1 ## Unconstrained 0.5023 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 0.2491 0.06592 0.04615 0.03723 0.02125 0.01662 0.01477 ## Proportion Explained 0.4959 0.13122 0.09188 0.07412 0.04230 0.03309 0.02940 ## Cumulative Proportion 0.4959 0.62715 0.71903 0.79315 0.83544 0.86853 0.89794 ## PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 0.01297 0.01054 0.006666 0.00504 0.004258 0.002767 ## Proportion Explained 0.02582 0.02098 0.013269 0.01003 0.008477 0.005507 ## Cumulative Proportion 0.92376 0.94474 0.958011 0.96804 0.976521 0.982029 ## PC14 PC15 PC16 PC17 PC18 PC19 ## Eigenvalue 0.002612 0.001505 0.001387 0.001037 0.0007815 0.0004749 ## Proportion Explained 0.005200 0.002996 0.002761 0.002065 0.0015557 0.0009454 ## Cumulative Proportion 0.987229 0.990225 0.992986 0.995051 0.9966069 0.9975523 ## PC20 PC21 PC22 PC23 PC24 ## Eigenvalue 0.0004263 0.0002812 0.0002188 0.0001382 0.0000876 ## Proportion Explained 0.0008487 0.0005598 0.0004356 0.0002752 0.0001744 ## Cumulative Proportion 0.9984010 0.9989608 0.9993965 0.9996717 0.9998460 ## PC25 PC26 PC27 ## Eigenvalue 5.336e-05 1.476e-05 9.216e-06 ## Proportion Explained 1.062e-04 2.938e-05 1.835e-05 ## Cumulative Proportion 1.000e+00 1.000e+00 1.000e+00 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 1.953663 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## CHA 0.17113 0.08669 -0.060772 0.2536941 -0.027774 0.0129709 ## TRU 0.64097 0.02193 -0.232895 -0.1429053 -0.059150 0.0013299 ## VAI 0.51106 0.19774 0.165053 0.0203364 0.105582 0.1226508 ## LOC 0.38002 0.22219 0.235145 -0.0344577 0.126570 0.0570437 ## OMB 0.16679 0.06494 -0.087248 0.2444247 0.016349 0.0537362 ## BLA 0.07644 0.14707 -0.041152 0.2304754 -0.104984 -0.0424943 ## HOT -0.18392 0.05238 -0.042963 0.0222676 0.071349 0.0299974 ## TOX -0.14601 0.17844 -0.029561 0.0622957 -0.002615 -0.0839282 ## VAN -0.11487 0.18186 0.125113 -0.0190457 -0.197543 0.0253735 ## CHE -0.09792 -0.08873 0.281887 0.1094447 0.027418 -0.0153611 ## BAR -0.19770 0.21144 -0.072035 0.0956702 0.005739 -0.0309983 ## SPI -0.17618 0.16158 -0.048527 0.0387172 0.031417 -0.0535635 ## GOU -0.23096 0.12248 0.062575 -0.0025267 -0.144007 0.1420702 ## BRO -0.15129 0.14242 0.028378 -0.1206878 -0.096488 0.0729418 ## PER -0.15699 0.19145 0.034700 -0.0969487 -0.041512 -0.0483958 ## BOU -0.22734 0.13581 -0.074011 -0.0111431 0.080027 -0.0019656 ## PSO -0.22670 0.08348 -0.067212 0.0198179 0.063237 0.0144138 ## ROT -0.19113 0.03578 -0.007672 -0.0724430 -0.070206 0.0686039 ## CAR -0.18609 0.13050 -0.063693 0.0005777 0.039554 -0.0284575 ## TAN -0.19138 0.17582 0.094398 -0.0867317 0.010912 -0.0851924 ## BCO -0.20055 0.08332 -0.074787 -0.0504875 0.073890 0.0249842 ## PCH -0.14626 0.05268 -0.072012 -0.0432572 0.050318 0.0178776 ## GRE -0.29970 -0.01158 -0.069047 -0.0118488 0.029937 0.1407698 ## GAR -0.35085 -0.09353 0.198664 0.0178669 0.023796 -0.0971362 ## BBO -0.24167 0.03598 -0.079528 -0.0339049 0.096690 0.0620979 ## ABL -0.42269 -0.22879 0.007158 0.1128353 0.006759 0.1248913 ## ANG -0.20521 0.11557 -0.072060 -0.0159902 0.072030 -0.0003801 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## 1 0.370801 -0.49113 -1.018411 -0.58387 -0.491672 -0.622062 ## 2 0.507019 -0.05688 -0.175969 -0.42418 0.407790 0.160725 ## 3 0.464652 0.02937 -0.067360 -0.49566 0.324339 0.313053 ## 4 0.299434 0.19037 0.241315 -0.54538 0.009838 0.197974 ## 5 -0.003672 0.13483 0.515723 -0.53640 -0.796183 -0.208554 ## 6 0.212943 0.16142 0.538108 -0.44366 -0.138553 -0.066196 ## 7 0.440596 -0.01853 0.174782 -0.31336 0.171713 0.131172 ## 8 0.032182 -0.53492 -0.354289 -0.07870 -0.125287 -0.632592 ## 9 0.040265 -0.33590 0.937036 0.06715 0.610763 -0.828040 ## 10 0.299210 0.06061 0.564573 -0.12200 -0.091261 0.461350 ## 11 0.470590 -0.10233 -0.100020 0.31166 0.343997 0.217288 ## 12 0.479878 -0.05742 -0.117576 0.30965 0.375923 0.191067 ## 13 0.486974 0.03698 -0.429138 0.55813 0.050055 0.110758 ## 14 0.373684 0.16616 -0.209667 0.63367 -0.181029 0.289613 ## 15 0.277984 0.25815 0.075162 0.61382 -0.490132 0.026548 ## 16 0.076037 0.48507 0.102428 0.32702 -0.598708 -0.551888 ## 17 -0.056099 0.44027 -0.008316 0.42040 -0.063619 -0.422902 ## 18 -0.138360 0.40056 0.006109 0.39558 -0.005006 -0.202673 ## 19 -0.273219 0.33436 0.144602 0.08318 0.176567 0.023573 ## 20 -0.383501 0.21441 0.030917 -0.03750 0.161033 -0.009409 ## 21 -0.414203 0.22676 -0.110283 -0.12803 0.156309 0.088863 ## 22 -0.448647 0.16664 -0.159017 -0.12942 0.103049 -0.008044 ## 23 -0.244257 -1.03990 0.345313 0.42836 0.062931 -0.378721 ## 24 -0.361605 -0.78826 -0.007626 0.30578 0.222002 0.528274 ## 25 -0.328522 -0.56914 0.211636 0.02271 -1.012465 0.829481 ## 26 -0.446605 0.02551 -0.126306 -0.13538 0.213013 0.226330 ## 27 -0.449471 0.11911 -0.170655 -0.13637 0.182680 0.072495 ## 28 -0.451333 0.11548 -0.200522 -0.14906 0.242752 0.055270 ## 29 -0.360249 0.26693 -0.299234 -0.01005 0.120624 0.101802 ## 30 -0.472505 0.16141 -0.333315 -0.20807 0.058535 -0.094556 # summary(spe.h.pca, diplay=NULL)# only axis eigenvalues # and contribution CA results are presented in a similar manner as PCA results. You can see here that CA1 explains 51.50% of the variation in species abundances, while CA2 explain 12.37% of the variation. par(mfrow = c(1, 2)) #### scaling 1 plot(spe.ca, scaling = 1, type = &quot;none&quot;, main = &quot;CA - biplot scaling 1&quot;, xlab = c(&quot;CA1 (%)&quot;, round((spe.ca$CA$eig[1]/sum(spe.ca$CA$eig)) * 100, 2)), ylab = c(&quot;CA2 (%)&quot;, round((spe.ca$CA$eig[2]/sum(spe.ca$CA$eig)) * 100, 2))) points(scores(spe.ca, display = &quot;sites&quot;, choices = c(1, 2), scaling = 1), pch = 21, col = &quot;black&quot;, bg = &quot;steelblue&quot;, cex = 1.2) text(scores(spe.ca, display = &quot;species&quot;, choices = c(1), scaling = 1), scores(spe.ca, display = &quot;species&quot;, choices = c(2), scaling = 1), labels = rownames(scores(spe.ca, display = &quot;species&quot;, scaling = 1)), col = &quot;red&quot;, cex = 0.8) #### scaling 2 plot(spe.ca, scaling = 1, type = &quot;none&quot;, main = &quot;CA - biplot scaling 2&quot;, xlab = c(&quot;CA1 (%)&quot;, round((spe.ca$CA$eig[1]/sum(spe.ca$CA$eig)) * 100, 2)), ylab = c(&quot;CA2 (%)&quot;, round((spe.ca$CA$eig[2]/sum(spe.ca$CA$eig)) * 100, 2)), ylim = c(-2, 3)) points(scores(spe.ca, display = &quot;sites&quot;, choices = c(1, 2), scaling = 2), pch = 21, col = &quot;black&quot;, bg = &quot;steelblue&quot;, cex = 1.2) text(scores(spe.ca, display = &quot;species&quot;, choices = c(1), scaling = 2), scores(spe.ca, display = &quot;species&quot;, choices = c(2), scaling = 2), labels = rownames(scores(spe.ca, display = &quot;species&quot;, scaling = 2)), col = &quot;red&quot;, cex = 0.8) These biplots show that a group of sites located in the left part with similar fish community characterized by numerous species such as GAR, TAN, PER, ROT, PSO and CAR; in the upper right corner, an other site cluster characterized by the species LOC, VAI and TRU is identified; the last site cluster in the lower right corner of the biplot is characterized by the species BLA, CHA and OMB. Challenge 4 Run a CA of the “mite” species abundance data. What are the significant axes of variation? Which groups of sites can you identify? Which species are related to each group of sites? Challenge 4 - Solution Your code likely looks something like the following: # CA on mite species mite.spe &lt;- mite mite.spe.ca &lt;- cca(mite.spe) # What are the significant axes ? ev &lt;- mite.spe.ca$CA$eig ev[ev &gt; mean(ev)] ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 ## 0.52511362 0.22727580 0.17401743 0.12661241 0.08621687 0.07484890 0.06694738 ## CA8 ## 0.05061316 n = length(ev) barplot(ev, main = &quot;Eigenvalues&quot;, col = &quot;grey&quot;, las = 2) abline(h = mean(ev), col = &quot;red&quot;) legend(&quot;topright&quot;, &quot;Average eigenvalue&quot;, lwd = 1, col = 2, bty = &quot;n&quot;) # Output summary/results summary(mite.spe.ca, display = NULL) ## ## Call: ## cca(X = mite.spe) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 1.696 1 ## Unconstrained 1.696 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 ## Eigenvalue 0.5251 0.2273 0.1740 0.12661 0.08622 0.07485 0.06695 ## Proportion Explained 0.3096 0.1340 0.1026 0.07465 0.05083 0.04413 0.03947 ## Cumulative Proportion 0.3096 0.4436 0.5462 0.62088 0.67171 0.71584 0.75532 ## CA8 CA9 CA10 CA11 CA12 CA13 CA14 ## Eigenvalue 0.05061 0.04373 0.03323 0.03184 0.02970 0.02830 0.02652 ## Proportion Explained 0.02984 0.02578 0.01959 0.01877 0.01751 0.01668 0.01564 ## Cumulative Proportion 0.78516 0.81094 0.83054 0.84931 0.86682 0.88350 0.89914 ## CA15 CA16 CA17 CA18 CA19 CA20 ## Eigenvalue 0.02224 0.01978 0.016598 0.015868 0.014777 0.012515 ## Proportion Explained 0.01311 0.01166 0.009787 0.009356 0.008713 0.007379 ## Cumulative Proportion 0.91226 0.92392 0.933705 0.943061 0.951774 0.959153 ## CA21 CA22 CA23 CA24 CA25 CA26 ## Eigenvalue 0.010378 0.009262 0.007844 0.006765 0.006606 0.005757 ## Proportion Explained 0.006119 0.005461 0.004625 0.003989 0.003895 0.003394 ## Cumulative Proportion 0.965272 0.970733 0.975358 0.979347 0.983242 0.986636 ## CA27 CA28 CA29 CA30 CA31 CA32 ## Eigenvalue 0.005236 0.004512 0.003406 0.002828 0.002486 0.001867 ## Proportion Explained 0.003087 0.002661 0.002008 0.001667 0.001466 0.001101 ## Cumulative Proportion 0.989723 0.992384 0.994392 0.996059 0.997525 0.998625 ## CA33 CA34 ## Eigenvalue 0.0012956 0.0010357 ## Proportion Explained 0.0007639 0.0006106 ## Cumulative Proportion 0.9993894 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions # Plot the biplot plot(mite.spe.ca, scaling = 1, type = &quot;none&quot;, xlab = c(&quot;PC1 (%)&quot;, round((mite.spe.ca$CA$eig[1]/sum(mite.spe.ca$CA$eig)) * 100, 2)), ylab = c(&quot;PC2 (%)&quot;, round((mite.spe.ca$CA$eig[2]/sum(mite.spe.ca$CA$eig)) * 100, 2))) points(scores(mite.spe.ca, display = &quot;sites&quot;, choices = c(1, 2), scaling = 1), pch = 21, col = &quot;black&quot;, bg = &quot;steelblue&quot;, cex = 1.2) text(scores(mite.spe.ca, display = &quot;species&quot;, choices = c(1), scaling = 1), scores(mite.spe.ca, display = &quot;species&quot;, choices = c(2), scaling = 1), labels = rownames(scores(mite.spe.ca, display = &quot;species&quot;, scaling = 1)), col = &quot;red&quot;, cex = 0.8) "],["principal-coordinates-analysis.html", "Chapter 15 Principal Coordinates Analysis", " Chapter 15 Principal Coordinates Analysis PCA as well as CA impose the distance preserved among objects: the Euclidean distance (and several others with pre-transformations) for PCA and the Chi2 distance for CA. If one wishes to ordinate objects on the basis of another distance measure, more appropriate to the problem at hand, then PCoA is the method of choice. In PCA we rotated and plotted our data so that as much variation was explained by a first Principal Component and we can see how much “species”, either actual species or environmental variables, contribute to each component by looking at the scores (also called “loadings”). Another type of unconstrained ordination is called Principal Coordinate Analysis (PCoA). In PCoA, points are added to plane space one at a time using Euclidean distance (or whatever distance (dissimilarity) metric you choose). Basically, one point is added, then a second so that it’s distance is correct from the first point and then the third point and so on adding as many axes (dimensions) as necessary along the way. Choosing between PCA and PCoA can be tricky, but generally PCA is used to summarize multivariate data into as few dimensions as possible, whereas PCoA can be used to visualize distances between points. PCoA can be particularly suited for datasets that have more columns than rows. For example, if hundreds of species have been observed over a set of quadrats, then a approach based on a PCoA using Bray-Curtis similarity (see below) may be best suited. Run a PCoA on the Hellinger transformed species abundances (back to DoubsSpe): # Using cmdscale() ?cmdscale cmdscale(dist(spe.hel), # k=(nrow(spe)-1), eig=TRUE) # Using pcoa() `?`(pcoa) spe.h.pcoa &lt;- pcoa(dist(spe.hel)) # Extract the results spe.h.pcoa # Construct the biplot biplot.pcoa(spe.h.pcoa, spe.hel, dir.axis2 = -1) The output looks like this (and here here is a video that might help with the explanation of eigenvalues in terms of ordination). You can also run a PCoA using a different distance measure (e.g. Bray-Curtis). Here is a PCoA run on a Bray-Curtis dissimilarity matrix: spe.bray.pcoa &lt;- pcoa(spe.db) #where spe.db is the species dissimilarity matrix using Bray-Curtis. # spe.bray.pcoa biplot.pcoa(spe.bray.pcoa, spe.hel, dir.axis2 = -1) # Note that the distance measure chosen strongly influences # the results. Challenge 5 Run a PCoA on the Hellinger-transformed mite species abundance data. What are the significant axes? Which groups of sites can you identify? Which species are related to each group of sites? How do the PCoA results compare with the PCA results? Challenge 5 - Solution Your code likely looks something like this: mite.spe.h.pcoa &lt;- pcoa(dist(mite.spe.hel)) # mite.spe.h.pcoa biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel, dir.axis2 = -1) We see that species 16 and 31 are farther away from other species in terms of distance and therefore their distribution across the sites is highly dissimilar from the other species of mites (and each other). Site labels that practically overlap each other are good examples of sites with low dissimilarity (i.e. high similarity) to each other in terms of the species that are found at those sites. "],["nonmetric-multidimensional-scaling.html", "Chapter 16 Nonmetric MultiDimensional Scaling", " Chapter 16 Nonmetric MultiDimensional Scaling The unconstrained ordination methods presented above allow to organize objects (e.g. sites) characterized by descriptors (e.g. species) in full-dimensional space. In other words, PCA, CA and PCoA computes a large number of ordination axes (proportional to the number of descriptors) representing the variation of descriptors among sites and preserve distance among objects (the Euclidean distances in PCA, the Chi2 distances in CA and the type of distances defined by the user in PCoA). Users can then select the axis of interest (generally the first two ones as the explained the larger part of the variation) to represent objects in an ordination plot. The produced biplot thus represents well the distance among objects (e.g. the between-sites similarity), but fails to represent the whole variation dimensions of the ordination space (as Axis3, Axis4, …, Axisn are not represented on the biplot, but still contribute to explain the variation among objects). In some case, the priority is not to preserve the exact distances among sites, but rather to represent as accurately as possible the relationships among objects in a small and number of axes (generally two or three) specified by the user. In such cases, nonmetric multidimensional scaling (NMDS) is the solution. If two axes are selected, the biplot produced from NMDS is the better 2D graphical representation of between-objects similarity: dissimilar objects are far apart in the ordination space and similar objects close to one another. Moreover, NMDS allows users to choose the distance measure applied to calculate the ordination. To find the best representation of objects, NMDS applies an iterative procedure that tries to position the objects in the requested number of dimensions in such a way as to minimize a stress function (scaled from 0 to 1) which measure the goodness-of-fit of the distance adjustment in the reduced-space configuration. Consequently, the lower the stress value, the better the representation of objects in the ordination-space is. An additional way to assess the appropriateness of an NDMS is to construct a Shepard diagram which plot distances among objects in the ordination plot against the original distances. The R2 obtained from the regression between these two distances measure the goodness-of-fit of the NMDS ordination. # Run the NMDS spe.nmds &lt;- metaMDS(spe[, -8], distance = &quot;bray&quot;, k = 2) ## Warning in distfun(comm, method = distance, ...): you have empty rows: their dissimilarities may be ## meaningless in method &quot;bray&quot; ## Run 0 stress 0.001629362 ## Run 1 stress 0.0001181382 ## ... New best solution ## ... Procrustes: rmse 0.003724052 max resid 0.007515734 ## ... Similar to previous best ## Run 2 stress 9.553164e-05 ## ... New best solution ## ... Procrustes: rmse 0.0002547948 max resid 0.0005130467 ## ... Similar to previous best ## Run 3 stress 9.892474e-05 ## ... Procrustes: rmse 0.0001455056 max resid 0.0002503519 ## ... Similar to previous best ## Run 4 stress 9.981792e-05 ## ... Procrustes: rmse 0.0002383265 max resid 0.0004859631 ## ... Similar to previous best ## Run 5 stress 9.623433e-05 ## ... Procrustes: rmse 0.0001940406 max resid 0.0003775032 ## ... Similar to previous best ## Run 6 stress 9.717781e-05 ## ... Procrustes: rmse 0.0001516497 max resid 0.0003184701 ## ... Similar to previous best ## Run 7 stress 7.407554e-05 ## ... New best solution ## ... Procrustes: rmse 8.093881e-05 max resid 0.0001634946 ## ... Similar to previous best ## Run 8 stress 9.326399e-05 ## ... Procrustes: rmse 7.160365e-05 max resid 0.0001282936 ## ... Similar to previous best ## Run 9 stress 9.90932e-05 ## ... Procrustes: rmse 0.0002401237 max resid 0.0004369419 ## ... Similar to previous best ## Run 10 stress 9.267306e-05 ## ... Procrustes: rmse 7.009771e-05 max resid 0.0001393932 ## ... Similar to previous best ## Run 11 stress 9.869138e-05 ## ... Procrustes: rmse 0.0002399132 max resid 0.0005355816 ## ... Similar to previous best ## Run 12 stress 8.491674e-05 ## ... Procrustes: rmse 0.0001255121 max resid 0.0001843846 ## ... Similar to previous best ## Run 13 stress 3.93213e-05 ## ... New best solution ## ... Procrustes: rmse 6.521976e-05 max resid 0.0001557835 ## ... Similar to previous best ## Run 14 stress 0.0001109086 ## ... Procrustes: rmse 0.000279454 max resid 0.0005355093 ## ... Similar to previous best ## Run 15 stress 6.024348e-05 ## ... Procrustes: rmse 2.808533e-05 max resid 5.203858e-05 ## ... Similar to previous best ## Run 16 stress 0.0001046671 ## ... Procrustes: rmse 0.0002652845 max resid 0.0005638928 ## ... Similar to previous best ## Run 17 stress 8.994448e-05 ## ... Procrustes: rmse 0.0001924131 max resid 0.0003504478 ## ... Similar to previous best ## Run 18 stress 9.211753e-05 ## ... Procrustes: rmse 3.630429e-05 max resid 8.648011e-05 ## ... Similar to previous best ## Run 19 stress 9.962588e-05 ## ... Procrustes: rmse 0.0002406458 max resid 0.0004501666 ## ... Similar to previous best ## Run 20 stress 6.785095e-05 ## ... Procrustes: rmse 3.353513e-05 max resid 6.318942e-05 ## ... Similar to previous best ## *** Best solution repeated 8 times ## Warning in metaMDS(spe[, -8], distance = &quot;bray&quot;, k = 2): stress is (nearly) ## zero: you may have insufficient data ### Extract the results spe.nmds ## ## Call: ## metaMDS(comm = spe[, -8], distance = &quot;bray&quot;, k = 2) ## ## global Multidimensional Scaling using monoMDS ## ## Data: spe[, -8] ## Distance: bray ## ## Dimensions: 2 ## Stress: 3.93213e-05 ## Stress type 1, weak ties ## Best solution was repeated 8 times in 20 tries ## The best solution was from try 13 (random start) ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;spe[, -8]&#39; ### Assess the goodness of fit and draw a Shepard plot spe.nmds$stress ## [1] 3.93213e-05 stressplot(spe.nmds, main = &quot;Shepard plot&quot;) # Construct the biplot plot(spe.nmds, type = &quot;none&quot;, main = paste(&quot;NMDS/Bray - Stress=&quot;, round(spe.nmds$stress, 3)), xlab = c(&quot;NMDS1&quot;), ylab = c(&quot;NMDS2&quot;)) points(scores(spe.nmds, display = &quot;sites&quot;, choices = c(1, 2)), pch = 21, col = &quot;black&quot;, bg = &quot;steelblue&quot;, cex = 1.2) text(scores(spe.nmds, display = &quot;species&quot;, choices = c(1)), scores(spe.nmds, display = &quot;species&quot;, choices = c(2)), labels = rownames(scores(spe.nmds, display = &quot;species&quot;)), col = &quot;red&quot;, cex = 0.8) The Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance (R2 &gt; 0.95), highlighting a high goodness-of-fit of the NMDS. The biplot of the NMDS shows a group of closed sites characterized by the species BLA, TRU, VAI, LOC, CHA and OMB, while the other species form a cluster of sites in the upper right part of the graph. Four sites in the lower part of the graph are strongly different from the others. Challenge 6 Run the NMDS of the mite species abundances in 2 dimensions based on a Bray-Curtis distance. Assess the goodness-of-fit of the ordination and interpret the biplot. Challenge 6 - Solution Your code likely looks something like this: mite.spe.nmds &lt;- metaMDS(mite.spe, distance = &quot;bray&quot;, k = 2) ## Square root transformation ## Wisconsin double standardization ## Run 0 stress 0.1491318 ## Run 1 stress 0.1554939 ## Run 2 stress 0.1491563 ## ... Procrustes: rmse 0.001854294 max resid 0.009942776 ## ... Similar to previous best ## Run 3 stress 0.1722885 ## Run 4 stress 0.1640206 ## Run 5 stress 0.1510132 ## Run 6 stress 0.4079681 ## Run 7 stress 0.151013 ## Run 8 stress 0.149146 ## ... Procrustes: rmse 0.001409456 max resid 0.01034491 ## Run 9 stress 0.1509079 ## Run 10 stress 0.1547448 ## Run 11 stress 0.1549654 ## Run 12 stress 0.1582657 ## Run 13 stress 0.1564172 ## Run 14 stress 0.1515057 ## Run 15 stress 0.1651025 ## Run 16 stress 0.1491414 ## ... Procrustes: rmse 0.001307532 max resid 0.006493007 ## ... Similar to previous best ## Run 17 stress 0.1670749 ## Run 18 stress 0.1706057 ## Run 19 stress 0.1618246 ## Run 20 stress 0.1703877 ## *** Best solution repeated 2 times ### Extract the results mite.spe.nmds ## ## Call: ## metaMDS(comm = mite.spe, distance = &quot;bray&quot;, k = 2) ## ## global Multidimensional Scaling using monoMDS ## ## Data: wisconsin(sqrt(mite.spe)) ## Distance: bray ## ## Dimensions: 2 ## Stress: 0.1491318 ## Stress type 1, weak ties ## Best solution was repeated 2 times in 20 tries ## The best solution was from try 0 (metric scaling or null solution) ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;wisconsin(sqrt(mite.spe))&#39; ### Assess the goodness of fit mite.spe.nmds$stress ## [1] 0.1491318 stressplot(mite.spe.nmds, main = &quot;Shepard plot&quot;) ### Construct the biplot plot(mite.spe.nmds, type = &quot;none&quot;, main = paste(&quot;NMDS/Bray - Stress=&quot;, round(mite.spe.nmds$stress, 3)), xlab = c(&quot;NMDS1&quot;), ylab = c(&quot;NMDS2&quot;)) points(scores(mite.spe.nmds, display = &quot;sites&quot;, choices = c(1, 2)), pch = 21, col = &quot;black&quot;, bg = &quot;steelblue&quot;, cex = 1.2) text(scores(mite.spe.nmds, display = &quot;species&quot;, choices = c(1)), scores(mite.spe.nmds, display = &quot;species&quot;, choices = c(2)), labels = rownames(scores(mite.spe.nmds, display = &quot;species&quot;)), col = &quot;red&quot;, cex = 0.8) The correlation between observed dissimilarity and ordination distance (R2 &gt; 0.91) and the stress value relatively low, showing together a good accuracy of the NMDS ordination. No cluster of sites can be precisely defined from the NMDS biplot showing that most of the species occurred in most of the sites, i.e. a few sites shelter specific communities. "],["summary.html", "Chapter 17 Summary", " Chapter 17 Summary Ordination is a powerful statistical tecnhique for studying the relationships of objects characterized by several descriptors (e.g. sites described by biotic communities, or environmental variables), but serveral ordination methods exist. These methods mainly differ in the type of distance preserved, the type of variables allowed and the dimensions of the ordination space. To better guide your choice of the ordination method to use, the following table identify the main characteristics of the four ordination methods presented : In the next workshop, you will see how to identify the relationships between biotic communities and sets of environmental variables describing the same sites, using canonical analyses. "],["additional-resources.html", "Chapter 18 Additional resources", " Chapter 18 Additional resources "],["references.html", "Chapter 19 References", " Chapter 19 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
