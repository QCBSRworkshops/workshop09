<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Clustering | Workshop 9: Multivariate Analyses in R</title>
  <meta name="description" content="Multivariate Analyses in R" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Clustering | Workshop 9: Multivariate Analyses in R" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://github.com/qcbsRworkshops/assets/images/logo/csbq_logo_accueil.png" />
  <meta property="og:description" content="Multivariate Analyses in R" />
  <meta name="github-repo" content="qcbsRworkshops/workshop09" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Clustering | Workshop 9: Multivariate Analyses in R" />
  
  <meta name="twitter:description" content="Multivariate Analyses in R" />
  <meta name="twitter:image" content="https://github.com/qcbsRworkshops/assets/images/logo/csbq_logo_accueil.png" />

<meta name="author" content="Developed and maintained by the contributors of the QCBS R Workshop Series" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="transformations.html"/>
<link rel="next" href="what-does-unconstrained-mean.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/qcbs-style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="assets/images/csbq_logo_gray_accueil.png"></a></li>
<link rel="icon" type="image/png" href="assets/images/favicon.ico"/>

<li class="divider"></li>
<li class="part"><span><b>QCBS R Workshop Series</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#code-of-conduct"><i class="fa fa-check"></i><b>0.1</b> Code of conduct</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#expected-behaviour"><i class="fa fa-check"></i><b>0.1.1</b> Expected behaviour</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#unacceptable-behaviour"><i class="fa fa-check"></i><b>0.1.2</b> Unacceptable behaviour</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i><b>0.2</b> Contributors</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#contributing"><i class="fa fa-check"></i><b>0.3</b> Contributing</a></li>
</ul></li>
<li class="part"><span><b>Multivariate Analyses in <code>R</code></b></span></li>
<li class="chapter" data-level="1" data-path="learning-objectives.html"><a href="learning-objectives.html"><i class="fa fa-check"></i><b>1</b> Learning objectives</a></li>
<li class="chapter" data-level="2" data-path="preparing-for-the-workshop.html"><a href="preparing-for-the-workshop.html"><i class="fa fa-check"></i><b>2</b> Preparing for the workshop</a></li>
<li class="part"><span><b>PREAMBLE</b></span></li>
<li class="chapter" data-level="3" data-path="recap-univariate-analyses.html"><a href="recap-univariate-analyses.html"><i class="fa fa-check"></i><b>3</b> Recap: Univariate analyses</a></li>
<li class="chapter" data-level="4" data-path="intro-multivariate-analyses.html"><a href="intro-multivariate-analyses.html"><i class="fa fa-check"></i><b>4</b> Intro: Multivariate analyses</a></li>
<li class="chapter" data-level="5" data-path="setting-up-our-goals.html"><a href="setting-up-our-goals.html"><i class="fa fa-check"></i><b>5</b> Setting up our goals</a></li>
<li class="chapter" data-level="6" data-path="matrix-algebra-very-briefly.html"><a href="matrix-algebra-very-briefly.html"><i class="fa fa-check"></i><b>6</b> Matrix algebra, very briefly</a>
<ul>
<li class="chapter" data-level="6.1" data-path="matrix-algebra-very-briefly.html"><a href="matrix-algebra-very-briefly.html#data-sets-are-matrices"><i class="fa fa-check"></i><b>6.1</b> Data sets <em>are</em> matrices</a></li>
<li class="chapter" data-level="6.2" data-path="matrix-algebra-very-briefly.html"><a href="matrix-algebra-very-briefly.html#association-matrices"><i class="fa fa-check"></i><b>6.2</b> Association matrices</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="exploring-the-dataset.html"><a href="exploring-the-dataset.html"><i class="fa fa-check"></i><b>7</b> Exploring the dataset</a>
<ul>
<li class="chapter" data-level="7.1" data-path="exploring-the-dataset.html"><a href="exploring-the-dataset.html#doubs-river-fish-communities"><i class="fa fa-check"></i><b>7.1</b> Doubs river fish communities</a></li>
<li class="chapter" data-level="7.2" data-path="exploring-the-dataset.html"><a href="exploring-the-dataset.html#doubs-river-environmental-data"><i class="fa fa-check"></i><b>7.2</b> Doubs river environmental data</a></li>
</ul></li>
<li class="part"><span><b>(Dis)similarity measures</b></span></li>
<li class="chapter" data-level="8" data-path="dist.html"><a href="dist.html"><i class="fa fa-check"></i><b>8</b> <code>dist()</code></a></li>
<li class="chapter" data-level="9" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html"><i class="fa fa-check"></i><b>9</b> Types of distance coefficients</a>
<ul>
<li class="chapter" data-level="9.1" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#metric-distances"><i class="fa fa-check"></i><b>9.1</b> Metric distances</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#euclidean-distances"><i class="fa fa-check"></i><b>9.1.1</b> Euclidean distances</a></li>
<li class="chapter" data-level="9.1.2" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#challenge-1"><i class="fa fa-check"></i><b>9.1.2</b> Challenge #1</a></li>
<li class="chapter" data-level="9.1.3" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#chord-distances"><i class="fa fa-check"></i><b>9.1.3</b> Chord distances</a></li>
<li class="chapter" data-level="9.1.4" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#jaccards-coefficient"><i class="fa fa-check"></i><b>9.1.4</b> Jaccard’s coefficient</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#semimetric-distances"><i class="fa fa-check"></i><b>9.2</b> Semimetric distances</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#sørensens-coefficient"><i class="fa fa-check"></i><b>9.2.1</b> Sørensen’s coefficient</a></li>
<li class="chapter" data-level="9.2.2" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#bray-curtis-coefficient"><i class="fa fa-check"></i><b>9.2.2</b> Bray-Curtis’ coefficient</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#nonmetric-distances"><i class="fa fa-check"></i><b>9.3</b> Nonmetric distances</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.3.1</b> Mahalanobis distance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="types-of-distance-coefficients.html"><a href="types-of-distance-coefficients.html#representing-distance-matrices"><i class="fa fa-check"></i><b>9.4</b> Representing distance matrices</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>10</b> Transformations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="transformations.html"><a href="transformations.html#presence-absence-transformation"><i class="fa fa-check"></i><b>10.1</b> Presence-absence transformation</a></li>
<li class="chapter" data-level="10.2" data-path="transformations.html"><a href="transformations.html#species-profiles-transformation"><i class="fa fa-check"></i><b>10.2</b> Species profiles transformation</a></li>
<li class="chapter" data-level="10.3" data-path="transformations.html"><a href="transformations.html#hellinger-transformation"><i class="fa fa-check"></i><b>10.3</b> Hellinger transformation</a></li>
<li class="chapter" data-level="10.4" data-path="transformations.html"><a href="transformations.html#z-score-standardization"><i class="fa fa-check"></i><b>10.4</b> Z-score standardization</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="transformations.html"><a href="transformations.html#other-association-metrics"><i class="fa fa-check"></i><b>10.4.1</b> Other association metrics</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Clustering</b></span></li>
<li class="chapter" data-level="11" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>11</b> Clustering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="clustering.html"><a href="clustering.html#single-linkage-agglomerative-clustering"><i class="fa fa-check"></i><b>11.1</b> Single linkage agglomerative clustering</a></li>
<li class="chapter" data-level="11.2" data-path="clustering.html"><a href="clustering.html#complete-linkage-agglomerative-clustering"><i class="fa fa-check"></i><b>11.2</b> Complete linkage agglomerative clustering</a></li>
<li class="chapter" data-level="11.3" data-path="clustering.html"><a href="clustering.html#unweighted-pair-group-method-with-arithmetic-mean-upgma"><i class="fa fa-check"></i><b>11.3</b> Unweighted Pair Group Method with Arithmetic Mean (UPGMA)</a></li>
<li class="chapter" data-level="11.4" data-path="clustering.html"><a href="clustering.html#weighted-pair-group-method-with-arithmetic-mean-wpgma"><i class="fa fa-check"></i><b>11.4</b> Weighted Pair Group Method with Arithmetic Mean (WPGMA)</a></li>
<li class="chapter" data-level="11.5" data-path="clustering.html"><a href="clustering.html#wards-minimum-variance"><i class="fa fa-check"></i><b>11.5</b> Ward’s minimum variance</a></li>
<li class="chapter" data-level="11.6" data-path="clustering.html"><a href="clustering.html#deciding-on-cut-off-points"><i class="fa fa-check"></i><b>11.6</b> Deciding on cut-off points</a></li>
<li class="chapter" data-level="11.7" data-path="clustering.html"><a href="clustering.html#playing-with-real-data-the-doubs-fish-species-data"><i class="fa fa-check"></i><b>11.7</b> Playing with real data: the Doubs fish species data</a></li>
</ul></li>
<li class="part"><span><b>Unconstrained Ordination</b></span></li>
<li class="chapter" data-level="12" data-path="what-does-unconstrained-mean.html"><a href="what-does-unconstrained-mean.html"><i class="fa fa-check"></i><b>12</b> What does “unconstrained” mean?</a></li>
<li class="chapter" data-level="13" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>13</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-component-analysis-not-in-a-nutshell"><i class="fa fa-check"></i><b>13.1</b> Principal component analysis <em>not</em> in a nutshell</a></li>
<li class="chapter" data-level="13.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-component-analysis-using-package-functions"><i class="fa fa-check"></i><b>13.2</b> Principal component analysis using package functions</a></li>
<li class="chapter" data-level="13.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-component-analysis-on-ecological-data"><i class="fa fa-check"></i><b>13.3</b> Principal component analysis on ecological data</a></li>
<li class="chapter" data-level="13.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#condensing-data-with-principal-component-analysis"><i class="fa fa-check"></i><b>13.4</b> Condensing data with principal component analysis</a></li>
<li class="chapter" data-level="13.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#scaling"><i class="fa fa-check"></i><b>13.5</b> Scaling</a></li>
<li class="chapter" data-level="13.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#challenge-2"><i class="fa fa-check"></i><b>13.6</b> Challenge #2</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="correspondence-analysis.html"><a href="correspondence-analysis.html"><i class="fa fa-check"></i><b>14</b> Correspondence Analysis</a></li>
<li class="chapter" data-level="15" data-path="principal-coordinates-analysis.html"><a href="principal-coordinates-analysis.html"><i class="fa fa-check"></i><b>15</b> Principal Coordinates Analysis</a></li>
<li class="chapter" data-level="16" data-path="nonmetric-multidimensional-scaling.html"><a href="nonmetric-multidimensional-scaling.html"><i class="fa fa-check"></i><b>16</b> Nonmetric MultiDimensional Scaling</a></li>
<li class="part"><span><b>Final considerations</b></span></li>
<li class="chapter" data-level="17" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>17</b> Summary</a></li>
<li class="chapter" data-level="18" data-path="additional-resources.html"><a href="additional-resources.html"><i class="fa fa-check"></i><b>18</b> Additional resources</a></li>
<li class="chapter" data-level="19" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>19</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/qcbsRworkshops" target="blank">QCBS R Workshop Series</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Workshop 9: Multivariate Analyses in <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!------------------------ Hero Image Container --------------------------> 

<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=10.0,initial-scale=1.0">
  <script src="https://kit.fontawesome.com/6a26f47516.js"></script>
  <script src="assets/qcbs-hideOutput.js"></script>
  <link href="assets/qcbs-style.css" rel="stylesheet">
</head>


<!-- 
<div class="hero-image-container"> 
  <img class= "hero-image" src="assets/images/jean-philippe-delberghe-75xPHEQBmvA-unsplash_hero_image.jpg">
</div>
--!>
<div id="clustering" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Clustering<a href="clustering.html#clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>One application of association matrices is clustering. Clustering highlights structures in the data by partitioning either the objects or the descriptors. As a result, similar objects are combined into groups, allowing distinctions – or contrasts – between groups to be identified. One goal of ecologists could be to divide a set of sites into groups with respect to their environmental conditions or their community composition.</p>
<p>Clustering results are often represented as <em>dendrograms</em> (trees), where objects agglomerate into groups. There are several families of clustering methods, but for the purpose of this workshop, we will present an overview of three hierarchical agglomerative clustering methods: single linkage, complete linkage, and Ward’s minimum variance clustering. <em>See Chapter 8 of Legendre and Legendre (2012) for more details on the different families of clustering methods.</em></p>
<p>In hierarchical methods, elements of lower clusters (or groups) become members of larger, higher ranking clusters, e.g. species, genus, family, order. Prior to clustering, one needs to create an association matrix among the objects. Distance matrix is the default choice of clustering functions in R. The association matrix is first sorted in increasing distance order (or decreasing similarities). Then, groups are formed hierarchically following rules specific to each method.</p>
<p><img src="book-en_files/figure-html/unnamed-chunk-53-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>R has several built-in functions for computing agglomerative clusters and visualizing the results. Some of the most commonly used functions are:</p>
<ul>
<li><code>hclust()</code> function in the <code>stats</code> package: This function computes hierarchical clustering using a variety of linkage methods, including single, complete, average, and Ward’s minimum variance. The output is a <code>dendrogram</code> object that can be plotted using the <code>plot()</code> function.</li>
<li><code>agnes()</code> function in the <code>cluster</code> package: This function also computes hierarchical clustering using several linkage methods, but it can handle larger datasets than <code>hclust()</code>. The output is an object of class “<code>agnes</code>” that can be plotted using the <code>plot()</code> function.</li>
<li><code>dendextend()</code> package: This package provides several functions for manipulating and visualizing dendrograms, including <code>color_branches()</code>, <code>rotate()</code> and <code>cutree()</code> .</li>
<li><code>ggdendro()</code> function in the <code>ggdendro</code> package: This function creates a dendrogram plot using <code>ggplot2</code> syntax and provides more customization options than the base <code>plot()</code> function.</li>
</ul>
<p>Below, we will learn several types of clustering algorithms, while applying the simplest function in R to produce them, <code>hclust()</code>.</p>
<div id="single-linkage-agglomerative-clustering" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Single linkage agglomerative clustering<a href="clustering.html#single-linkage-agglomerative-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Single linkage agglomerative clustering is a hierarchical clustering algorithm that works by iteratively merging the two closest clusters based on the minimum distance between their closest members. The steps involved in it are:</p>
<ol style="list-style-type: decimal">
<li>Start with assigning each observation to its own cluster.</li>
<li>Compute the distance between all pairs of clusters using a chosen distance metric (e.g., Euclidean distance).</li>
<li>Merge the two closest clusters into a single cluster.</li>
<li>Recompute the distance between the new cluster and all remaining clusters.</li>
<li>Repeat steps 3 and 4 until all observations belong to a single cluster, or until a pre-defined number of clusters has been reached.</li>
</ol>
<p>In single linkage agglomerative clustering, the distance between two clusters is defined as the minimum distance between any two points in the clusters. This is why it’s also called the “nearest neighbor” or “single linkage” clustering.</p>
<p>One disadvantage of single linkage agglomerative clustering is that it can produce long, trailing clusters that do not represent well-defined groups, also known as chaining phenomenon. This can be overcome by using other linkage criteria such as complete linkage, average linkage, or Ward’s linkage.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="clustering.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate some sample data</span></span>
<span id="cb80-2"><a href="clustering.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb80-3"><a href="clustering.html#cb80-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">20</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb80-4"><a href="clustering.html#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="clustering.html#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="co"># perform single linkage agglomerative clustering</span></span>
<span id="cb80-6"><a href="clustering.html#cb80-6" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;single&quot;</span>)</span>
<span id="cb80-7"><a href="clustering.html#cb80-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-8"><a href="clustering.html#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb80-9"><a href="clustering.html#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of Single Linkage Agglomerative Clustering&quot;</span>,</span>
<span id="cb80-10"><a href="clustering.html#cb80-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-single-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="complete-linkage-agglomerative-clustering" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Complete linkage agglomerative clustering<a href="clustering.html#complete-linkage-agglomerative-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Complete linkage agglomerative clustering is another hierarchical clustering algorithm that works by iteratively merging the two closest clusters based on the maximum distance between their furthest members.</p>
<p>The steps involved in the complete linkage agglomerative clustering algorithm are:</p>
<ol style="list-style-type: decimal">
<li>Start with assigning each observation to its own cluster.</li>
<li>Compute the distance between all pairs of clusters using a chosen distance metric (e.g., Euclidean distance).</li>
<li>Merge the two closest clusters into a single cluster.</li>
<li>Recompute the distance between the new cluster and all remaining clusters.</li>
<li>Repeat steps 3 and 4 until all observations belong to a single cluster, or until a pre-defined number of clusters has been reached.</li>
<li>In complete linkage agglomerative clustering, the distance between two clusters is defined as the maximum distance between any two points in the clusters. This is why it’s also called the “furthest neighbor” or “complete linkage” clustering.</li>
</ol>
<p>Compared to single linkage agglomerative clustering, complete linkage tends to produce more compact, spherical clusters that are less prone to the chaining phenomenon. However, it’s more sensitive to outliers and can produce unbalanced clusters if there are extreme values or noise in the data.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="clustering.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform complete linkage agglomerative clustering</span></span>
<span id="cb81-2"><a href="clustering.html#cb81-2" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>)</span>
<span id="cb81-3"><a href="clustering.html#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="clustering.html#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb81-5"><a href="clustering.html#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of Complete Linkage Agglomerative Clustering&quot;</span>,</span>
<span id="cb81-6"><a href="clustering.html#cb81-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-complete-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="unweighted-pair-group-method-with-arithmetic-mean-upgma" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Unweighted Pair Group Method with Arithmetic Mean (UPGMA)<a href="clustering.html#unweighted-pair-group-method-with-arithmetic-mean-upgma" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another hierarchical clustering algorithm that is commonly used in bioinformatics and evolutionary biology is the Unweighted Pair Group Method with Arithmetic Mean (UPGMA).</p>
<p>The steps involved in the UPGMA algorithm:</p>
<ol style="list-style-type: decimal">
<li>Begin by assigning each data point to its own cluster.</li>
<li>Compute the pairwise distances between all clusters based on the distance metric of choice, such as Euclidean distance, Manhattan distance, or Pearson correlation.</li>
<li>Find the two closest clusters based on the pairwise distances and merge them into a single cluster. The distance between the two clusters is calculated as the average of the pairwise distances between their members.</li>
<li>Update the pairwise distances between the new cluster and all remaining clusters. The distance between the new cluster and any other cluster is calculated as the average of the pairwise distances between the members of the new cluster and the members of the other cluster.</li>
<li>Repeat steps 3 and 4 until all data points belong to a single cluster.</li>
</ol>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="clustering.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Unweighted Pair Group Method with Arithmetic Mean</span></span>
<span id="cb82-2"><a href="clustering.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="co"># clustering</span></span>
<span id="cb82-3"><a href="clustering.html#cb82-3" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;average&quot;</span>)</span>
<span id="cb82-4"><a href="clustering.html#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="clustering.html#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb82-6"><a href="clustering.html#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of </span><span class="sc">\n</span><span class="st">Unweighted Pair Group Method with Arithmetic Mean </span><span class="sc">\n</span><span class="st">Agglomerative Clustering&quot;</span>,</span>
<span id="cb82-7"><a href="clustering.html#cb82-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-upgma-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The UPGMA algorithm assumes a constant rate of evolution, and therefore it is often used to construct phylogenetic trees from genetic or molecular data. The output of UPGMA is a dendrogram, which shows the hierarchical structure of the clusters.</p>
<p>A limitation of UPGMA is that it can be sensitive to outliers and can produce biased results if there are non-random patterns of missing data or convergent evolution. Additionally, it is an unweighted method, meaning that it assumes all data points are equally important, which may not always be the case.</p>
</div>
<div id="weighted-pair-group-method-with-arithmetic-mean-wpgma" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Weighted Pair Group Method with Arithmetic Mean (WPGMA)<a href="clustering.html#weighted-pair-group-method-with-arithmetic-mean-wpgma" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Weighted Pair Group Method with Arithmetic Mean (WPGMA) is similar to UPGMA, but but it takes into account the weights of the observations (e.g., when some observations are more important than others). The algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>Begin by assigning each data point to its own cluster.</li>
<li>Compute the pairwise distances between all clusters based on the distance metric of choice, such as Euclidean distance, Manhattan distance, or Pearson correlation.</li>
<li>Find the two closest clusters based on the pairwise distances and merge them into a single cluster. The distance between the two clusters is calculated as the average of the pairwise distances between their members, weighted by their respective weights.</li>
<li>Update the pairwise distances between the new cluster and all remaining clusters. The distance between the new cluster and any other cluster is calculated as the average of the pairwise distances between the members of the new cluster and the members of the other cluster, weighted by their respective weights.</li>
<li>Repeat steps 3 and 4 until all data points belong to a single cluster.</li>
</ol>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="clustering.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Weighted Pair Group Method with Arithmetic Mean</span></span>
<span id="cb83-2"><a href="clustering.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="co"># clustering</span></span>
<span id="cb83-3"><a href="clustering.html#cb83-3" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;mcquitty&quot;</span>)</span>
<span id="cb83-4"><a href="clustering.html#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="clustering.html#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb83-6"><a href="clustering.html#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of </span><span class="sc">\n</span><span class="st">Weighted Pair Group Method with Arithmetic Mean </span><span class="sc">\n</span><span class="st">Agglomerative Clustering&quot;</span>,</span>
<span id="cb83-7"><a href="clustering.html#cb83-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-wpgma-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="wards-minimum-variance" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Ward’s minimum variance<a href="clustering.html#wards-minimum-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ward’s minimum variance method is a hierarchical clustering algorithm that aims to minimize the variance within each cluster by merging clusters that minimize the increase in the total sum of squared distances. The algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>Begin by assigning each data point to its own cluster.</li>
<li>Compute the distance between each pair of clusters using a chosen distance metric, such as Euclidean distance or Manhattan distance.</li>
<li>Merge the two clusters that minimize the increase in the total sum of squared distances. The increase in the sum of squared distances is calculated as the sum of squared distances within each cluster plus the squared distance between the centroids of the two clusters multiplied by the number of data points in each cluster.</li>
<li>Compute the distance between the new cluster and all remaining clusters using the chosen distance metric.</li>
<li>Repeat steps 3 and 4 until all data points belong to a single cluster.</li>
</ol>
<p>Ward’s method is often preferred when the data contains clusters of different sizes and densities. The output of Ward’s method is a dendrogram that shows the hierarchical structure of the clusters.</p>
<p>Ward’s method is sensitive to outliers and can produce biased results if there are non-random patterns of missing data or if the underlying assumptions of normality and equal variances are violated.</p>
<div class="explanation">
<p>Ward’s minimum variance method can be formulated in different ways, leading to different variants known as Ward D and Ward D2.</p>
<p>Ward D uses the sum of squared distances as the criterion to minimize when merging clusters, which is equivalent to minimizing the increase in the sum of squared deviations from the mean of the combined cluster.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="clustering.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Ward&#39;s minimum variance clustering</span></span>
<span id="cb84-2"><a href="clustering.html#cb84-2" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;ward.D&quot;</span>)</span>
<span id="cb84-3"><a href="clustering.html#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="clustering.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb84-5"><a href="clustering.html#cb84-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of </span><span class="sc">\n</span><span class="st">Ward&#39;s minimum variance Agglomerative Clustering&quot;</span>,</span>
<span id="cb84-6"><a href="clustering.html#cb84-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-ward-d-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>On the other hand, Ward D2 uses the sum of squared differences from the centroid as the criterion to minimize when merging clusters, which is equivalent to minimizing the increase in the sum of squared deviations from the centroid of the combined cluster.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="clustering.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform Ward&#39;s minimum variance clustering</span></span>
<span id="cb85-2"><a href="clustering.html#cb85-2" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(x), <span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb85-3"><a href="clustering.html#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="clustering.html#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the dendrogram</span></span>
<span id="cb85-5"><a href="clustering.html#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc, <span class="at">main =</span> <span class="st">&quot;Dendrogram of </span><span class="sc">\n</span><span class="st">Ward&#39;s D2 minimum variance Agglomerative Clustering&quot;</span>,</span>
<span id="cb85-6"><a href="clustering.html#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/hclust-ward-d2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In other words, Ward D2 considers the distance between the centroids of the clusters being merged, while Ward D considers the distance between the individual data points and the mean of the merged cluster.</p>
<p>Empirically, Ward D2 tends to produce more compact and spherical clusters, while Ward D may be more sensitive to outliers and can produce elongated or irregularly shaped clusters. However, the choice of Ward D or Ward D2 may depend on the specific characteristics of the data and the research question.</p>
</div>
</div>
<div id="deciding-on-cut-off-points" class="section level2 hasAnchor" number="11.6">
<h2><span class="header-section-number">11.6</span> Deciding on cut-off points<a href="clustering.html#deciding-on-cut-off-points" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Deciding the cutoff point to consider cluster groups is an important step in hierarchical clustering analysis. The cutoff point determines the number of clusters to consider and can have a significant impact on the interpretation of the results.</p>
<p>Some common methods for determining the cutoff point are:</p>
<ol style="list-style-type: decimal">
<li>Dendrogram visual inspection: One approach is to visually inspect the dendrogram and identify a point where the branches start to become long and sparse. This point represents a natural breaking point in the hierarchy and can be used as the cutoff point.</li>
<li>Elbow method: The elbow method involves plotting a measure of cluster quality, such as the within-cluster sum of squares, against the number of clusters. The point where the plot starts to level off is considered the elbow point and can be used as the cutoff point.</li>
<li>Gap statistic: The gap statistic compares the within-cluster sum of squares for the actual data to the expected within-cluster sum of squares for a null reference distribution. The optimal number of clusters is the one with the largest gap statistic.</li>
<li>Domain-specific knowledge: In some cases, domain-specific knowledge or prior research can provide guidance on the appropriate number of clusters to consider.</li>
</ol>
</div>
<div id="playing-with-real-data-the-doubs-fish-species-data" class="section level2 hasAnchor" number="11.7">
<h2><span class="header-section-number">11.7</span> Playing with real data: the Doubs fish species data<a href="clustering.html#playing-with-real-data-the-doubs-fish-species-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us compare the single and complete linkage clustering methods using the Doubs fish species data.</p>
<p>Species data were already Hellinger-transformed. The cluster analysis requiring similarity or dissimilarity indices, the first step will be to generate the Hellinger distance indices.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="clustering.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generates the distance matrix from Hellinger transformed</span></span>
<span id="cb86-2"><a href="clustering.html#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb86-3"><a href="clustering.html#cb86-3" aria-hidden="true" tabindex="-1"></a>spe.dhel <span class="ot">&lt;-</span> <span class="fu">vegdist</span>(spe.hel, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb86-4"><a href="clustering.html#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="clustering.html#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="co"># See difference between the two matrices</span></span>
<span id="cb86-6"><a href="clustering.html#cb86-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(spe.hel)  <span class="co"># Hellinger-transformed species data</span></span></code></pre></div>
<pre><code>##   CHA       TRU       VAI       LOC OMB BLA HOT TOX       VAN       CHE BAR SPI
## 1   0 1.0000000 0.0000000 0.0000000   0   0   0   0 0.0000000 0.0000000   0   0
## 2   0 0.6454972 0.5773503 0.5000000   0   0   0   0 0.0000000 0.0000000   0   0
## 3   0 0.5590170 0.5590170 0.5590170   0   0   0   0 0.0000000 0.0000000   0   0
## 4   0 0.4364358 0.4879500 0.4879500   0   0   0   0 0.0000000 0.2182179   0   0
## 5   0 0.2425356 0.2970443 0.2425356   0   0   0   0 0.3834825 0.2425356   0   0
## 6   0 0.3779645 0.4364358 0.4879500   0   0   0   0 0.2182179 0.3086067   0   0
##         GOU       BRO       PER BOU PSO       ROT CAR       TAN BCO PCH GRE
## 1 0.0000000 0.0000000 0.0000000   0   0 0.0000000   0 0.0000000   0   0   0
## 2 0.0000000 0.0000000 0.0000000   0   0 0.0000000   0 0.0000000   0   0   0
## 3 0.0000000 0.2500000 0.0000000   0   0 0.0000000   0 0.0000000   0   0   0
## 4 0.2182179 0.3086067 0.3086067   0   0 0.0000000   0 0.2182179   0   0   0
## 5 0.2425356 0.3429972 0.3429972   0   0 0.2425356   0 0.2970443   0   0   0
## 6 0.2182179 0.2182179 0.2182179   0   0 0.0000000   0 0.3086067   0   0   0
##         GAR BBO ABL ANG
## 1 0.0000000   0   0   0
## 2 0.0000000   0   0   0
## 3 0.0000000   0   0   0
## 4 0.0000000   0   0   0
## 5 0.3834825   0   0   0
## 6 0.2182179   0   0   0</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="clustering.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(spe.dhel)  <span class="co"># Hellinger distances among sites</span></span></code></pre></div>
<pre><code>## [1] 0.8420247 0.9391305 1.0616631 1.2308244 1.1153793 0.9391305</code></pre>
<p>Use <code>hclust()</code> to compute both single and complete linkage clustering algorithms for this data:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="clustering.html#cb90-1" aria-hidden="true" tabindex="-1"></a>spe.dhel.single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(spe.dhel, <span class="at">method =</span> <span class="st">&quot;single&quot;</span>)</span>
<span id="cb90-2"><a href="clustering.html#cb90-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-3"><a href="clustering.html#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spe.dhel.single)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-55-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="clustering.html#cb91-1" aria-hidden="true" tabindex="-1"></a>spe.dhel.complete <span class="ot">&lt;-</span> <span class="fu">hclust</span>(spe.dhel, <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>)</span>
<span id="cb91-2"><a href="clustering.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spe.dhel.complete)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-55-2.png" width="672" style="display: block; margin: auto;" /></p>
<p><em>Are there big differences between the two dendrograms?</em></p>
<p>In single linkage clustering, chains of objects occur (e.g. 19, 29, 30, 20, 26, etc.), whereas more contrasted groups are formed in the complete linkage clustering.</p>
<p>Again, it is possible to generate a Ward’s minimum variance clustering with hclust(). However, the dendogram shows squared distances by default. In order to compare this dendrogram to the single and complete linkage clustering results, one must calculate the square root of the distances.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="clustering.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Ward minimum variance clustering</span></span>
<span id="cb92-2"><a href="clustering.html#cb92-2" aria-hidden="true" tabindex="-1"></a>spe.dhel.ward <span class="ot">&lt;-</span> <span class="fu">hclust</span>(spe.dhel, <span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb92-3"><a href="clustering.html#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spe.dhel.ward)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-56-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="clustering.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-plot the dendrogram by using the square roots of the</span></span>
<span id="cb93-2"><a href="clustering.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fusion levels</span></span>
<span id="cb93-3"><a href="clustering.html#cb93-3" aria-hidden="true" tabindex="-1"></a>spe.dhel.ward<span class="sc">$</span>height <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(spe.dhel.ward<span class="sc">$</span>height)</span>
<span id="cb93-4"><a href="clustering.html#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spe.dhel.ward)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-56-2.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="clustering.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(spe.dhel.ward, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)  <span class="co"># hang=-1 aligns all objets on the same line</span></span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-56-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>One must be careful in the choice of an association measure and clustering method in order to correctly address a problem.</p>
<p>What are you most interested in: gradients? contrasts between objects?</p>
<p>Moreover, the results should be interpreted with respect to the properties of the method used. If more than one method seems suitable to an ecological question, computing them all and compare the results would be the way to go.</p>
<p>As a reminder, clustering is not a statistical method, but further steps can be taken to identify interpretative clusters (e.g. where to cut the tree), or to compute clustering statistics. Clustering can also be combined to ordination in order to distinguish groups of sites. These go beyond the scope of this workshop, but see Borcard et al. 2011 for more details.</p>

</div>
</div>



<p>We will now dive into <strong>ordination methods</strong>.</p>
<p>Ordination is a method used to visualize and explore relationships among variables or observations in a multivariate dataset. The goal of ordination is to represent the data in a lower-dimensional space, such as a two-dimensional or three-dimensional plot, while preserving the overall structure of the data. Ordination can be used to explore patterns and relationships among variables or observations, to identify important variables or patterns in the data, or to visualize the results of other analyses.</p>
<p>The first group of ordination methods we will explore are the <strong>unconstrained ordinations</strong>.</p>
</div>
<hr>
<center> 
  <div class="footer">
      All the content of the workshop series is under a <a href="https://creativecommons.org/licenses/by-nc/2.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="transformations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="what-does-unconstrained-mean.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
