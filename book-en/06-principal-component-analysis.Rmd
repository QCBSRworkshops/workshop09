---
editor_options: 
  markdown: 
    wrap: 72
---

```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.align = 'center')
```

# Principal Component Analysis

Principal Component Analysis (PCA) is a statistical technique used to
reduce the dimensionality of a dataset while retaining most of its
variability. It is a linear transformation method that converts the
original set of variables into a new set of linearly uncorrelated
variables, called principal components (PCs), which are sorted in
decreasing order of variance.

PCA was first introduced by Karl Pearson in 1901, who developed the
mathematical foundation for the method. Later, Harold Hotelling (1933)
provided a more detailed and modern interpretation of the PCA method.

PCA has become one of the most commonly used techniques in data analysis
due to its ability to identify hidden patterns and reduce the complexity
of high-dimensional data.

In essence, PCA aims to find the linear combinations of the original
variables that account for the largest possible amount of variation in
the dataset. The resulting principal components are orthogonal to each
other, meaning that they are not correlated, and their order reflects
their importance in explaining the variability of the data.

To become comfortable with PCA, we will follow with detailed examples on
how to perform it step-by-step, and then we will use functions from R
packages to do it.

## Principal component analysis *not* in a nutshell

Suppose we have a dataset with $n$ observations and $p$ variables
represented by an n x p matrix $X$. The goal of PCA is to transform this
dataset into a new set of $p$ uncorrelated variables, called principal
components (PCs), which capture the maximum amount of variance in the
original data.

#### Load data

In this workshop, we use the `data(varechem)` dataset, which contains
measurements of chemical properties of 18 soil samples from a field
experiment. We will select the first two variables:

```{r}
# Load the datasets package
library(datasets)

# Load the varechem dataset
data(varechem)

# Select data
(data <- varechem[, 1:2])

```

#### Standardize data

We first need to standardize the data to have mean zero and unit
variance:

$$ Z_{ij} = \frac{X_{ij} - \bar{X_j}}{s_j} $$

where Z is the standardized matrix, X is the original matrix,
$\bar{X_j}$ is the mean of variable j, and $s_j$ is the standard
deviation of variable j.

```{r}
data_std <- scale(data)
```

#### Compute the covariance matrix

Next, we compute the covariance matrix of $Z$:

$$ C = \frac{1}{n-1}ZZ^T $$

where $C$ is the covariance matrix and $T$ denotes the transpose
operation.

The covariance matrix is a symmetric matrix that represents the pairwise
covariances between the variables. The formula for the covariance
between two variables $X$ and $Y$ is:

$$\text{Cov}(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})$$

where $n$ is the sample size, $X_i$ and $Y_i$ are the values of the
variables for observation \$i\$, and $\bar{X}$ and $\bar{Y}$ are the
sample means of the variables.

```{r}
cov_matrix <- cov(data_std)
```

#### Perform the Eigendecomposition of the covariance matrix

Then, we calculate the eigenvalues and eigenvectors of $C$:

$$ Cv = \lambda v $$

where $C$ is the covariance matrix, $v$ is the eigenvector and $\lambda$
is the corresponding eigenvalue.

```{r}
eigen_decomp <- eigen(cov_matrix)
Eigenvalues <- eigen_decomp$values
Eigenvectors <- eigen_decomp$vectors
```

The eigenvectors represent the directions in the $p$-dimensional space
that capture the maximum amount of variance in the data, and the
eigenvalues indicate the amount of variance captured by each
eigenvector.

#### Select principal components that will be retained

Finally, we project the standardized data matrix $Z$ onto the new basis
vectors to obtain the principal components. Here, we'll calculate the
principal component scores by multiplying the standardized data by the
eigenvectors of all principal components:

$$ Y = ZV $$

where $Y$ is the transformed data matrix, and $V$ is the matrix of
eigenvectors.

```{r}
F_PrComps <- data_std %*% Eigenvectors

head(F_PrComps)
```

The score matrix, $F$, (object `F_PrComps`) allows one to *rotate* the
new data space, so it is represented in relation to the principal
components. For instance, see the figure below:

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align='center', fig.cap = "Relationship between N and P from the `varechem` dataset. The purple and orange diagonal lines represent the first and second principal components, respectively. Purple lines denote the residuals between the data point and the first principal component, while orange segmented lines denote the distance between points and the second principal componenet. Arrows indicate the Eigenvalues (direction) from the principal component analysis done on these two variables."}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(data_std)

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)       

plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0, 
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0, 
#        y0 = 0, 
#        x1 = Eigenvalues[2]*Eigenvectors[1,2], 
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange", 
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0, 
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'purple')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4))


plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0, 
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[2]*Eigenvectors[1,2], 
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange", 
       lwd = 2)


line2 <- c(0, 
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line2)

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'orange')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4)
)

title(xlab = "N",
      ylab = "P",
      outer = TRUE, line = 3)

par(op)
```

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align = 'center', fig.cap = "Relationship between PC1 and PC2 from the principal component analysis done on the P and N variables from the `varechem` dataset. The purple and orange lines represent the first and second principal components, respectively. Purple lines denote the residuals between the data point and the first principal component, while orange segmented lines denote the distance between points and the second principal componenet."}

score <- as.data.frame(F_PrComps)

colnames(score) <- c("PC1", "PC2")

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)


plot(PC2 ~ PC1, 
     col = as.factor(rownames(score)), 
     pch = 19, 
     xlim = c(-2.2, 2.2), ylim = c(-2.2,2.2), xlab='PC1', ylab='PC2',data = score)

abline(h = 0, col = 'purple')
abline(v = 0, col='orange')


perp.segment.horiz <- function(x0, y0){
  x1 <- x0
  y1 <- 0
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1 <- perp.segment.horiz(score[,1], score[,2])

segments(x0 = ss1$x0, x1 = ss1$x1, y0 = ss1$y0, y1 = ss1$y1, col='purple')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)
with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))


plot(PC2 ~ PC1, col=as.factor(rownames(score)), 
     pch = 19, xlim=c(-2.2, 2.2), ylim = c(-2.2,2.2),
     xlab='PC1', ylab='PC2',data=score)

abline(h = 0, col = 'purple')
abline(v = 0, col ='orange')


perp.segment.vert <- function(x0, y0){
  x1 <- 0
  y1 <- y0
  
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1a <- perp.segment.vert(score[,1], score[,2])
segments(x0 = ss1a$x0, x1 = ss1a$x1, y0 = ss1a$y0, y1 = ss1a$y1, col='orange')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)

with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))

title(xlab = "PC1",
      ylab = "PC2",
      outer = TRUE, 
      line = 3)

par(op)
```

## Principal component analysis using package functions

PCA can also be computed using the `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, and `ade4::dudi.pca()` functions.

In a nutshell, this is what we have done:

```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```

How, how does it compare to `stats::prcomp()`?
```{r}
PCA_prcomp <- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp <- prcomp(Y_std)

head(PCA_prcomp$x)
```

And, how does it compare to `stats::princomp()`?
```{r}
PCA_princomp <- princomp(Y_std)

head(PCA_princomp$scores)
```

And to `vegan::rda()`?
```{r}
PCA_vegan_rda <- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
```

`vegan::rda()` is a bit special. It uses alternative scalings. We will not cover them here, but you can study the `vignette("decision-vegan")`.

## Principal component analysis on ecological data

We have implemented PCA on a two-variables dataset, for simplicity.

Let us advance and apply it to our fish species dataset.

For this, we will use the `vegan::rda()` function on the *Hellinger-transformed* fish data and summarise the results:

```{r}
spe.h.pca <- rda(spe.hel)

# summary(spe.h.pca)
```

The first lines of `summary.rda()` tell us about the *Total variance* and *Unconstrained variance* in our model.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[5:8])
```

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(12:16, 21:24)])
```

This is followed by the *Eigenvalues*, and their contribution to the variance.

In fact, if we sum all our *Eigenvalues*, we will obtain the amount of uncostrained variance explained by the analysis!

```{r}
sum(spe.h.pca$CA$eig)
```

The next information is related to the *scaling*, to the *species scores*, and to the *site scores*.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(26:29, 31:32, 34:40, 63:64, 66:72)])
```

*Species* refer to your descriptors (i.e. the columns in your dataset), which here are the fish species.

*Scores* refer to the position of every species along the principal components.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(32, 34:40)])
```

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(64, 66:72)])
```

*Sites* represent the rows in your dataset, which here are the different sites along the *Doubs* river.

This information can be obtained with the `score()` function that we used before:

```{r, eval = FALSE, echo = TRUE}
scores(spe.h.pca,
       display = "species" or "sites")
```

## Condensing data with principal component analysis

Here, we have 27 principal components. However, we can apply algorithms to select the lowest number of principal components that still account for a large variance in the data.

#### Kaiser-Guttman criterion

We can select the principal components that capture more variance than the average explanation of all principal components. We do this by:

1. Extracting the *Eigenvalues* associated to the principal components;

2. Subsetting the *Eigenvalues* above the mean *Eigenvalue*:

```{r}
ev <- spe.h.pca$CA$eig
# ev[ev > mean(ev)]
```

```{r, echo = -1, fig.width=10, fig.height = 5.5}
par(mar=c(1,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

#### Broken-stick model

The broken-stick model retains components that explain more variance than would be expected by randomly dividing the variance into $p$ parts.

```{r}
head(bstick(spe.h.pca))
```

```{r, echo = TRUE, fig.width=4.5, fig.height = 4.5}
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```

## Scaling


All that is left is to discuss *scaling* and to *visualize* our results.

Let us practice and compute a PCA on the standardized environmental variables for the same dataset.

```{r}
env.pca <- rda(env.z)
# summary(env.pca, scaling  = 2)
```

Determine our subset of *Eigenvalues* and their corresponding *Eigenvectors*:

```{r}
ev <- env.pca$CA$eig
```

```{r}
ev[ev>mean(ev)]
```

```{r, echo = FALSE, fig.width=8, fig.height = 5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```


The information computed by the PCA can be represented with *biplots*.

We can produce a *quick and dirty* biplot of the PCA using the function `plot()` in base `R`.

```{r, echo = -1}
par(mar=c(4,4, 0.1,0.1), cex = 1.5)
plot(spe.h.pca)
```

`biplot()` from `base` `R` allows for a better interpretation.

```{r, echo = -1, fig.height=6, fig.width=6.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca)
```

The arrows are plotted to show the directionality and angle of the descriptors in the ordination.

- Descriptors at 180 degrees of each other are negatively correlated;
- Descriptors at 90 degrees of each other have zero correlation;
- Descriptors at 0 degrees of each other are positively correlated.

*Type 2 scaling* (`default`): distances among objects are not approximations of Euclidean distances; angles between descriptor (species) vectors reflect their correlations.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 2)
```

*Type 1 scaling*: attempts to preserve the Euclidean distance (in multidimensional space)
among objects (sites): the angles among descriptor (species) vector are not meaningful.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 1)
```

## Challenge #2

Using everything you have learned, compute a PCA on the mite species abundance data

```{r}
data(mite)
```

Be ready to discuss and answer:

- What are the *most relevant* principal components, i.e. subset them?
- Which groups of sites can you identify?
- Which groups of species are related to these groups of sites?

**Challenge 2 - Solution** <hidden> Your code likely looks like the following.

Compute PCA on the Hellinger-transformed species data

```{r}
mite.spe.hel <- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca <- rda(mite.spe.hel)
```

Apply the Kaiser-Guttman criterion

```{r, eval = F}
ev <- mite.spe.h.pca$CA$eig
ev[ev>mean(ev)]
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, 
       col = "red3", bty = "n")
```

```{r, echo = F, fig.width=5, fig.height=4}
par(mar=c(4,4,2,1), cex = 1.2)
ev <- mite.spe.h.pca$CA$eig
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, col = "red3", 
       bty = "n")
```

```{r, echo = -1, fig.height=6.5, fig.width=7}
par(mar = c(4,4,0.05,0.05), cex = 1.5)
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

</hidden>
