
```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.align = 'center')
```

# Analyse en composantes principales

L'analyse en composantes principales (ACP) est une technique statistique utilisée pour réduire la dimensionnalité d'un ensemble de données tout en conservant la majeure partie de sa variabilité. variabilité. Il s'agit d'une méthode de transformation linéaire qui convertit l'ensemble de variables d'origine en un nouvel ensemble de variables linéairement non corrélées. l'ensemble original de variables en un nouvel ensemble de variables linéairement non corrélées, appelées linéairement non corrélées, appelées composantes principales (CP), qui sont classées par ordre décroissant de variance. par ordre décroissant de variance.

L'ACP a été introduite pour la première fois par Karl Pearson en 1901, qui a développé les fondements mathématiques de la méthode. les fondements mathématiques de la méthode. Plus tard, Harold Hotelling (1933) a fourni une interprétation plus détaillée et plus moderne de la méthode ACP.

L'ACP est devenue l'une des techniques les plus utilisées dans l'analyse des données données en raison de sa capacité à identifier des modèles cachés et à réduire la complexité des données à haute dimension.

En substance, l'ACP vise à trouver les combinaisons linéaires des variables qui représentent la plus grande quantité possible de variation dans l'ensemble l'ensemble des données. Les composantes principales qui en résultent sont orthogonales les unes par rapport aux autres, ce qui signifie qu'elles ne sont pas corrélées entre elles et leur ordre reflète leur importance dans l'explication de la variabilité des données.

Pour vous familiariser avec l'ACP, nous allons suivre des exemples détaillés sur la façon de l'exécuter étape par étape. comment l'effectuer étape par étape, puis nous utiliserons les fonctions des paquets R pour la réaliser.

## L'analyse en composantes principales *pas* en bref

Supposons que nous ayons un ensemble de données avec $n$ observations et $p$ variables représentées par une matrice n x p $X$. L'objectif de l'ACP est de transformer cet ensemble de en un nouvel ensemble de $p$ variables non corrélées, appelées composantes principales composantes principales (CP), qui capturent le maximum de variance dans les données originales. données originales.

#### Charger les données

Dans cet atelier, nous utilisons le jeu de données `data(varechem)`, qui contient des des mesures des propriétés chimiques de 18 échantillons de sol provenant d'une d'une expérience sur le terrain. Nous sélectionnerons les deux premières variables :

```{r}
# Chargement du paquet datasets
library(datasets)

# Charger le jeu de données varechem
data(varechem)

# Sélectionner les données
(data <- varechem[, 1:2])

```

#### Normaliser les données

Nous devons d'abord normaliser les données pour qu'elles aient une moyenne nulle et une variance unitaire :

$$ Z_{ij} = \frac{X_{ij} - \bar{X_j}}{s_j} $$

où Z est la matrice standardisée, X est la matrice originale, $\bar{X_j}$ est la moyenne de la variable j, et $s_j$ est l'écart-type de la variable j.

```{r}
data_std <- scale(data)
```

#### Calculer la matrice de covariance

Ensuite, nous calculons la matrice de covariance de $Z$ :

$$ C = \frac{1}{n-1}ZZ^T $$

où $C$ est la matrice de covariance et $T$ représente l'opération de transposition.

La matrice de covariance est une matrice symétrique qui représente les covariances par paire entre les variables. La formule de la covariance entre deux variables $X$ et $Y$ est la suivante :

$$\text{Cov}(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})$$

où $n$ est la taille de l'échantillon, $X_i$ et $Y_i$ sont les valeurs des variables pour l'observation \$i\$, et $\bar{X}$ et $\bar{Y}$ sont les moyennes de l'échantillon des variables.

```{r}
cov_matrix <- cov(data_std)
```

#### Effectuer la décomposition en valeurs propres de la matrice de covariance

Ensuite, nous calculons les valeurs propres et les vecteurs propres de $C$ :

$$ Cv = \lambda v $$

où $C$ est la matrice de covariance, $v$ est le vecteur propre et $\lambda$ est la valeur propre correspondante.

```{r}
eigen_decomp <- eigen(cov_matrix)
Eigenvalues <- eigen_decomp$values
Eigenvectors <- eigen_decomp$vectors
```

Les vecteurs propres représentent les directions dans l'espace à $p$ dimensions qui capturent le maximum de variance dans les données, et les valeurs propres indiquent la quantité de variance capturée par chaque vecteur propre.

#### Projeter les données normalisées sur l'espace des valeurs propres

Enfin, nous projetons la matrice de données normalisées $Z$ sur les nouveaux vecteurs de base pour obtenir les composantes principales. Ici, nous calculons les scores des composantes principales en multipliant les données normalisées par les vecteurs propres de toutes les composantes principales :

$$ Y = ZV $$

où $Y$ est la matrice des données transformées et $V$ est la matrice des vecteurs propres.

```{r}
F_PrComps <- data_std %*% Eigenvectors

head(F_PrComps)
```

La matrice de score, $F$, (objet `F_PrComps`) permet de *rotationner* le nouvel espace de données, de sorte qu'il soit représenté par rapport aux composantes principales. Par exemple, voir la figure ci-dessous :



```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align='center', fig.cap = "Relation entre N et P à partir de l'ensemble de données `varechem`. Les lignes diagonales violette et orange représentent respectivement la première et la deuxième composante principale. Les lignes violettes indiquent les résidus entre le point de données et la première composante principale, tandis que les lignes orange segmentées indiquent la distance entre les points et la deuxième composante principale. Les flèches indiquent les valeurs propres (direction) de l'analyse en composantes principales effectuée sur ces deux variables."}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(data_std)

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)       

plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0, 
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0, 
#        y0 = 0, 
#        x1 = Eigenvalues[2]*Eigenvectors[1,2], 
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange", 
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0, 
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'purple')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4))


plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0, 
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[2]*Eigenvectors[1,2], 
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange", 
       lwd = 2)


line2 <- c(0, 
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line2)

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'orange')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4)
)

title(xlab = "N",
      ylab = "P",
      outer = TRUE, line = 3)

par(op)
```


```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align = 'center', fig.cap = "Relation entre PC1 et PC2 de l'analyse en composantes principales effectuée sur les variables P et N de l'ensemble de données `varechem`. Les lignes violettes et orange représentent respectivement la première et la deuxième composante principale. Les lignes violettes indiquent les résidus entre le point de données et la première composante principale, tandis que les lignes orange segmentées indiquent la distance entre les points et la deuxième composante principale."}

score <- as.data.frame(F_PrComps)

colnames(score) <- c("PC1", "PC2")

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)


plot(PC2 ~ PC1, 
     col = as.factor(rownames(score)), 
     pch = 19, 
     xlim = c(-2.2, 2.2), ylim = c(-2.2,2.2), xlab='PC1', ylab='PC2',data = score)

abline(h = 0, col = 'purple')
abline(v = 0, col='orange')


perp.segment.horiz <- function(x0, y0){
  x1 <- x0
  y1 <- 0
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1 <- perp.segment.horiz(score[,1], score[,2])

segments(x0 = ss1$x0, x1 = ss1$x1, y0 = ss1$y0, y1 = ss1$y1, col='purple')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)
with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))


plot(PC2 ~ PC1, col=as.factor(rownames(score)), 
     pch = 19, xlim=c(-2.2, 2.2), ylim = c(-2.2,2.2),
     xlab='PC1', ylab='PC2',data=score)

abline(h = 0, col = 'purple')
abline(v = 0, col ='orange')


perp.segment.vert <- function(x0, y0){
  x1 <- 0
  y1 <- y0
  
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1a <- perp.segment.vert(score[,1], score[,2])
segments(x0 = ss1a$x0, x1 = ss1a$x1, y0 = ss1a$y0, y1 = ss1a$y1, col='orange')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)

with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))

title(xlab = "PC1",
      ylab = "PC2",
      outer = TRUE, 
      line = 3)

par(op)
```

## Analyse en composantes principales à l'aide des fonctions du paquet

L'ACP peut également être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

En résumé, voici ce que nous avons fait :


```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```

Comment cela se compare-t-il à `stats::prcomp()` ?


```{r}
PCA_prcomp <- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp <- prcomp(Y_std)

head(PCA_prcomp$x)
```

Et quelle est la comparaison avec `stats::princomp()` ?

```{r}
PCA_princomp <- princomp(Y_std)

head(PCA_princomp$scores)
```


Et avec `vegan::rda()`?

```{r}
PCA_vegan_rda <- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
```

`vegan::rda()` est un peu spéciale. Elle utilise des échelles alternatives. Nous ne les aborderons pas ici, mais vous pouvez étudier la `vignette("decision-vegan")`.

## Analyse en composantes principales sur des données écologiques

Nous avons mis en œuvre l'ACP sur un ensemble de données à deux variables, pour des raisons de simplicité.

Avançons et appliquons-la à notre jeu de données sur les espèces de poissons.

Pour cela, nous allons utiliser la fonction `vegan::rda()` sur les données de poissons *transformées par Hellinger* et résumer les résultats :

```{r}
spe.h.pca <- rda(spe.hel)

# summary(spe.h.pca)
```

Les premières lignes de `summary.rda()` nous renseignent sur la *variance totale* et la *variance non contrainte* de notre modèle.

``{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[5:8])
```

``{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(12:16, 21:24)])
```

Viennent ensuite les *valeurs propres* et leur contribution à la variance.

En fait, si nous additionnons toutes nos *valeurs propres*, nous obtiendrons la quantité de variance non contrainte expliquée par l'analyse !

```{r}
sum(spe.h.pca$CA$eig)
```

Les informations suivantes sont liées à l'*échelle*, aux *scores d'espèces* et aux *scores de sites*.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(26:29, 31:32, 34:40, 63:64, 66:72)])
```

Les *espèces* font référence à vos descripteurs (c'est-à-dire les colonnes de votre jeu de données), qui sont ici les espèces de poissons.

Les *scores* font référence à la position de chaque espèce le long des composantes principales.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(32, 34:40)])
```

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(64, 66:72)])
```

*Sites* représente les lignes de votre jeu de données, qui sont ici les différents sites le long de la rivière *Doubs*.

Cette information peut être obtenue avec la fonction `score()` que nous avons utilisée précédemment :

```{r, eval = FALSE, echo = TRUE}
scores(spe.h.pca,
       display = "species" or "sites")
```


## Condenser les données avec l'analyse en composantes principales

Ici, nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

#### Critère de Kaiser-Guttman

Nous pouvons sélectionner les composantes principales qui capturent plus de variance que l'explication moyenne de toutes les composantes principales. Pour ce faire, nous procédons comme suit

1.  Extraire les *valeurs propres* associées aux composantes principales ;

2.  Sous-ensemble des *valeurs propres* supérieures à la *valeur propre* moyenne :


```{r}
ev <- spe.h.pca$CA$eig
# ev[ev > mean(ev)]
```

```{r, echo = -1, fig.width=10, fig.height = 5.5}
par(mar=c(1,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

#### Modèle à bâtons rompus

Le modèle à bâtons (ou branches) rompus retient les composantes qui expliquent plus de variance que ce que l'on pourrait attendre en divisant aléatoirement la variance en $p$ parties.

```{r}
head(bstick(spe.h.pca))
```

```{r, echo = TRUE, fig.width=4.5, fig.height = 4.5}
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```

## Échelonnement

Il ne nous reste plus qu'à discuter de la *mise à l'échelle* et à *visualiser* nos résultats.

Pratiquons et calculons une ACP sur les variables environnementales standardisées pour le même ensemble de données.

```{r}
env.pca <- rda(env.z)
# summary(env.pca, scaling  = 2)
```

Déterminer notre sous-ensemble de *valeurs propres* et leurs *vecteurs propres* correspondants :


```{r}
ev <- env.pca$CA$eig
```

```{r}
ev[ev>mean(ev)]
```

```{r, echo = FALSE, fig.width=8, fig.height = 5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

Les informations calculées par l'ACP peuvent être représentées par des *biplots*.

Nous pouvons produire un *biplot simple et rapide* de l'ACP en utilisant la fonction `plot()` dans la base `R`.


```{r, echo = -1}
par(mar=c(4,4, 0.1,0.1), cex = 1.5)
plot(spe.h.pca)
```

`biplot()` de `base` `R` permet une meilleure interprétation.

```{r, echo = -1, fig.height=6, fig.width=6.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca)
```

Les flèches sont tracées pour montrer la directionnalité et l'angle des descripteurs dans l'ordination.

- Les descripteurs situés à 180 degrés les uns des autres sont négativement corrélés ;
- Les descripteurs situés à 90 degrés les uns des autres ont une corrélation nulle ;
- Les descripteurs situés à 0 degré les uns des autres sont positivement corrélés.

*Échelle de type 2* (par défaut) : les distances entre les objets ne sont pas des approximations des distances euclidiennes ; les angles entre les vecteurs des descripteurs (espèces) reflètent leurs corrélations.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 2)
```

*Échelle de type 1* : tente de préserver la distance euclidienne (dans l'espace multidimensionnel) entre les objets (sites) : les angles entre les vecteurs des descripteurs (espèces) ne sont pas significatifs.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 1)
```


## Défi #2

En utilisant tout ce que vous avez appris, calculez une ACP sur les données d'abondance des espèces d'acariens

```{r}
data(mite)
```

Préparez-vous à discuter et à répondre :

- Quelles sont les composantes principales les plus pertinentes, c'est-à-dire les sous-ensembles ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?

**Défi 2 - Solution** <hidden> Votre code ressemble probablement à ce qui suit.

Calculer l'ACP sur les données d'espèces transformées par Hellinger

```{r}
mite.spe.hel <- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca <- rda(mite.spe.hel)
```

Appliquer le critère de Kaiser-Guttman

```{r, eval = F}
ev <- mite.spe.h.pca$CA$eig
ev[ev>mean(ev)]
n <- length(ev)
barplot(ev, main = "Valeurs propres", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Valeur propre moyenne", 
       lwd = 2, 
       col = "red3", bty = "n")
```


```{r, echo = F, fig.width=5, fig.height=4}
par(mar=c(4,4,2,1), cex = 1.2)
ev <- mite.spe.h.pca$CA$eig
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Valeur propre moyenne", 
       lwd = 2, col = "red3", 
       bty = "n")
```

```{r, echo = -1, fig.height=6.5, fig.width=7}
par(mar = c(4,4,0.05,0.05), cex = 1.5)
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

</hidden>




