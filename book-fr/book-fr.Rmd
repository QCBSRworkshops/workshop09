--- 
title: "Atelier 9 : Analyses multivariées en `R`"
subtitle: "Série d'ateliers R du CSBQ"
author:
- Développé et entretenu par les contributeurs et les contributrices de la Série d'ateliers R du CSBQ^[La Série d'ateliers R du CSBQ fait partie du [Centre de la science de la biodiversité du Québec](https://www.qcbs.ca), et est maintenue par les coordonnateurs et les coordonnatrices de la série, et les membres étudiants diplômés, postdoctoraux et professionnels de la recherche. **La liste des contributeurs et des contributrices de cet atelier sont accessibles[ici](lien)**].
date: "`r Sys.time()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: ["references.bib"]
biblio-style: apalike
link-citations: yes
cover-image: assets/images/logo/csbq_logo_accueil.png
github-repo: "qcbsRworkshops/workshop09"
description: "Analyses multivariées en `R`"
favicon: "assets/images/favicon.ico"
always_allow_html: yes
url: 'https\://github.com/qcbsRworkshops/'
config:
  edit:
    link: https://github.com/qcbsRworkshops/workshop09/edit/main/book-fr/%s
    text: "Suggest changes"
---
# (PART\*) Série d'ateliers R du CSBQ {-}

# Préface {-}

La **Série d'ateliers R du CSBQ** est une série de 10 ateliers qui guide les participants à travers les étapes nécessaires à l'utilisation de R pour un large éventail d'analyses statistiques pertinentes pour la recherche en biologie et en écologie. Ces ateliers en accès libre ont été créés par des membres du CSBQ, à la fois pour les membres du CSBQ et pour la communauté au sens large.

Le contenu de cet atelier a été revu par plusieurs membres du CSBQ. Si vous souhaitez suggérer des modifications, veuillez contacter les coordinateurs de la série actuelle, dont la liste figure sur la page principale de Github

## Code de conduite

La Série d'ateliers R du CSBQ et le Symposium R du CSBQ sont des lieux dédiés à fournir un environnement accueillant et favorable à toutes les personnes, indépendamment de leurs origines ou de leur identité.

Les participants, les présentateurs et les organisateurs de la série d'ateliers et d'autres activités connexes acceptent le présent code de conduite lorsqu'ils assistent à des activités liées aux ateliers.

Nous ne tolérons pas les comportements irrespectueux ou qui excluent, intimident ou gênent les autres.

Nous ne tolérons pas la discrimination ou le harcèlement fondés sur des caractéristiques telles que, mais sans s'y limiter, l'identité et l'expression du genre, l'orientation sexuelle, le handicap, l'apparence physique, la taille du corps, la citoyenneté, la nationalité, les origines ethniques ou sociales, la grossesse, le statut familial, les informations génétiques, la religion ou les convictions (ou l'absence de celles-ci), l'appartenance à une minorité nationale, la propriété, l'âge, l'éducation, le statut socio-économique, les choix techniques et le niveau d'expérience.

Il s'applique à tous les espaces gérés par l'atelier ou affiliés à celui-ci, y compris, mais sans s'y limiter, les ateliers, les listes de diffusion et les forums en ligne tels que GitHub, Slack et Twitter.

### Comportement attendu

Tous les participants sont tenus de faire preuve de respect et de courtoisie envers les autres. Toutes les interactions doivent être professionnelles, quelle que soit la plateforme utilisée : en ligne ou en personne. 

Afin de favoriser un environnement d'apprentissage positif et professionnel, nous encourageons les types de comportements suivants dans tous les événements et plates-formes des ateliers :

- Utiliser un langage accueillant et inclusif ;
- Respecter les différents points de vue et expériences ;
- Accepter avec grâce les critiques constructives ;
- Se concentrer sur ce qui est le mieux pour la communauté ;
- Faire preuve de courtoisie et de respect envers les autres membres de la communauté.

### Comportements inacceptables

Voici quelques exemples de comportements inacceptables de la part des participants à tout événement ou plateforme d'atelier :

- les commentaires écrits ou verbaux qui ont pour effet d'exclure des personnes sur la base de leur appartenance à un groupe spécifique ;
- faire craindre à quelqu'un pour sa sécurité, par exemple en le harcelant ou en l'intimidant ;
- des menaces ou des propos violents dirigés contre une autre personne ;
- l'affichage d'images sexuelles ou violentes ;
- l'attention sexuelle non désirée ;
- les contacts physiques non consensuels ou non désirés ;
- des insultes ou des rabais ;
- les blagues sexistes, racistes, homophobes, transphobes, incapables ou d'exclusion ;
- l'incitation à la violence, au suicide ou à l'automutilation ;
- la poursuite de l'interaction (y compris la photographie ou l'enregistrement) avec une personne après qu'on - lui a demandé d'arrêter ;
- la publication d'une communication privée sans consentement.

## Contributeurs et contributrices

```{r eval=FALSE, include=FALSE}
CETTE SECTION NE DOIT ÊTRE ÉDITÉE QUE PAR LES COORDINATEURS DE LA SÉRIE D'ATELIERS R
```

Développé à l'origine par Bérenger Bourgeois, Xavier Giroux-Bougard, Amanda Winegardner, Emmanuelle Chrétien, Monica Granados et Pedro Henrique Pereira Braga. Le matériel contenu dans le script R provient de Borcard, Gillet & Legendre (2011). Numerical Ecology with R. Springer New York.

Depuis 2014, plusieurs membres du CSBQ ont contribué à développer et à mettre à jour cet atelier de manière cohérente et collaborative, dans le cadre de la bourse d'apprentissage et de développement du CSBQ. Il s'agit de :

|      2025 - 2024 - 2023      |       2022 - 2021 - 2020      |      2019 - 2018 - 2017     |      2016 - 2015 - 2014      |
|:----------------------------:|:----------------------------:|:---------------------------:|:----------------------------:|
|  Pedro Henrique P. Braga  | Pedro Henrique P. Braga  | Gabriel Muñoz | Monica Granados |
|                              |  Katherine Hébert        | Marie Hélène-Brice  |   Emmanuelle Chrétien        |
|                              |  Mi Lin                  | Pedro Henrique P. Braga | Bérenger Bourgeois     |
|                              |  Linley Sherin           |                             | Amanda Winegardner       |
|                              |                              |                             | Xavier Giroux-Bougard    |
|                              |                              |                             | Vincent Fugère           |
|                              |                              |                             | Zofia Taranu          |


## Contribuez à la série!

En construction.

```{r include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60),
               tidy = TRUE)
```

<!--chapter:end:index.Rmd-->

# (PART\*) Analyses multivariées en `R` {-}

# Objectifs d'apprentissage

1. Apprendre les bases de l'analyse multivariée pour révéler des modèles dans les données sur la composition des communautés.

2. Utiliser `R` pour effectuer une ordination sans contrainte

3. Apprendre à connaître les coefficients de similarité et de dissimilarité et les transformations pour effectuer une analyse multivariée

4. Utiliser `R` pour créer des dendrogrammes

5. Apprenez les méthodes suivantes :

  * Analyse de regroupement
  * Analyse en composantes principales (ACP)
  * Analyse des coordonnées principales (PCoA)
  * Échelle multidimensionnelle non métrique (NMDS)
  
# Préparez-vous pour cet atelier

```{r eval=FALSE, echo=FALSE, purl=TRUE}
##### Avis ###
###                                                                             #
### Ceci est un script généré automatiquement basé sur les morceaux de code du  #
### livre pour cet atelier.                                                     #
###                                                                             #
### Il est minimalement annoté pour permettre aux participants de fournir leurs #
### commentaires : une pratique que nous encourageons vivement.                 #
###                                                                             #
### Notez que les solutions aux défis sont également incluses dans ce script.   #
### Lorsque vous résolvez les défis par vous-méme, essayez de ne pas parcourir  #
### le code et de regarder les solutions.                                       #
###                                                                             # 
### Bon codage !                                                               #

```


Pour préparer cet atelier, vous devez suivre les étapes suivantes :

Télécharger les données nécessaires à cet atelier :

- [Données de DoubsEnv](http://qcbs.ca/wiki/_media/DoubsEnv.csv)
- [Données de DoubsSpe](http://qcbs.ca/wiki/_media/DoubsSpe.csv)


Leurs données peuvent également être récupérées à partir du paquet `ade4` :

```{r eval=FALSE, echo = TRUE}
library (ade4)
data (doubs)

spe <- doubs$fish
env <- doubs$env
```

Alternativement, à partir du paquet `codep` :

```{r eval=FALSE, echo = TRUE}
library (codep)
data (Doubs)

spe <- Doubs.fish
env <- Doubs.env
```


Téléchargez le script contenant la fonction `coldiss()` :

- [`R` script](http://qcbs.ca/wiki/_media/multivar1_e.r)
- [R` script contenant la fonction `coldiss()`](http://qcbs.ca/wiki/_media/coldiss.R)

Vous devez également utiliser ces paquets :

- [ape](https://cran.r-project.org/package=ape)
- [ade4](https://cran.r-project.org/package=ade4)
- [codep](https://cran.r-project.org/package=codep)
- [gclus](https://cran.r-project.org/package=gclus)
- [vegan](https://cran.r-project.org/package=vegan)
- [GGally](https://cran.r-project.org/package=GGally)
- [PlaneGeometry](https://cran.r-project.org/package=PlaneGeometry)
- [remotes](https://cran.r-project.org/package=remotes)
- [MASS](https://cran.r-project.org/package=MASS)


```{r preparing, eval=T, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
list.of.packages <- c("ape", "ade4", "codep", 
                      "gclus", "vegan", "GGally", 
                      "PlaneGeometry", "remotes", 
                      "matlib",
                      "MASS")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages) > 0) {
  install.packages(new.packages, dependencies = TRUE) 
  print(paste0("The following package was installed:", new.packages)) 
} else if(length(new.packages) == 0) {
  print("All packages were already installed previously")
}

# Chargement de toutes les bibliothèques nécessaires en une seule fois
invisible(lapply(list.of.packages, library, character.only = TRUE, quietly = TRUE))

# source(file.choose()) #  utilisez coldiss.R que vous avez téléchargé dans votre propre répertoire
```

<!--chapter:end:01-preparation-pour-l-atelier.Rmd-->

# (PART\*) PRÉAMBULE {-}

# Récapitulatif : Analyses univariées

Nous avons appris une multitude d'analyses qui nous ont permis d'interpréter des données écologiques en décrivant les effets d'une ou de plusieurs variables sur une variable réponse.

Nous pouvons rappeler les :

1. Les modèles linéaires généraux, dont nous avons utilisé les fonctions :
  1. `lm()` ;
  2. `anova()` ;
  3. `t.test()` ;
  4. `lmer()`.

2. Modèles linéaires généralisés, où nous avons appris à les appliquer à l'aide de `t.test()` ; 4. `lmer()` :
  1. `glm()` et `glmer()` avec plusieurs fonctions de liaison `family()`.

3. Modèles Additifs Généralisés, avec les fonctions de liaison `family()` et `glm()` : 
  1. `gam()`.

Ces modèles nous ont permis de poser des questions telles que :

1. Quels sont les effets des précipitations et de la température sur la richesse des espèces ?
2. Comment l'abondance des microbes change-t-elle d'un hôte à l'autre ?
3. Les poissons cooccurrents deviennent-ils plus agressifs après avoir été incités à avoir peur ?

Cependant, il peut être intéressant de faire des déductions à partir de données écologiques contenant "plus d'un" résultat ou d'une variable dépendante.

Cet intérêt peut être motivé par la vérification d'hypothèses et la modélisation, mais il peut aussi être entièrement exploratoire.

# Intro : Analyses multivariées

Par exemple, notre question de recherche pourrait être la suivante

1. Comment la _composition bactérienne_ des feuilles d'érable change-t-elle le long du gradient d'altitude ?

2. Quelle est la _dissemblance compositionnelle_ des communautés de chauves-souris ?

3. Quelle est la _proximité_ des communautés locales d'araignées par rapport à leur _composition_ ?

Dans tous ces cas, le résultat est composé de plusieurs variables, _par exemple_ habituellement une matrice échantillon par espèce ou échantillon par environnement.

# Définir nos objectifs

Nous allons maintenant nous plonger dans les **statistiques multivariées**, un ensemble d'outils qui nous permettra d'aborder des questions nécessitant l'observation ou l'analyse simultanée de plus d'une variable de résultat.

Nous explorerons certaines méthodes, telles que:

1. Mesures et matrices d'association (ou de dissemblance) ;

2. L'analyse de classification (ou de regroupement) ;

3. Ordination sans contrainte ;

4. Ordination contrainte (ou canonique) (dans l'atelier 10).

Avant tout, nous allons faire un petit rappel sur l'algèbre des matrices.

<!--chapter:end:02-introduction-fr.Rmd-->

# L'algèbre matricielle, très brièvement

L'algèbre matricielle est bien adaptée à l'écologie, car la plupart (sinon tous) des _ensembles de données_ avec lesquels nous travaillons sont dans un format _matrix_.

## Les ensembles de données _sont_ des matrices

Les tableaux de données écologiques sont obtenus sous forme d'observations d'objets ou d'unités d'échantillonnage, et sont souvent enregistrés sous cette forme : 

| Objets | $y_1$     | $y_2$  | $\dots$           | $y_n$  |
| :-------------: |:-------------:| :-----:|:-----:|:-----:|
|  $x_1$        | $y_{1,1}$     | $y_{1,2}$  | $\dots$  | $y_{1,n}$  |
|  $x_2$        | $y_{2,1}$     | $y_{2,2}$  | $\dots$  | $y_{2,n}$  |
|  $\vdots$     | $\vdots$     | $\vdots$  | $\ddots$  | $\vdots$  |
|  $x_m$        | $y_{m,1}$     | $y_{m,2}$  | $\dots$  | $y_{m,n}$  |


où $x_m$ est l'unité d'échantillonnage $m$ ; et $y_n$ est le descripteur écologique qui peut être, par exemple, les espèces présentes dans une unité d'échantillonnage, une localité ou une variable chimique.

Le même tableau de données écologiques peut être représenté en _notation matricielle_ de la manière suivante :

$$Y = [y_{m,n}] =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}$$

où les lettres minuscules indiquent les _éléments_, et les lettres en indice indiquent la _position de ces éléments_ dans la matrice (et dans le tableau !).

De plus, tout sous-ensemble d'une matrice peut être reconnu.

Nous pouvons sous-ensembler _une matrice de lignes_, comme ci-dessous :

$$\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}$$

Nous pouvons également sous-ensemble _une matrice de colonnes_, comme ci-dessous :


$$\begin{bmatrix}
y_{1,1} \\ y_{2,2} \\ \vdots \\ y_{m,2}
\end{bmatrix}$$


## Matrices d'association

Deux matrices importantes peuvent être dérivées de la matrice des données écologiques : la _**matrice d'association entre les objets**_ et la _**matrice d'association entre les descripteurs**_.

En utilisant les données de notre matrice $Y$,


<div class="math">
\[ Y = 
\begin{array}{cc}
\begin{array}{ccc}
x_1 \rightarrow\\
x_2 \rightarrow\\
\vdots \\
x_m \rightarrow\\
\end{array}
&
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}
\end{array}
\]
</div>

on peut examiner la relation entre les deux premiers objets :

<div class="math">
\[x_1 \rightarrow \begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}
\]
</div>

<div class="math">
\[x_2 \rightarrow 
\begin{bmatrix}
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\end{bmatrix}
\]
</div>

<p>et obtenir \(a_{1,2}\). </p>

Nous pouvons remplir la matrice d'association $A_{n,n}$ avec les relations entre tous les objets de $Y$ :

<div class="math">
\[A_{n,n} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{n,2} & \cdots & a_{n,n} 
\end{bmatrix}\]
</div>

<p>Parce que \(A_{n,n}\) a le même nombre de lignes et de colonnes, on la désigne comme une <i>matrice carrée</i>.</p> <p>Donc \(A_{n,n}\) a \(n^2\) éléments. 

<p> Par conséquent, \(A_{n,n}\) a \(n^2\) éléments. </p>

Nous pouvons également obtenir la relation entre les deux premiers descripteurs de $Y$, $y_1$ et $y_2$ :


$$\begin{bmatrix}
y_{1,2} \\
y_{2,2} \\
\vdots  \\
y_{m,2} 
\end{bmatrix}$$

$$\begin{bmatrix}
y_{1,1} \\
y_{2,1} \\
\vdots  \\
y_{m,1} 
\end{bmatrix}$$

et le stocker dans $a_{1,2}$.

Nous pouvons remplir la matrice d'association $A_{m,m}$ avec les relations entre tous les descripteurs de $Y$ :

$$A_{m,m} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m} 
\end{bmatrix}$$

<p>Cette \(A_{m,m}\) est une <i>matrice carrée</i>, et elle a \(m^2\) éléments.</p>

Ces matrices, \N(A_{n,n}\N) et \N(A_{m,m}\N), sont à la base des analyses **_Q-mode_** et **_R-mode_** en écologie.

La **_R-mode_** consiste à analyser l'association entre des descripteurs ou des espèces, tandis que la **_Q-mode_** analyse l'association entre des OTU, des objets ou des sites.

# Exploration de l'ensemble de données

Nous utiliserons deux jeux de données principaux dans la première partie de cet atelier.

Ils proviennent de la thèse de doctorat de Verneaux (1973), dans laquelle il a proposé d'utiliser les espèces de poissons pour caractériser les zones écologiques le long des rivières et des ruisseaux européens.

Il a recueilli des données dans **30 localités** le long du Doubs, qui coule près de la frontière franco-suisse, dans les montagnes du Jura.

Il a montré que les communautés de poissons étaient des indicateurs biologiques de ces masses d'eau.

Leurs données sont réparties en trois matrices :

1. L'abondance de 27 espèces de poissons dans les communautés (`DoubsSpe.csv` et ci-après, l'objet `spe`) ;
2. Les variables environnementales enregistrées sur chaque site (`DoubsEnv.csv` et ici, l'objet `env`) ; et,
3. Les coordonnées géographiques de chaque site.

Verneaux, J. (1973) _Cours d'eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs_. Essai de biotypologie. Thèse d'état, Besançon. 1-257.

## Communautés de poissons du Doubs 

Vous pouvez télécharger ces jeux de données à partir de [r.qcbs.ca/workshops/r-workshop-09] (http://r.qcbs.ca/workshops/r-workshop-09/).

Nous pouvons charger leurs données à partir du répertoire `data/` dans cet atelier :

```{r}
spe <- read.csv("data/doubsspe.csv", 
                row.names = 1) 

env <- read.csv("data/doubsenv.csv", 
                row.names = 1)
```

Leurs données peuvent également être récupérées à partir du paquet `ade4` :

```{r eval=FALSE, echo = TRUE}
library (ade4)
data (doubs)

spe <- doubs$fish
env <- doubs$env
```

Alternativement, à partir du paquet `codep` :

```{r eval = FALSE, echo = TRUE}
library (codep)
data (Doubs)

spe <- Doubs.fish
env <- Doubs.env
```

Nous pouvons alors explorer les objets contenant nos données nouvellement chargées. 

Jetons un coup d'œil aux données `spe` :

```{r, echo = T}
head(spe)[, 1:8]
```

Nous pouvons également utiliser la fonction `str()`, que nous avons étudiée dans les ateliers 1 et 2 :

```{r, echo = T, output.lines=1:8}
str(spe)
```

Vous pouvez également essayer certains de ces exemples !

```{r, echo = TRUE, eval = FALSE}
# Essayez-en quelques-uns !

names(spe) # noms des objets
dim(spe) # dimensions

str(spe) # structure des objets
summary(spe) # statistiques sommaires

head(spe) # 6 premières lignes
``` 

## Données environnementales de la rivière Doubs

```{r, echo = T}
str(env)
```

Il contient les variables suivantes:

|Variable |Description|
|:--:|:--|
|das|Distance from the source [km]  |
|alt|Altitude [m a.s.l.]  |
|pen|Slope [per thousand]  |
|deb|Mean min. discharge [m<sup>3</sup>s<sup>-1</sup>]  |
|pH|pH of water  |
|dur|Ca conc. (hardness) [mgL<sup>-1</sup>]  |
|pho|K conc. [mgL<sup>-1</sup>]  |
|nit|N conc. [mgL<sup>-1</sup>]  |
|amn|NH₄⁺ conc. [mgL<sup>-1</sup>]  |
|oxy|Diss. oxygen [mgL<sup>-1</sup>]  |
|dbo|Biol. oxygen demand [mgL<sup>-1</sup>]  |

Vous pouvez également utiliser `summary()` pour obtenir des statistiques sommaires à partir des variables de `env` :

```{r, eval = F}
summary(env) # statistiques sommaires
```

<!--chapter:end:03-exploration-des-donnees.Rmd-->

# (PART\*) Mesures d'association {.unnumbered}

# `dist()`

La ressemblance des communautés est presque toujours évaluée sur la base de données relatives à la composition des espèces, sous la forme d'un tableau de données site par site $Y_{m,n}$.

Nous pouvons obtenir une matrice d'association $A_{m,m}$ sous la forme de distances ou de dissimilarités par paire $D_{m,m}$ (ou de similitudes $S_{m,m}$), puis analyser ces distances. Les matrices d'association entre objets ou entre descripteurs permettent de calculer la similarité ou les distances entre objets ou descripteurs (Legendre et Legendre 2012).

Dans `R`, nous pouvons calculer des matrices de distance ou de dissimilarité en utilisant `stats::dist()`. Pour plus de simplicité, nous allons le faire sans spécifier d'arguments :

```{r, eval = T, results='hide'}
dist(spe)
```

Exécutez `dist(spe)` de votre côté, et vous devriez observer que la sortie de \`dist(spe) est une *matrice triangulaire inférieure* représentant les associations par paires entre les colonnes de votre matrice originale.

Voyons ce que les commandes ci-dessous nous montrent :

```{r, eval = T}
class(dist(spe))
```

La sortie de `dist()` est un objet de la classe `dist` par défaut. Cet objet est composé d'un vecteur qui contient le triangle inférieur de la matrice de distance, réparti sur les colonnes. Vous pouvez le transformer en matrice avec `as.matrix()`, comme on le voit ci-dessous :

```{r, eval = F}
as.matrix(dist(spe))
```

Notamment, vous pouvez forcer une matrice qui contient des distances ($D_{m,m}$) en utilisant `as.dist()`.

Vous pouvez également explorer la structure et les dimensions de notre objet de classe `dist` et de notre matrice de distance :

```{r, eval = F}
str(dist(spe))
```

```{r, eval = T}
dim(as.matrix(dist(spe)))
```

# Types de coefficients de distance

Il existe trois groupes de coefficients de distance : *métriques*, *sémimétriques* et *nonmétriques*.

## Distances métriques

Le premier groupe est constitué des *métriques*, et ses coefficients satisfont aux propriétés suivantes :

1.  minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ; ✅

2.  positivité : si $a \neq b$, alors $D(a,b) > 0$ ; ✅

3.  symétrie : $D(a,b) = D(b,a)$ ; ✅

4.  inégalité triangulaire : $D(a,b) + D(b,c) \geq D(a,c)$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien est égale ou supérieure au troisième côté. ✅

Nous pouvons repérer toutes ces propriétés ci-dessous :

```{r, eval = T}
as.matrix(dist(spe))[1:6, 1:6]
```

### Distances euclidiennes

La mesure de distance métrique la plus courante est la *distance euclidienne*.

La distance euclidienne est une mesure de la distance entre deux points dans l'espace euclidien. En deux dimensions, la distance euclidienne entre deux points (x1, y1) et (x2, y2) peut être calculée à l'aide du théorème de Pythagore :

$$D_{1} (x_1,x_2) = \sqrt{\sum_{j=1}^p(y_{1j} - y_{2j})^2}$$

```{r echo=FALSE}
#setting up the plot
xlim <- c(0,4)
ylim <- c(-1,5)
par(mar=c(1,1,1,1)+.1)
plot(xlim, 
     ylim, type="n", 
     xlab="X1", 
     ylab="X2", 
     asp=1)
grid()
# define some vectors
a=c(4, 0)
b=c(0, 4)
# plot the vectors
vectors(a, labels="D(y21, y11)", pos.lab=3, frac.lab=.5, col="grey")
vectors(a + b, labels="D1(x1,x2)", pos.lab=4, frac.lab=.5, col="red")
# vector a+b starting from a is equal to b.
vectors(a + b, labels="D(y12, y22)", pos.lab=4, frac.lab=.5, origin=a, col="grey")

points(x = 4, y = 0, type = "p")
text(x=4.1, y=-0.2, labels="")

points(x = 0, y = 0, type = "p")
text(x=-0.1, y=-0.2, labels="x1")

points(x = 4, y = 4, type = "p")
text(x=4.1, y=4.2, labels="x2")
```

La distance euclidienne est une mesure couramment utilisée dans les analyses multivariées, car elle offre un moyen simple et intuitif de mesurer la distance ou la similarité entre les observations dans un espace multidimensionnel.

En utilisant `stats::dist()`, nous pouvons la calculer avec :

```{r, eval = T}
spe.D.Euclid <- dist(x = spe,  
                     method = "euclidean")
```

Et nous pouvons tester si une distance est euclidienne en utilisant :

```{r, eval = T}
is.euclid(spe.D.Euclid)
```

### Défi #1

**Votre tour!** En utilisant la fonction `dist()`, calculez la matrice de distance euclidienne $D_{hmm}$ pour les abondances d'espèces par matrice de site $Y_{hmm}$ ci-dessous :

| Sites | $y_1$ | $y_2$ | $y_3$ |
|:-----:|:-----:|:-----:|:-----:|
| $s_1$ |   0   |   4   |   8   |
| $s_2$ |   0   |   1   |   1   |
| $s_3$ |   1   |   0   |   0   |

```{r, eval = T}
Y.hmm <- data.frame(
  y1 = c(0, 0, 1),
  y2 = c(4, 1, 0),
  y3 = c(8, 1, 0))
```

Après cela, examinez les chiffres, réfléchissez-y de manière critique !

**Solution:**

Vous devriez obtenir quelque chose de similaire à ceci :

```{r, eval = T}
Y.hmm.DistEu <- dist(x = Y.hmm,  
                     method = "euclidean")

as.matrix(Y.hmm.DistEu)
```

*Maintenant, regardez la composition et les distances entre les sites* $s_2$ et $s_3$ et entre $s_1$ et $s_2$. Que se passe-t-il ?

La distance euclidienne entre les sites $s_2$ et $s_3$, qui n'ont aucune espèce en commun, est plus petite que la distance entre $s_1$ et $s_2$, qui partagent les espèces $y_2$ et $y_3$ ( !).

D'un point de vue écologique, il s'agit d'une évaluation problématique de la relation entre les sites.

Ce problème est connu sous le nom de **problème du double zéro**, \*c'est-à-dire que les doubles zéros sont traités de la même manière que les doubles présences, de sorte que les doubles zéros réduisent la distance entre deux sites.

Les distances euclidiennes ( $D_1$ ) ne doivent donc pas être utilisées pour comparer des sites sur la base de l'abondance des espèces.

### Distances d'accord

Orlóci (1967) a proposé la *distance de corde* pour analyser la composition des communautés.

La distance de corde, également connue sous le nom de distance angulaire ou distance de grand cercle, est une mesure de la distance entre deux points sur une sphère, telle que la Terre.

Elle se compose de :

1\. Normaliser les données, *c'est-à-dire* mettre à l'échelle les vecteurs de site à la longueur 1 en divisant les abondances des espèces dans un échantillon donné par la somme à racine carrée des abondances carrées dans tous les échantillons, comme suit

$$y'_{Uj}=y_{Uj}/\sum^s_{j=1}{y^2_{Uj}}$$

2\. Calculer les distances euclidiennes sur ces données normalisées :

$$D_{3} (x_1,x_2) = \sqrt{\sum_{j=1}^p(y'_{1j} - y'_{2j})^2}$$

Nous pouvons utiliser `vegan::vegdist()` pour cela :

```{r, eval = T}
spe.D.Ch <- vegdist(spe,  
                    method = "chord")

as.matrix(spe.D.Ch)[1:3, 1:3]
```

Lorsque deux sites partagent les mêmes espèces dans les mêmes proportions du nombre d'individus, la valeur de $D_3$ est de 0\$, et lorsqu'aucune espèce n'est partagée, sa valeur est de $\sqrt{2}$.

*Que se passe-t-il si nous calculons les distances d'accord dans la même matrice site par espèce* $Y_{hmm}$ ?

Essayons de calculer les distances d'accord dans la même matrice que celle utilisée pour le défi n° 1 :

```{r, eval = T}
Y.hmm.DistCh <- vegdist(Y.hmm,  
                    method = "chord")
```

```{r echo=TRUE}
as.matrix(Y.hmm.DistCh)
```

Comparons maintenant avec ce que nous avons obtenu en utilisant les distances euclidiennes :

```{r echo=TRUE}
as.matrix(Y.hmm.DistEu)
```

Voyez à nouveau à quoi ressemble notre matrice :

```{r, eval = T}
Y.hmm
```

Ainsi, l'ajout d'un nombre quelconque de doubles zéros à une paire de sites *ne change pas* la valeur de $D_3$. Par conséquent, les *distances de corde* peuvent être utilisées pour comparer des sites décrits par des abondances d'espèces !

### Coefficient de Jaccard

Un autre coefficient d'association populaire est le *coefficient de similarité de Jaccard* (1900).

Le coefficient de similarité de Jaccard a été proposé à l'origine par le mathématicien français Paul Jaccard en 1901, dans le contexte de l'écologie. Jaccard s'intéressait à la comparaison de la composition en espèces de différentes communautés végétales et a proposé l'indice de Jaccard comme mesure de la similarité entre deux communautés sur la base de leur richesse en espèces.

Le coefficient de similarité de Jaccard n'est approprié que pour les **données binaires**, et son coefficient de distance est défini par la taille de l'intersection divisée par la taille de l'union des ensembles d'échantillons.

$$D_{7}(x_1,x_2) = 1 - \frac{\vert x_1 \cap x_2 \vert}{\vert x_1 \cup x_2 \vert} = 1 - \frac{\vert x_1 \cap x_2 \vert}{\vert x_1 \vert + \vert x_2 \vert - \vert x_1 \cap x_2 \vert} = 1-\frac{a}{a+b+c}$$

où,

-   $a$ est le nombre d'espèces partagées entre $x_1$ et $x_2$ qui sont codées $1$ ;
-   $b$ est le nombre d'occurrences où l'on sait que $x_1$ et $x_2$ sont différents ;
-   c\$ est le nombre d'absences communes entre $x_1$ et $x_2$, \*c.-à-d. toutes deux 0\$.

Par exemple, pour les sites $x_1$ et $x_2$ :

| $x_1,x_2$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|:---------:|:-----:|:-----:|:-----:|:-----:|:-----:|
|   $x_1$   |   0   |   1   |   0   |   1   |   0   |
|   $x_2$   |   0   |   1   |   1   |   1   |   1   |

On peut donc calculer $a$, $b$ et $c$ : - $a$ = 1 + 1 = 2

-   $b$ = 1 + 1 = 2

-   $c$ = 1

Et donc notre coefficient de distance :

$$D_{7}(x_1,x_2) = 1-\frac{2}{2+2+1}= 0.6$$

Dans `R`, vous pouvez utiliser la fonction `vegan::vegdist()` pour calculer le coefficient de Jaccard :

```{r}
spe.D.Jac <- vegdist(spe, 
                     method = "jaccard",
                     binary = TRUE)
```

## Distances semimétriques

Le deuxième groupe est constitué des *distances sémimétriques*, et elles violent la propriété de l'*inégalité des triangles* :

1.  minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ; ✅

2.  positivité : si $a \neq b$, alors $D(a,b) > 0$ ; ✅

3.  symétrie : $D(a,b) = D(b,a)$ ; ✅

4.  ~~inégalité des triangles~~ : ${D(a,b) + D(b,c) \geq ou < D(a,c)}$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien n'est *pas* égale ou supérieure au troisième côté. ❌

### Coefficient de Sørensen

Tous les paramètres du *coefficient de similarité de Jaccard* ont le même poids.

$$D_{7}(x_1,x_2)=1-\frac{a}{a+b+c}$$

Cependant, vous pouvez considérer que la présence d'une espèce est plus informative que son absence.

La distance correspondant au *coefficient de similarité de Sørensen* (1948) donne du poids aux doubles présences :

$$D_{13}(x_1,x_2)=1-\frac{2a}{2a+b+c}=\frac{b+c}{2a+b+c}$$

où,

-   $a$ est le nombre d'espèces partagées entre $x_1$ et $x_2$ qui sont codées $1$ ;
-   $b$ est le nombre d'occurrences où l'on sait que $x_1$ et $x_2$ sont différents ;
-   $c$ est le nombre d'absences communes entre $x_1$ et $x_2$, \*c'est-à-dire les deux 0\$.

Dans `R`, vous pouvez également utiliser la fonction `vegan::vegdist()` pour calculer le coefficient de Sørensen :

```{r}
spe.D.Sor <- vegdist(spe, 
                     method = "bray",
                     binary = TRUE)
```

> Comme les coefficients de Jaccard et de Sørensen ne sont appropriés que pour les données de présence-absence, vous devez effectuer une transformation binaire des données d'abondance en utilisant `binary = TRUE` dans `vegdist()`.

### Coefficient de Bray-Curtis

Le *coefficient de dissimilarité de Bray-Curtis* est une version modifiée de l'indice de Sørensen et tient compte de l'abondance des espèces :

$$D_{14}(x_1,x_2)=\frac{\sum{\vert y_{1j}-y_{2j}\vert}}{\sum{( y_{1j}+y_{2j})}}=$$

$$D_{14}(x_1,x_2)=1 - \frac{2W}{A+B}$$

où,

-   $W$ est la somme des abondances les plus faibles de chaque espèce trouvée entre les sites $x_1$ et $x_2$ ;
-   $A$ est la somme de toutes les abondances dans $x_1$ ; et,
-   $B$ est la somme de toutes les abondances dans $x_2$.

Par exemple, pour les sites $x_1$ et $x_2$ :

| $x_1,x_2$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|:---------:|:-----:|:-----:|:-----:|:-----:|:-----:|
|   $x_1$   |  *2*  |  *1*  |  *0*  |   5   |   2   |
|   $x_2$   |   5   |  *1*  |   3   |  *1*  |   1   |

Alors:

-   $W = 2 + 1 + 0 + 1 + 1 = 5$
-   $A = 2 + 1 + 0 + 5 + 0 = 8$
-   $B = 5 + 1 + 3 + 1 + 2 = 12$

$$D_{14}(x_1,x_2) = 1-\frac{2 \times 5}{8+12} = 0.5$$

Pour calculer le *coefficient de dissimilarité de Bray-Curtis*, qui peut prendre en compte les abondances, vous devez définir `binary = FALSE`.

```{r}
spe.db.pa <- vegdist(spe, 
                      method = "bray",
                      binary = FALSE)

spe.db <- as.matrix(spe.db.pa)
```

## Distances non métriques

Les distances non métriques ne satisfont pas les propriétés métriques de symétrie, d'inégalité des triangles et d'identité des indiscernables :

1.  minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ; ✅

2.  ~~positivité :~~ si $a \neq b$, alors $D(a,b) > ou < 0$ ; ❌

3.  symétrie : $D(a,b) = D(b,a)$ ; ✅

4.  ~~inégalité des triangles~~ : ${D(a,b) + D(b,c) \geq ou < D(a,c)}$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien n'est *pas* égale ou supérieure au troisième côté. ❌

### Distance de Mahalanobis

La distance de Mahalanobis entre un point $x$ et un groupe de points de moyenne $\mu$ et de covariance $\Sigma$ est définie comme suit :

$$
D_{M}(x, \mu)=\sqrt{(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$ où $T$ représente la transposition et $\Sigma^{-1}$ est l'inverse (ou l'inverse généralisé) de la matrice de covariance $\Sigma$.

La distance de Mahalanobis est une mesure de la distance entre un point et un groupe de points, qui tient compte de la structure de covariance des données.

L'inverse de la matrice de covariance peut ne pas exister dans certains cas, par exemple lorsque les variables sont linéairement dépendantes ou lorsqu'il y a plus de variables que d'observations. Dans ces cas, nous pouvons utiliser l'inverse généralisé de la matrice de covariance au lieu de l'inverse pour calculer la distance de Mahalanobis.

Cette inverse généralisée peut être calculée à l'aide de différentes méthodes, telles que la pseudo-inverse de Moore-Penrose ou la décomposition en valeurs singulières.

```{r}
# Créer une matrice
x <- matrix(rnorm(100*3), ncol = 3)

# Calculer la matrice de covariance et son inverse généralisé
cov_mat <- cov(x)
cov_inv <- MASS::ginv(cov_mat)

# Calculer la distance de Mahalanobis en utilisant l'inverse généralisé
mah_dist <- mahalanobis(x, 
                        colMeans(x), 
                        cov_inv)

# Imprimer la distance de Mahalanobis
mah_dist
```

## Représentation des matrices de distance

Nous pouvons créer des représentations graphiques des matrices d'association à l'aide de la fonction `coldiss()` :

```{r, echo = TRUE}
# coldiss() function
# Color plots of a dissimilarity matrix, without and with ordering
#
# License: GPL-2 
# Author: Francois Gillet, 23 August 2012
#

"coldiss" <- function(D, nc = 4, byrank = TRUE, diag = FALSE)
{
  require(gclus)
  
  if (max(D)>1) D <- D/max(D)
  
  if (byrank) {
    spe.color <- dmat.color(1-D, cm.colors(nc))
  }
  else {
    spe.color <- dmat.color(1-D, byrank=FALSE, cm.colors(nc))
  }
  
  spe.o <- order.single(1-D)
  speo.color <- spe.color[spe.o, spe.o]
  
  op <- par(mfrow=c(1,2), pty="s")
  
  if (diag) {
    plotcolors(spe.color, rlabels=attributes(D)$Labels, 
               main="Dissimilarity Matrix", 
               dlabels=attributes(D)$Labels)
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o], 
               main="Ordered Dissimilarity Matrix", 
               dlabels=attributes(D)$Labels[spe.o])
  }
  else {
    plotcolors(spe.color, rlabels=attributes(D)$Labels, 
               main="Dissimilarity Matrix")
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o], 
               main="Ordered Dissimilarity Matrix")
  }
  
  par(op)
}

# Utilisation :
# coldiss(D = dissimilarity.matrix, nc = 4, byrank = TRUE, diag = FALSE)

# Si D n'est pas une matrice de dissimilarité (max(D) > 1), alors D est divisée par max(D)
# nc nombre de couleurs (classes)
# byrank = TRUE classes de taille égale
# byrank = FALSE intervalles de longueur égale
# diag = TRUE imprime les étiquettes des objets également sur la diagonale

# Exemple :
# coldiss(spe.dj, nc=9, byrank=F, diag=T)
```

```{r, fig.height = 8, fig.width = 16}
coldiss(spe.D.Jac)
```

Vous pouvez également utiliser `ggplot2::ggplot()` pour représenter votre matrice en utilisant `geom_tile()` :

```{r,  fig.height = 8, fig.width = 9}
# obtenir l'ordre des lignes et des colonnes
order_spe.D.Jac <- hclust(spe.D.Jac, method = "complete")$order

# réorganiser la matrice pour produire une figure ordonnée par similarités
order_spe.D.Jac_matrix <- as.matrix(spe.D.Jac)[order_spe.D.Jac, order_spe.D.Jac]

# converts to data frame
molten_spe.D.Jac <- reshape2::melt(
  as.matrix(order_spe.D.Jac_matrix)
  )

# créer un objet ggplot
ggplot(data = molten_spe.D.Jac, 
       aes(x = Var1, y = Var2, 
           fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black") +
  theme_minimal()
```

# Transformations

Les communautés échantillonnées dans des conditions environnementales homogènes ou courtes peuvent avoir des compositions d'espèces avec peu de zéros, de sorte que les distances euclidiennes pourraient suffire à les caractériser.

Néanmoins, c'est rarement la réalité.

Les espèces peuvent être très fréquentes lorsque les conditions sont favorables, ou absentes de nombreux sites. Parfois, cette asymétrie peut introduire des problèmes parasites dans nos analyses.

Nous devons alors transformer nos données de composition pour les analyser de manière appropriée.

Dans `R`, nous pouvons compter sur `vegan::decostand()` pour de nombreux types de transformations.

Jetez un coup d'oeil à l'aide de cette fonction pour voir les options disponibles :

```         
?decostand()
```

## Transformation présence-absence

Nous pouvons changer l'argument `method` en `"pa"` dans `vegdist()` pour transformer nos données d'abondance en données de présence-absence :

<center>Si $y_{ij} \geq 1$, alors, $y'_{ij} = 1$.</center>

Rappelons notre jeu de données `spe` :

```{r}
spe[1:6, 1:6]
```

Transformons les abondances `spe` en présences-absences :

```{r}
spe.pa <- decostand(spe, method = "pa")
spe.pa[1:6, 1:6]
```

## Transformation des profils d'espèces

Il arrive que l'on veuille supprimer les effets des unités très abondantes. Nous pouvons transformer les données en profils d'abondance relative des espèces à l'aide de l'équation suivante :

$$y'_{ij} = \frac{y_{ij}}{y_{i+}}$$

où $yi+$ indique le nombre total de l'échantillon sur toutes les espèces $j=1,...,m$, pour le $i$-ième échantillon.

Dans `decostand()`, nous pouvons utiliser la `méthode` avec `"total"` :

```{r}
spe.total <- decostand(spe, 
                       method = "total")
spe.total[1:5, 1:6]
```

## Transformation de Hellinger

Nous pouvons prendre la racine carrée de la *transformation du profil d'espèce* et obtenir la *transformation de Hellinger*, qui possède de très bonnes propriétés mathématiques et nous permet de réduire les effets des valeurs de $y_{ij}$ qui sont extrêmement grandes.

$$y'_{ij} = \sqrt{\frac{y_{ij}}{y_{i+}}}$$

Dans `decostand()`, nous pouvons changer l'argument `method` avec `"hellinger"` :

```{r}
spe.total <- decostand(spe, 
                       method = "hellinger")
spe.total[1:5, 1:6]
```

## Normalisation par le score Z

La normalisation par le score Z, également connue sous le nom de normalisation par le score standard, est une technique utilisée pour transformer une distribution de données en une distribution normale standard avec une moyenne de 0 et un écart-type de 1. Elle consiste à soustraire la moyenne des données et à la diviser par l'écart-type.

La normalisation des variables environnementales est cruciale, car il est impossible de comparer les effets de variables ayant des unités différentes :

```{r, eval = -1}
?decostand
env.z <- decostand(env, method = "standardize")
```

Cela permet de centrer et d'échelonner les variables afin de rendre votre analyse en cours plus appropriée :

```{r}
apply(env.z, 2, mean)
apply(env.z, 2, sd)
```

Nous verrons plus de détails sur cette transformation dans les prochaines sections !

#### Petite révision

::: explanation
**Association -** "terme général décrivant toute mesure ou tout coefficient permettant de quantifier la ressemblance ou la différence entre des objets ou des descripteurs. Dans une analyse entre descripteurs, zéro signifie qu'il n'y a pas d'association." (Legendre et Legendre 2012).

**Similarité -** une mesure qui est "maximale (S=1) lorsque deux objets sont identiques et minimale lorsque deux objets sont complètement différents." (Legendre et Legendre 2012).

**La distance (également appelée dissimilarité) -** est une mesure qui est "maximale (D=1) lorsque deux objets sont complètement différents". (Legendre et Legendre 2012). Distance ou dissimilarité (D) = 1-S
:::

Le choix d'une mesure d'association dépend de vos données, mais aussi de ce que vous savez, d'un point de vue écologique, à propos de vos données.

Voici quelques mesures de dissimilarité (distance) couramment utilisées (d'après Gotelli et Ellison 2004) :

| Nom de la mesure | Propriété   | Description                                                                                                                                                                  |
|:---------|:---------|:---------------------------------------------------|
| Euclidean        | Metric      | Distance between two points in 2D space.                                                                                                                                     |
| Manhattan        | Metric      | Distance between two points, where the distance is the sum of differences of their Cartesian coordinates, i.e. if you were to make a right able between the points.          |
| Chord            | Metric      | This distance is generally used to assess differences due to genetic drift.                                                                                                  |
| Mahalanobis      | Metric      | Distance between a point and a set distribution, where the distance is the number of standard deviations of the point from the mean of the distribution.                     |
| Chi-square       | Metric      | Similar to Euclidean.                                                                                                                                                        |
| Bray-Curtis      | Semi-metric | Dissimilarity between two samples (or sites) where the sum of lower values for species present in both samples are divided by the sum of the species counted in each sample. |
| Jaccard          | Metric      | [Description](http://en.wikipedia.org/wiki/Jaccard_index)                                                                                                                    |
| Sorensen's       | Semi-metric | Bray-Curtis is 1 - Sorensen                                                                                                                                                  |

### Autres métriques d'association

**Données environnementales quantitatives**

Examinons les *associations* entre les variables environnementales (également connues sous le nom d'analyse des modes Q) :

```{r, echo = TRUE, eval = FALSE}
?dist

# matrice de distance euclidienne des variables environnementales standardisées
env.de <- dist(env.z, method = "euclidean")  

windows() # Créer une fenêtre graphique séparée
coldiss(env.de, diag=TRUE)
```

Nous pouvons alors examiner la *dépendance* entre les variables environnementales (également connue sous le nom d'analyse de mode R) :

```{r, echo = TRUE, eval = FALSE}
(env.pearson<-cor(env)) # Calcul du r de Pearson entre les variables
round(env.pearson, 2) # Arrondit les coefficients à 2 points décimaux 
(env.ken <- cor(env, method="kendall")) # Corrélation de rang du tau de Kendall
round(env.ken, 2) 
```

La corrélation de Pearson mesure la corrélation linéaire entre deux variables. Le tau de Kendall est une corrélation de rang, ce qui signifie qu'il quantifie la relation entre deux descripteurs ou variables lorsque les données sont ordonnées au sein de chaque variable.

Dans certains cas, il peut y avoir des types mixtes de variables environnementales. Le mode Q peut toujours être utilisé pour trouver des associations entre ces variables environnementales. Pour ce faire, nous commencerons par créer un exemple de cadre de données :

```{r, echo = TRUE, eval = FALSE}
var.g1 <- rnorm(30, 0, 1)
var.g2 <- runif(30, 0, 5)
var.g3 <- gl(3, 10)
var.g4 <- gl(2, 5, 30)

(dat2 <- data.frame(var.g1, var.g2, var.g3, var.g4))

str(dat2)
summary(dat2)
```

Une matrice de dissimilarité peut être générée pour ces variables mixtes à l'aide de la matrice de dissimilarité de Gower :

```{r, echo = TRUE, eval = FALSE}

`?daisy #Cette fonction peut gérer les NA dans les données 
(dat2.dg <- daisy(dat2, metric="gower")) 
coldiss(dat2.dg)`
```

**Défi 1 - Avancé** Calculez la dissimilarité de Bray-Curtis et la dissimilarité de Gower de l'abondance des espèces CHA, TRU et VAI pour les sites 1, 2 et 3 (en utilisant les cadres de données "spe" et "env") *sans utiliser la fonction decostand().*

**Défi 1 - Solution avancée** <hidden>

Sous-ensembler des données sur les espèces de sorte que seuls les sites 1 et 2 soient inclus et que seules les espèces CHA, TRU et VAI soient prises en compte.

```{r, echo = TRUE, eval = FALSE}
spe.challenge <- spe[1:3,1:3] #”[1:3,” refers to rows 1 to 3 while “,1:3]” refers to the first 3 species columns (in #this case the three variables of interest)
```

Déterminer l'abondance totale des espèces pour chaque site d'intérêt (somme des trois lignes). Ce chiffre servira de dénominateur dans l'équation ci-dessus.

```{r, echo = TRUE, eval = FALSE}
(Abund.s1<-sum(spe.challenge[1,]))
(Abund.s2<-sum(spe.challenge[2,]))
(Abund.s3<-sum(spe.challenge[3,]))
#() around code will cause output to print right away in console
```

Calculez maintenant la différence d'abondance des espèces pour chaque paire de sites. Par exemple, quelle est la différence entre l'abondance de CHA et de TRU dans le site 1 ? Vous devez calculer les différences suivantes : CHA et TRU site 1 CHA et VAI site 1 TRU et VAI site 1 CHA et TRU site 2 CHA et VAI site 2 TRU et VAI site 2 CHA et TRU site 3 CHA et VAI site 3 TRU et VAI site 3

```{r, echo = TRUE, eval = FALSE}
Spec.s1s2<-0
Spec.s1s3<-0
Spec.s2s3<-0
for (i in 1:3) {
  Spec.s1s2<-Spec.s1s2+abs(sum(spe.challenge[1,i]-spe.challenge[2,i]))
  Spec.s1s3<-Spec.s1s3+abs(sum(spe.challenge[1,i]-spe.challenge[3,i]))
  Spec.s2s3<-Spec.s2s3+abs(sum(spe.challenge[2,i]-spe.challenge[3,i])) }
```

Prenez maintenant les différences que vous avez calculées comme numérateur dans l'équation de la dissimilarité de Bray-Curtis et l'abondance totale des espèces que vous avez déjà calculée comme dénominateur.

```{r, echo = TRUE, eval = FALSE}
(db.s1s2<-Spec.s1s2/(Abund.s1+Abund.s2)) #Site 1 comparé au site 2
(db.s1s3<-Spec.s1s3/(Abund.s1+Abund.s3)) #Site 1 comparé au site 3
(db.s2s3<-Spec.s2s3/(Abund.s2+Abund.s3)) #Site 2 comparé au site 3 
```

Vous devriez trouver des valeurs de 0,5 pour le site 1 vers le site 2, 0,538 pour le site 1 vers le site 3 et 0,053 pour le site 2 vers le site 3.

Vérifiez vos résultats manuels avec ceux que vous obtiendriez en utilisant la fonction vegdist() avec la méthode de Bray-Curtis :

```{r, echo = TRUE, eval = FALSE}
(spe.db.challenge<-vegdist(spe.challenge, method="bray"))
```

Une matrice ressemblant à celle-ci est produite, qui devrait être identique à vos calculs manuels :

|        | Site 1 | Site 2 |
|--------|:-------|:-------|
| Site 2 | 0.5    | \--    |
| Site 3 | 0.538  | 0.0526 |

Pour la dissimilarité de Gower, procédez de la même manière mais utilisez l'équation appropriée :

```{r, echo = TRUE, eval = FALSE}
# Calculez le nombre de colonnes dans votre jeu de données
M<-ncol(spe.challenge)

# Calculez les différences d'abondance entre les paires de sites pour chaque espèce
Spe1.s1s2<-abs(spe.challenge[1,1]-spe.challenge[2,1])
Spe2.s1s2<-abs(spe.challenge[1,2]-spe.challenge[2,2])
Spe3.s1s2<-abs(spe.challenge[1,3]-spe.challenge[2,3])
Spe1.s1s3<-abs(spe.challenge[1,1]-spe.challenge[3,1])
Spe2.s1s3<-abs(spe.challenge[1,2]-spe.challenge[3,2])
Spe3.s1s3<-abs(spe.challenge[1,3]-spe.challenge[3,3])
Spe1.s2s3<-abs(spe.challenge[2,1]-spe.challenge[3,1])
Spe2.s2s3<-abs(spe.challenge[2,2]-spe.challenge[3,2])
Spe3.s2s3<-abs(spe.challenge[2,3]-spe.challenge[3,3])

# Calculer l'étendue de l'abondance de chaque espèce entre les sites  
Range.spe1<-max(spe.challenge[,1]) - min (spe.challenge[,1])
Range.spe2<-max(spe.challenge[,2]) - min (spe.challenge[,2])
Range.spe3<-max(spe.challenge[,3]) - min (spe.challenge[,3])

# Calculer la dissimilarité de Gower  
(dg.s1s2<-(1/M)*((Spe2.s1s2/Range.spe2)+(Spe3.s1s2/Range.spe3)))
(dg.s1s3<-(1/M)*((Spe2.s1s3/Range.spe2)+(Spe3.s1s3/Range.spe3)))
(dg.s2s3<-(1/M)*((Spe2.s2s3/Range.spe2)+(Spe3.s2s3/Range.spe3)))

# Comparez vos résultats
(spe.db.challenge<-vegdist(spe.challenge, method="gower"))
```

</hidden>

# (PART\*) Clustering {.unnumbered}

# Regroupement

L'une des applications des matrices d'association est le regroupement. Le regroupement met en évidence les structures des données en partitionnant soit les objets, soit les descripteurs. Les objets similaires sont ainsi combinés en groupes, ce qui permet d'identifier les distinctions - ou les contrastes - entre les groupes. L'un des objectifs des écologistes pourrait être de diviser un ensemble de sites en groupes en fonction de leurs conditions environnementales ou de la composition de leur communauté.

Les résultats des regroupements sont souvent représentés sous forme de *dendrogrammes* (arbres), où les objets s'agglomèrent en groupes. Il existe plusieurs familles de méthodes de regroupement, mais pour les besoins de cet atelier, nous présenterons une vue d'ensemble de trois méthodes de regroupement agglomératif hiérarchique : le lien simple, le lien complet et le regroupement à variance minimale de Ward. \*Voir le chapitre 8 de Legendre et Legendre (2012) pour plus de détails sur les différentes familles de méthodes de clustering.

Dans les méthodes hiérarchiques, les éléments des clusters (ou groupes) inférieurs deviennent membres de clusters plus importants, de rang supérieur, par exemple espèce, genre, famille, ordre. Avant de procéder au regroupement, il faut créer une matrice d'association entre les objets. La matrice de distance est le choix par défaut des fonctions de regroupement dans R. La matrice d'association est d'abord triée par ordre de distance croissante (ou de similarités décroissantes). Ensuite, les groupes sont formés hiérarchiquement en suivant des règles spécifiques à chaque méthode.

```{r, fig.width=10, echo = FALSE}
# Demonstration of a cluster dendrogram

spe.hel<-decostand(spe, method="hellinger")
spe.dhel <- vegdist(spe.hel,method="euclidean")
spe.dhel.ward <- hclust(spe.dhel, method="ward.D2")
spe.dhel.ward$height<-sqrt(spe.dhel.ward$height)

plot(spe.dhel.ward, hang=-1) # hang=-1 aligns all objets on the same line
```

R dispose de plusieurs fonctions intégrées pour le calcul des clusters agglomérés et la visualisation des résultats. Voici quelques-unes des fonctions les plus couramment utilisées :

-   La fonction `hclust()` du package `stats` : Cette fonction calcule le regroupement hiérarchique à l'aide d'une variété de méthodes de liaison, y compris la méthode unique, la méthode complète, la méthode moyenne et la méthode de la variance minimale de Ward. La sortie est un objet `dendrogramme` qui peut être tracé à l'aide de la fonction `plot()`.
-   La fonction `agnes()` du paquet `cluster` : Cette fonction calcule également le clustering hiérarchique en utilisant plusieurs méthodes de liaison, mais elle peut traiter des ensembles de données plus importants que `hclust()`. La sortie est un objet de la classe "`agnes`" qui peut être tracé en utilisant la fonction `plot()`.
-   Le paquet `dendextend()` : Ce paquet fournit plusieurs fonctions pour manipuler et visualiser les dendrogrammes, y compris `color_branches()`, `rotate()` et `cutree()`.
-   Fonction `ggdendro()` du paquet `ggdendro` : Cette fonction crée un dendrogramme en utilisant la syntaxe `ggplot2` et offre plus d'options de personnalisation que la fonction de base `plot()`.

Ci-dessous, nous allons apprendre plusieurs types d'algorithmes de clustering, tout en appliquant la fonction la plus simple de R pour les produire, `hclust()`.

## Regroupement agglomératif à lien unique

Le regroupement agglomératif à lien unique est un algorithme de regroupement hiérarchique qui fusionne itérativement les deux groupes les plus proches en fonction de la distance minimale entre leurs membres les plus proches. Les étapes de cet algorithme sont les suivantes

1.  Commencez par assigner chaque observation à sa propre grappe.
2.  Calculer la distance entre toutes les paires de grappes à l'aide d'une mesure de distance choisie (par exemple, la distance euclidienne).
3.  Fusionner les deux grappes les plus proches en une seule grappe.
4.  Recalculer la distance entre la nouvelle grappe et toutes les grappes restantes.
5.  Répéter les étapes 3 et 4 jusqu'à ce que toutes les observations appartiennent à une seule grappe ou jusqu'à ce qu'un nombre prédéfini de grappes soit atteint.

Dans la classification agglomérative à lien unique, la distance entre deux grappes est définie comme la distance minimale entre deux points quelconques des grappes. C'est la raison pour laquelle on l'appelle aussi le "plus proche voisin" ou le "single linkage".

L'un des inconvénients du regroupement agglomératif à lien unique est qu'il peut produire des grappes longues et traînantes qui ne représentent pas des groupes bien définis, également connues sous le nom de phénomène de chaînage. Ce phénomène peut être surmonté en utilisant d'autres critères de liaison tels que la liaison complète, la liaison moyenne ou la liaison de Ward.

```{r hclust-single, echo=TRUE}
# générer des échantillons de données
set.seed(123)
x <- matrix(rnorm(20), ncol = 2)

# effectuer un clustering agglomératif à lien unique
hc <- hclust(dist(x), method = "single")

# tracer le dendrogramme
plot(hc, 
     main = "Dendrogramme du regroupement agglomératif à lien unique",
     hang = -1)
```

## Regroupement agglomératif à liens complets

Le regroupement agglomératif à liens complets est un autre algorithme de regroupement hiérarchique qui fusionne itérativement les deux groupes les plus proches sur la base de la distance maximale entre leurs membres les plus éloignés.

Les étapes de l'algorithme de regroupement agglomératif à liaison complète sont les suivantes :

1.  Commencer par assigner chaque observation à sa propre classe.
2.  Calculer la distance entre toutes les paires de groupes à l'aide d'une mesure de distance choisie (par exemple, la distance euclidienne).
3.  Fusionner les deux groupes les plus proches en un seul groupe.
4.  Recalculer la distance entre la nouvelle grappe et toutes les grappes restantes.
5.  Répéter les étapes 3 et 4 jusqu'à ce que toutes les observations appartiennent à une seule grappe ou jusqu'à ce qu'un nombre prédéfini de grappes soit atteint.
6.  Dans le cas de la classification agglomérative par liens complets, la distance entre deux grappes est définie comme la distance maximale entre deux points quelconques des grappes. C'est la raison pour laquelle on l'appelle aussi le "plus lointain voisin" ou le "lien complet".

Par rapport au regroupement agglomératif à lien unique, le regroupement à lien complet tend à produire des grappes plus compactes et sphériques, moins sujettes au phénomène de chaînage. Toutefois, il est plus sensible aux valeurs aberrantes et peut produire des grappes déséquilibrées en cas de valeurs extrêmes ou de bruit dans les données.

```{r hclust-complete, echo=TRUE}
# effectuer un clustering agglomératif à liens complets
hc <- hclust(dist(x), method = "complete")

# tracer le dendrogramme
plot(hc, 
     main = "Dendrogramme de l'agglomération de liens complète",
     hang = -1)
```

## Méthode des groupes de paires non pondérés avec moyenne arithmétique (UPGMA)

Un autre algorithme de regroupement hiérarchique couramment utilisé en bioinformatique et en biologie évolutive est la méthode des groupes de paires non pondérées avec moyenne arithmétique (UPGMA).

Les étapes de l'algorithme UPGMA sont les suivantes

1.  Commencez par assigner chaque point de données à son propre groupe.
2.  Calculer les distances par paire entre tous les groupes sur la base de la métrique de distance choisie, telle que la distance euclidienne, la distance de Manhattan ou la corrélation de Pearson.
3.  Trouver les deux grappes les plus proches sur la base des distances par paire et les fusionner en une seule grappe. La distance entre les deux groupes est calculée comme la moyenne des distances par paire entre leurs membres.
4.  Mettre à jour les distances par paire entre la nouvelle grappe et toutes les grappes restantes. La distance entre la nouvelle grappe et toute autre grappe est calculée comme la moyenne des distances par paire entre les membres de la nouvelle grappe et les membres de l'autre grappe.
5.  Répétez les étapes 3 et 4 jusqu'à ce que tous les points de données appartiennent à un seul groupe.

```{r hclust-upgma, echo=TRUE}
# effectuer un clustering par la méthode des groupes de paires non pondérés avec moyenne arithmétique
hc <- hclust(dist(x), method = "average")

# tracer le dendrogramme
plot(hc, 
     main = "Dendrogramme de la méthode du groupe de paires non pondéré avec la moyenne arithmétique \nAgglomerative Clustering",
     hang = -1)
```

L'algorithme UPGMA suppose un taux d'évolution constant et est donc souvent utilisé pour construire des arbres phylogénétiques à partir de données génétiques ou moléculaires. Le résultat de l'algorithme UPGMA est un dendrogramme, qui montre la structure hiérarchique des groupes.

Une des limites de l'UPGMA est qu'elle peut être sensible aux valeurs aberrantes et qu'elle peut produire des résultats biaisés s'il existe des modèles non aléatoires de données manquantes ou d'évolution convergente. En outre, il s'agit d'une méthode non pondérée, ce qui signifie qu'elle suppose que tous les points de données ont la même importance, ce qui n'est pas toujours le cas.

## Méthode des groupes de paires pondérés avec moyenne arithmétique (WPGMA)

La méthode des groupes de paires pondérés avec moyenne arithmétique (WPGMA) est similaire à la méthode UPGMA, mais elle prend en compte le poids des observations (par exemple, lorsque certaines observations sont plus importantes que d'autres). L'algorithme fonctionne comme suit

1.  Commencez par assigner chaque point de données à sa propre grappe.
2.  Calculer les distances par paire entre tous les groupes sur la base de la métrique de distance choisie, telle que la distance euclidienne, la distance de Manhattan ou la corrélation de Pearson.
3.  Trouver les deux grappes les plus proches sur la base des distances par paire et les fusionner en une seule grappe. La distance entre les deux grappes est calculée comme la moyenne des distances par paire entre leurs membres, pondérées par leurs poids respectifs.
4.  Mettre à jour les distances par paire entre la nouvelle grappe et toutes les grappes restantes. La distance entre la nouvelle grappe et toute autre grappe est calculée comme la moyenne des distances par paire entre les membres de la nouvelle grappe et les membres de l'autre grappe, pondérées par leurs poids respectifs.
5.  Répétez les étapes 3 et 4 jusqu'à ce que tous les points de données appartiennent à un seul groupe.

```{r hclust-wpgma, echo=TRUE}
# exécuter la méthode des groupes de paires pondérés avec la moyenne arithmétique du clustering
hc <- hclust(dist(x), method = "mcquitty")

# trace le dendrogramme
plot(hc, 
     main = "Dendrogramme de la méthode des groupes de paires pondérées avec le clustering agglomératif à moyenne arithmétique",
     hang = -1)
```

## Variance minimale de Ward

La méthode de variance minimale de Ward est un algorithme de regroupement hiérarchique qui vise à minimiser la variance au sein de chaque groupe en fusionnant les groupes qui minimisent l'augmentation de la somme totale des distances au carré. L'algorithme fonctionne comme suit

1.  Commencez par assigner chaque point de données à sa propre grappe.
2.  Calculer la distance entre chaque paire de grappes à l'aide d'une mesure de distance choisie, telle que la distance euclidienne ou la distance de Manhattan.
3.  Fusionnez les deux grappes qui minimisent l'augmentation de la somme totale des distances au carré. L'augmentation de la somme des distances quadratiques est calculée comme la somme des distances quadratiques à l'intérieur de chaque grappe plus la distance quadratique entre les centroïdes des deux grappes multipliée par le nombre de points de données dans chaque grappe.
4.  Calculer la distance entre la nouvelle grappe et toutes les grappes restantes à l'aide de la métrique de distance choisie.
5.  Répétez les étapes 3 et 4 jusqu'à ce que tous les points de données appartiennent à une seule grappe.

La méthode de Ward est souvent préférée lorsque les données contiennent des grappes de tailles et de densités différentes. Le résultat de la méthode de Ward est un dendrogramme qui montre la structure hiérarchique des grappes.

La méthode de Ward est sensible aux valeurs aberrantes et peut produire des résultats biaisés si les données manquantes ne sont pas aléatoires ou si les hypothèses sous-jacentes de normalité et d'égalité des variances ne sont pas respectées.

::: explanation
La méthode de la variance minimale de Ward peut être formulée de différentes manières, ce qui donne lieu à des variantes connues sous les noms de Ward D et Ward D2.

Ward D utilise la somme des distances au carré comme critère à minimiser lors de la fusion des grappes, ce qui équivaut à minimiser l'augmentation de la somme des écarts au carré par rapport à la moyenne de la grappe combinée.

```{r hclust-ward-d, echo=TRUE}
# effectuer le regroupement à variance minimale de Ward
hc <- hclust(dist(x), method = "ward.D")

# tracer le dendrogramme
plot(hc, 
     main = "Dendrogramme de \nWard's minimum variance Agglomerative Clustering",
     hang = -1)
```

D'autre part, Ward D2 utilise la somme des écarts quadratiques par rapport au centroïde comme critère à minimiser lors de la fusion des grappes, ce qui équivaut à minimiser l'augmentation de la somme des écarts quadratiques par rapport au centroïde de la grappe combinée.

```{r hclust-ward-d2, echo=TRUE}
# effectuer le regroupement à variance minimale de Ward
hc <- hclust(dist(x), method = "ward.D2")

# tracer le dendrogramme
plot(hc, 
     main = "Dendrogramme de \nWard's D2 minimum variance Agglomerative Clustering",
     hang = -1)
```

En d'autres termes, Ward D2 prend en compte la distance entre les centroïdes des grappes fusionnées, tandis que Ward D prend en compte la distance entre les points de données individuels et la moyenne de la grappe fusionnée.

Empiriquement, Ward D2 tend à produire des grappes plus compactes et sphériques, tandis que Ward D peut être plus sensible aux valeurs aberrantes et peut produire des grappes allongées ou de forme irrégulière. Toutefois, le choix de Ward D ou Ward D2 peut dépendre des caractéristiques spécifiques des données et de la question de recherche.
:::

## Décider des points de coupure

Le choix du point de coupure pour la prise en compte des groupes de grappes est une étape importante de l'analyse hiérarchique par grappes. Le point de coupure détermine le nombre de grappes à prendre en compte et peut avoir un impact significatif sur l'interprétation des résultats.

Voici quelques méthodes courantes pour déterminer le point de coupure :

1.  Inspection visuelle du dendrogramme : Une approche consiste à inspecter visuellement le dendrogramme et à identifier un point où les branches commencent à devenir longues et clairsemées. Ce point représente un point de rupture naturel dans la hiérarchie et peut être utilisé comme point de coupure.
2.  La méthode du coude : La méthode du coude consiste à tracer une mesure de la qualité des grappes, telle que la somme des carrés à l'intérieur d'une grappe, en fonction du nombre de grappes. Le point où le tracé commence à se stabiliser est considéré comme le point d'inflexion et peut être utilisé comme point de coupure.
3.  Statistique de l'écart : la statistique de l'écart compare la somme des carrés à l'intérieur des grappes pour les données réelles à la somme des carrés à l'intérieur des grappes attendue pour une distribution de référence nulle. Le nombre optimal de grappes est celui qui présente la statistique d'écart la plus élevée.
4.  Connaissances spécifiques au domaine : Dans certains cas, des connaissances spécifiques au domaine ou des recherches antérieures peuvent fournir des indications sur le nombre approprié de grappes à considérer.

## Jouer avec des données réelles : les données sur les espèces de poissons du Doubs

Comparons les méthodes de clustering à lien unique et à lien complet en utilisant les données sur les espèces de poissons du Doubs.

Les données sur les espèces ont déjà été transformées par Hellinger. L'analyse de cluster nécessitant des indices de similarité ou de dissimilarité, la première étape consistera à générer les indices de distance de Hellinger.

```{r, echo = VRAI, eval = VRAI}
# Génère la matrice de distance à partir des données transformées de Hellinger
spe.dhel <- vegdist(spe.hel, method="euclidean") 

# Voir la différence entre les deux matrices
head(spe.hel)# Données d'espèces transformées par Hellinger
head(spe.dhel)# distances de Hellinger entre les sites
```

Utilisez `hclust()` pour calculer les algorithmes de regroupement de liens simples et complets pour ces données :

```{r, echo = VRAI, eval = VRAI}
spe.dhel.single <- hclust(spe.dhel, method="single")

plot(spe.dhel.single)

spe.dhel.complete <- hclust(spe.dhel, method="complete")
plot(spe.dhel.complete)
```

*Y a-t-il de grandes différences entre les deux dendrogrammes* ?

Dans le clustering à lien unique, des chaînes d'objets apparaissent (par exemple 19, 29, 30, 20, 26, etc.), alors que des groupes plus contrastés sont formés dans le clustering à lien complet.

Là encore, il est possible de générer un regroupement à variance minimale de Ward avec hclust(). Cependant, le dendogramme montre les distances au carré par défaut. Afin de comparer ce dendrogramme aux résultats des regroupements par liens simples et complets, il faut calculer la racine carrée des distances.

```{r, echo = VRAI, eval = VRAI}
# Effectuer le clustering de variance minimale de Ward
spe.dhel.ward <- hclust(spe.dhel, method="ward.D2")
plot(spe.dhel.ward)

# Reprendre le dendrogramme en utilisant les racines carrées des niveaux de fusion
spe.dhel.ward$height<-sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward)
plot(spe.dhel.ward, hang=-1) # hang=-1 aligne tous les objets sur la même ligne
```

Il faut être prudent dans le choix d'une mesure d'association et d'une méthode de regroupement afin de traiter correctement un problème.

Qu'est-ce qui vous intéresse le plus : les gradients ? les contrastes entre les objets ?

En outre, les résultats doivent être interprétés en fonction des propriétés de la méthode utilisée. Si plusieurs méthodes semblent adaptées à une question écologique, il convient de les calculer toutes et de comparer les résultats.

Pour rappel, le clustering n'est pas une méthode statistique, mais d'autres mesures peuvent être prises pour identifier des clusters interprétatifs (par exemple, où couper l'arbre), ou pour calculer des statistiques de clustering. Le regroupement peut également être combiné à l'ordination afin de distinguer des groupes de sites. Cela dépasse le cadre de cet atelier, mais voir Borcard et al. 2011 pour plus de détails.

<!--chapter:end:04-association-distances.Rmd-->

# (PART\*) L'Ordination sans contrainte {-}

Nous venons de voir que le clustering est une méthode utilisée pour regrouper des objets ou des observations similaires en fonction de leurs attributs.

Nous allons maintenant nous intéresser aux **méthodes d'ordination**.

L'ordination est une méthode utilisée pour visualiser et explorer les relations entre les variables ou les observations dans un ensemble de données multivariées. L'objectif de l'ordination est de représenter les données dans un espace de dimension inférieure, tel qu'un graphique bidimensionnel ou tridimensionnel, tout en préservant la structure globale des données. L'ordination peut être utilisée pour explorer les modèles et les relations entre les variables ou les observations, pour identifier les variables ou les modèles importants dans les données, ou pour visualiser les résultats d'autres analyses.

Le premier groupe de méthodes d'ordination que nous allons étudier est celui des **ordinations sans contrainte**.

# Que signifie "sans contrainte" ?

Les méthodes d'ordination sans contrainte sont des techniques multivariées utilisées pour visualiser et explorer les relations entre les variables ou les observations d'un ensemble de données sans imposer de contraintes spécifiques sur ces relations. Ces méthodes sont appelées "sans contrainte" parce qu'elles ne nécessitent pas de connaissances préalables ou d'hypothèses sur la structure des données ou les relations entre les variables.

Les méthodes d'ordination sans contrainte consistent à représenter les variables ou les observations dans un espace de dimension inférieure, tel qu'un graphique bidimensionnel ou tridimensionnel, tout en préservant la structure globale des données.

Parmi les méthodes d'ordination sans contrainte les plus courantes figurent l'analyse en composantes principales (ACP), l'analyse des correspondances (AC) et la mise à l'échelle multidimensionnelle (MDS). Ces méthodes peuvent être appliquées à un large éventail de données écologiques, y compris les données sur l'abondance des espèces, les données environnementales et les matrices de similarité des communautés.

Dans l'ensemble, les méthodes d'ordination sans contrainte sont largement utilisées en écologie pour explorer les modèles et les relations entre les communautés biologiques et les variables environnementales. Ces méthodes sont des outils précieux pour identifier les facteurs clés qui influencent la composition et la structure des communautés, et pour élaborer des hypothèses sur les processus écologiques sous-jacents qui déterminent ces schémas.

<!--chapter:end:05-ordination-non-contrainte.Rmd-->


```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.align = 'center')
```

# Analyse en composantes principales

L'analyse en composantes principales (ACP) est une technique statistique utilisée pour réduire la dimensionnalité d'un ensemble de données tout en conservant la majeure partie de sa variabilité. variabilité. Il s'agit d'une méthode de transformation linéaire qui convertit l'ensemble de variables d'origine en un nouvel ensemble de variables linéairement non corrélées. l'ensemble original de variables en un nouvel ensemble de variables linéairement non corrélées, appelées linéairement non corrélées, appelées composantes principales (CP), qui sont classées par ordre décroissant de variance. par ordre décroissant de variance.

L'ACP a été introduite pour la première fois par Karl Pearson en 1901, qui a développé les fondements mathématiques de la méthode. les fondements mathématiques de la méthode. Plus tard, Harold Hotelling (1933) a fourni une interprétation plus détaillée et plus moderne de la méthode ACP.

L'ACP est devenue l'une des techniques les plus utilisées dans l'analyse des données données en raison de sa capacité à identifier des modèles cachés et à réduire la complexité des données à haute dimension.

En substance, l'ACP vise à trouver les combinaisons linéaires des variables qui représentent la plus grande quantité possible de variation dans l'ensemble l'ensemble des données. Les composantes principales qui en résultent sont orthogonales les unes par rapport aux autres, ce qui signifie qu'elles ne sont pas corrélées entre elles. orthogonales, ce qui signifie qu'elles ne sont pas corrélées. leur importance dans l'explication de la variabilité des données.

Pour vous familiariser avec l'ACP, nous allons suivre des exemples détaillés sur la façon de l'exécuter étape par étape. comment l'effectuer étape par étape, puis nous utiliserons les fonctions des paquets R pour la réaliser.

## L'analyse en composantes principales *pas* en bref

Supposons que nous ayons un ensemble de données avec $n$ observations et $p$ variables représentées par une matrice n x p $X$. L'objectif de l'ACP est de transformer cet ensemble de en un nouvel ensemble de $p$ variables non corrélées, appelées composantes principales composantes principales (CP), qui capturent le maximum de variance dans les données originales. données originales.

#### Charger les données

Dans cet atelier, nous utilisons le jeu de données `data(varechem)`, qui contient des des mesures des propriétés chimiques de 18 échantillons de sol provenant d'une d'une expérience sur le terrain. Nous sélectionnerons les deux premières variables :

```{r}
# Chargement du paquet datasets
library(datasets)

# Charger le jeu de données varechem
data(varechem)

# Sélectionner les données
(data <- varechem[, 1:2])

```

#### Normaliser les données

Nous devons d'abord normaliser les données pour qu'elles aient une moyenne nulle et une variance unitaire :

$$ Z_{ij} = \frac{X_{ij} - \bar{X_j}}{s_j} $$

où Z est la matrice standardisée, X est la matrice originale, $\bar{X_j}$ est la moyenne de la variable j, et $s_j$ est l'écart-type de la variable j.

```{r}
data_std <- scale(data)
```

#### Calculer la matrice de covariance

Ensuite, nous calculons la matrice de covariance de $Z$ :

$$ C = \frac{1}{n-1}ZZ^T $$

où $C$ est la matrice de covariance et $T$ représente l'opération de transposition.

La matrice de covariance est une matrice symétrique qui représente les covariances par paire entre les variables. La formule de la covariance entre deux variables $X$ et $Y$ est la suivante :

$$\text{Cov}(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})(Y_i - \bar{Y})$$

où $n$ est la taille de l'échantillon, $X_i$ et $Y_i$ sont les valeurs des variables pour l'observation \$i\$, et $\bar{X}$ et $\bar{Y}$ sont les moyennes de l'échantillon des variables.

```{r}
cov_matrix <- cov(data_std)
```

#### Effectuer la décomposition en valeurs propres de la matrice de covariance

Ensuite, nous calculons les valeurs propres et les vecteurs propres de $C$ :

$$ Cv = \lambda v $$

où $C$ est la matrice de covariance, $v$ est le vecteur propre et $\lambda$ est la valeur propre correspondante.

```{r}
eigen_decomp <- eigen(cov_matrix)
Valeurs propres <- eigen_decomp$values
Vecteurs propres <- eigen_decomp$vectors
```

Les vecteurs propres représentent les directions dans l'espace à $p$ dimensions qui capturent le maximum de variance dans les données, et les valeurs propres indiquent la quantité de variance capturée par chaque vecteur propre.


#### Projeter les données normalisées sur l'espace des valeurs propres

Enfin, nous projetons la matrice de données normalisées $Z$ sur les nouveaux vecteurs de base pour obtenir les composantes principales. Ici, nous calculons les scores des composantes principales en multipliant les données normalisées par les vecteurs propres de toutes les composantes principales :

$$ Y = ZV $$

où $Y$ est la matrice des données transformées et $V$ est la matrice des vecteurs propres.

```{r}
F_PrComps <- data_std %*% Eigenvectors

head(F_PrComps)
```

La matrice de score, $F$, (objet `F_PrComps`) permet de *rotationner* le nouvel espace de données, de sorte qu'il soit représenté par rapport aux composantes principales. Par exemple, voir la figure ci-dessous :



```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align='center', fig.cap = "Relation entre N et P à partir de l'ensemble de données `varechem`. Les lignes diagonales violette et orange représentent respectivement la première et la deuxième composante principale. Les lignes violettes indiquent les résidus entre le point de données et la première composante principale, tandis que les lignes orange segmentées indiquent la distance entre les points et la deuxième composante principale. Les flèches indiquent les valeurs propres (direction) de l'analyse en composantes principales effectuée sur ces deux variables."}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(data_std)

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)       

plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0, 
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0, 
#        y0 = 0, 
#        x1 = Eigenvalues[2]*Eigenvectors[1,2], 
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange", 
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0, 
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'purple')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4))


plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0, 
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[2]*Eigenvectors[1,2], 
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange", 
       lwd = 2)


line2 <- c(0, 
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line2)

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'orange')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
          pos = 1, 
          cex=1.4)
)

title(xlab = "N",
      ylab = "P",
      outer = TRUE, line = 3)

par(op)
```


```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5, fig.align = 'center', fig.cap = "Relation entre PC1 et PC2 de l'analyse en composantes principales effectuée sur les variables P et N de l'ensemble de données `varechem`. Les lignes violettes et orange représentent respectivement la première et la deuxième composante principale. Les lignes violettes indiquent les résidus entre le point de données et la première composante principale, tandis que les lignes orange segmentées indiquent la distance entre les points et la deuxième composante principale."}

score <- as.data.frame(F_PrComps)

colnames(score) <- c("PC1", "PC2")

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)


plot(PC2 ~ PC1, 
     col = as.factor(rownames(score)), 
     pch = 19, 
     xlim = c(-2.2, 2.2), ylim = c(-2.2,2.2), xlab='PC1', ylab='PC2',data = score)

abline(h = 0, col = 'purple')
abline(v = 0, col='orange')


perp.segment.horiz <- function(x0, y0){
  x1 <- x0
  y1 <- 0
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1 <- perp.segment.horiz(score[,1], score[,2])

segments(x0 = ss1$x0, x1 = ss1$x1, y0 = ss1$y0, y1 = ss1$y1, col='purple')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)
with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))


plot(PC2 ~ PC1, col=as.factor(rownames(score)), 
     pch = 19, xlim=c(-2.2, 2.2), ylim = c(-2.2,2.2),
     xlab='PC1', ylab='PC2',data=score)

abline(h = 0, col = 'purple')
abline(v = 0, col ='orange')


perp.segment.vert <- function(x0, y0){
  x1 <- 0
  y1 <- y0
  
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1a <- perp.segment.vert(score[,1], score[,2])
segments(x0 = ss1a$x0, x1 = ss1a$x1, y0 = ss1a$y0, y1 = ss1a$y1, col='orange')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)

with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))

title(xlab = "PC1",
      ylab = "PC2",
      outer = TRUE, 
      line = 3)

par(op)
```

## Analyse en composantes principales à l'aide des fonctions du paquet

L'ACP peut également être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

En résumé, voici ce que nous avons fait :


```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```

Comment cela se compare-t-il à `stats::prcomp()` ?


```{r}
PCA_prcomp <- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp <- prcomp(Y_std)

head(PCA_prcomp$x)
```

Et quelle est la comparaison avec `stats::princomp()` ?

```{r}
PCA_princomp <- princomp(Y_std)

head(PCA_princomp$scores)
```


Et avec `vegan::rda()`?

```{r}
PCA_vegan_rda <- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
```

`vegan::rda()` est un peu spéciale. Elle utilise des échelles alternatives. Nous ne les aborderons pas ici, mais vous pouvez étudier la `vignette("decision-vegan")`.

## Analyse en composantes principales sur des données écologiques

Nous avons mis en œuvre l'ACP sur un ensemble de données à deux variables, pour des raisons de simplicité.

Avançons et appliquons-la à notre jeu de données sur les espèces de poissons.

Pour cela, nous allons utiliser la fonction `vegan::rda()` sur les données de poissons *transformées par Hellinger* et résumer les résultats :

```{r}
spe.h.pca <- rda(spe.hel)

# summary(spe.h.pca)
```

Les premières lignes de `summary.rda()` nous renseignent sur la *variance totale* et la *variance non contrainte* de notre modèle.

``{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[5:8])
```

``{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(12:16, 21:24)])
```

Viennent ensuite les *valeurs propres* et leur contribution à la variance.

En fait, si nous additionnons toutes nos *valeurs propres*, nous obtiendrons la quantité de variance non contrainte expliquée par l'analyse !

```{r}
sum(spe.h.pca$CA$eig)
```

Les informations suivantes sont liées à l'*échelle*, aux *scores d'espèces* et aux *scores de sites*.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(26:29, 31:32, 34:40, 63:64, 66:72)])
```

Les *espèces* font référence à vos descripteurs (c'est-à-dire les colonnes de votre jeu de données), qui sont ici les espèces de poissons.

Les *scores* font référence à la position de chaque espèce le long des composantes principales.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(32, 34:40)])
```

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(64, 66:72)])
```

*Sites* représente les lignes de votre jeu de données, qui sont ici les différents sites le long de la rivière *Doubs*.

Cette information peut être obtenue avec la fonction `score()` que nous avons utilisée précédemment :

```{r, eval = FALSE, echo = TRUE}
scores(spe.h.pca,
       display = "species" or "sites")
```


## Condenser les données avec l'analyse en composantes principales

Ici, nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

#### Critère de Kaiser-Guttman

Nous pouvons sélectionner les composantes principales qui capturent plus de variance que l'explication moyenne de toutes les composantes principales. Pour ce faire, nous procédons comme suit

1.  Extraire les *valeurs propres* associées aux composantes principales ;

2.  Sous-ensemble des *valeurs propres* supérieures à la *valeur propre* moyenne :


```{r}
ev <- spe.h.pca$CA$eig
# ev[ev > mean(ev)]
```

```{r, echo = -1, fig.width=10, fig.height = 5.5}
par(mar=c(1,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

#### Modèle à bâtons rompus

Le modèle à bâtons (ou branches) rompus retient les composantes qui expliquent plus de variance que ce que l'on pourrait attendre en divisant aléatoirement la variance en $p$ parties.

```{r}
head(bstick(spe.h.pca))
```

```{r, echo = TRUE, fig.width=4.5, fig.height = 4.5}
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```

## Échelonnement

Il ne nous reste plus qu'à discuter de la *mise à l'échelle* et à *visualiser* nos résultats.

Pratiquons et calculons une ACP sur les variables environnementales standardisées pour le même ensemble de données.

```{r}
env.pca <- rda(env.z)
# summary(env.pca, scaling  = 2)
```

Déterminer notre sous-ensemble de *valeurs propres* et leurs *vecteurs propres* correspondants :


```{r}
ev <- env.pca$CA$eig
```

```{r}
ev[ev>mean(ev)]
```

```{r, echo = FALSE, fig.width=8, fig.height = 5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

Les informations calculées par l'ACP peuvent être représentées par des *biplots*.

Nous pouvons produire un *biplot simple et rapide* de l'ACP en utilisant la fonction `plot()` dans la base `R`.


```{r, echo = -1}
par(mar=c(4,4, 0.1,0.1), cex = 1.5)
plot(spe.h.pca)
```

`biplot()` de `base` `R` permet une meilleure interprétation.

```{r, echo = -1, fig.height=6, fig.width=6.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca)
```

Les flèches sont tracées pour montrer la directionnalité et l'angle des descripteurs dans l'ordination.

- Les descripteurs situés à 180 degrés les uns des autres sont négativement corrélés ;
- Les descripteurs situés à 90 degrés les uns des autres ont une corrélation nulle ;
- Les descripteurs situés à 0 degré les uns des autres sont positivement corrélés.

*Échelle de type 2* (par défaut) : les distances entre les objets ne sont pas des approximations des distances euclidiennes ; les angles entre les vecteurs des descripteurs (espèces) reflètent leurs corrélations.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 2)
```

*Échelle de type 1* : tente de préserver la distance euclidienne (dans l'espace multidimensionnel) entre les objets (sites) : les angles entre les vecteurs des descripteurs (espèces) ne sont pas significatifs.

```{r, echo = -1, fig.height=4, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 1)
```


## Défi #2

En utilisant tout ce que vous avez appris, calculez une ACP sur les données d'abondance des espèces d'acariens

```{r}
data(mite)
```

Préparez-vous à discuter et à répondre :

- Quelles sont les composantes principales les plus pertinentes, c'est-à-dire les sous-ensembles ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?

**Défi 2 - Solution** <hidden> Votre code ressemble probablement à ce qui suit.

Calculer l'ACP sur les données d'espèces transformées par Hellinger

```{r}
mite.spe.hel <- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca <- rda(mite.spe.hel)
```

Appliquer le critère de Kaiser-Guttman

```{r, eval = F}
ev <- mite.spe.h.pca$CA$eig
ev[ev>mean(ev)]
n <- length(ev)
barplot(ev, main = "Valeurs propres", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Valeur propre moyenne", 
       lwd = 2, 
       col = "red3", bty = "n")
```


```{r, echo = F, fig.width=5, fig.height=4}
par(mar=c(4,4,2,1), cex = 1.2)
ev <- mite.spe.h.pca$CA$eig
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Valeur propre moyenne", 
       lwd = 2, col = "red3", 
       bty = "n")
```

```{r, echo = -1, fig.height=6.5, fig.width=7}
par(mar = c(4,4,0.05,0.05), cex = 1.5)
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

</hidden>





<!--chapter:end:06-analyse-des-composantes-principales.Rmd-->

# Analyse des Correpondances

L'une des hypothèses clefs de la PCA postule que les espèces sont liées
les unes aux autres de façon linéaire, et qu'elles répondent de façon
linéaire aux gradients écologiques. Ce n'est cependant pas
nécessairement le cas dans les données écologiques (e.g. beaucoup
d'espèces montrent en effet un distribution unimodale le long des
gradients environnementaux). Utiliser une PCA avec des données contenant
des espèces à distribution unimodale, ou un grand nombre de zéros
(absence des espèces), peut conduire à un phénomène statistique appelé
\"horseshoe effect\" (ou effet fer à cheval) se produisant le long de
gradients écologiques. Dans de tels cas, l'Analyse des Correspondances
(CA) permet de mieux représenter les données (voir Legendre et Legendre
pour plus d'informations). Comme la CA préserve les distances de Chi2
entre objets (tandis que la PCA préserve les distances euclidiennes),
cette technique est, en effet, plus appropriée pour ordonner les jeux de
données contenant des espèces à distribution unimodale, et a, pendant
longtemps, était l'une des techniques les plus employées pour analyser
les données d'absence-présence ou d'abondances d'espèces. Lors d'une
CA, les données brutes sont d'abord transformées en une matrice Q des
contributions cellule-par-cellule à la statistique Chi2 de Pearson, puis
la matrice résultante est soumise à une décomposition en valeurs
singulières afin de calculer les valeurs propres et vecteurs propres de
l'ordination.

Le résultat d'une CA représente donc une ordination dans laquelle les
distances de Chi2 entre objets sont préservées (au lieu de la distance
euclidienne dans une PCA), le distance de Chi2 n'étant pas influencée
par la présence de double-zéros. Ainsi, la CA constitue une méthode
d'ordination puissante pour l'analyse des abondances brutes d'espèces
(i.e. sans pré-transformation). Contrairement à la PCA, la CA peut être
appliquée sur des données quantitatives ou binaires (telles que des
abondance ou absence-présence d'espèces). Comme dans une PCA, le
critère de Kaiser-Guttman peut être utilisé pour identifier les axes
significatifs d'une CA, et les scores des objets le long des axes
d'ordination peuvent être extraits pour être utlisés dans des
régressions multiples par exemple.

Exécuter une CA sur les données d'abondance d'espèces:

```{r, echo = TRUE, eval = FALSE}
#Effectuer une CA à l'aide de la fonction cca() (NB: cca() est utilisée à la fois pour les CA et CCA)
spe.ca <- cca(spe)
 
# Identifier les axes significatifs
ev<-spe.ca$CA$eig
ev[ev>mean(ev)]
n=length(ev)
barplot(ev, main="Eigenvalues", col="grey", las=2)
abline(h=mean(ev), col="red")
legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")
```

![](images//ca_guttmankaiser.png){.align-center}

D'après cet histogramme, à partir du sixième axe d'ordination CA6, la
proportion de variance expliquée diminue sous la proportion moyenne
expliquée par l'ensemble des axes. La sortie R de la CA ci-dessous
montre également que les cinq premiers axes d'ordination explique une
proportion cumulée de variance expliquée de 84.63%.

```{r, echo = TRUE, eval = FALSE}
summary(spe.h.pca) 
summary(spe.h.pca, diplay=NULL)
```

![](images//ca_summary.png){.align-center}

Les résultats d'une CA sont présentés sous R de la même façon que ceux
d'une PCA. On y observe que le premier axe CA1 explique 51.50% de la
variation de l'abondance des espèces tandis que le second axe CA2
explique 12.37% de la variation.

```{r, echo = TRUE, eval = FALSE}
par(mfrow=c(1,2))
#### scaling 1
plot(spe.ca, scaling=1, type="none", main='CA - biplot scaling 1', xlab=c("CA1 (%)", round((spe.ca$CA$eig[1]/sum(spe.ca$CA$eig))*100,2)),
ylab=c("CA2 (%)", round((spe.ca$CA$eig[2]/sum(spe.ca$CA$eig))*100,2)))

points(scores(spe.ca, display="sites", choices=c(1,2), scaling=1), pch=21, col="black", bg="steelblue", cex=1.2)

text(scores(spe.ca, display="species", choices=c(1), scaling=1),
     scores(spe.ca, display="species", choices=c(2), scaling=1),
     labels=rownames(scores(spe.ca, display="species", scaling=1)),col="red", cex=0.8)

#### scaling 2
plot(spe.ca, scaling=1, type="none", main='CA - biplot scaling 2', xlab=c("CA1 (%)", round((spe.ca$CA$eig[1]/sum(spe.ca$CA$eig))*100,2)),
     ylab=c("CA2 (%)", round((spe.ca$CA$eig[2]/sum(spe.ca$CA$eig))*100,2)), ylim=c(-2,3))

points(scores(spe.ca, display="sites", choices=c(1,2), scaling=2), pch=21, col="black", bg="steelblue", cex=1.2)
text(scores(spe.ca, display="species", choices=c(1), scaling=2),
     scores(spe.ca, display="species", choices=c(2), scaling=2),
     labels=rownames(scores(spe.ca, display="species", scaling=2)),col="red", cex=0.8)
```

![](images//ca_biplot.png){.align-center}

Ces biplots montrent qu'un groupe de sites (à gauche) possède des
communautés similaires de poissons caractérisées par de nombreuses
espèces dont GAR, TAN, PER, ROT, PSO et CAR; dans le coin supérieur
droit, un second groupe de sites se caractérisent par les espèces LOC,
VAI et TRU; le dernier groupe de sites dans le coin inférieur droit
montrent des communautés abondantes en BLA, CHA et OMB.

**Défi 4** Exécuter une CA sur les données d'abondance des *espèces
d'acariens* (données mite). Quels sont les axes importants? Quels
groupes de sites pouvez-vous identifier? Quelles espèces sont liées à
chaque groupe de sites?

**Défi 4 - Solution**  Votre code devrait s'apparenter à
celui-ci:

```{r, echo = TRUE, eval = FALSE}
# CA 
mite.spe.ca<-cca(mite.spe)

# Quels sont les axes importants? 
ev<-mite.spe.ca$CA$eig
ev[ev>mean(ev)]
n=length(ev)
barplot(ev, main="Eigenvalues", col="grey", las=2)
abline(h=mean(ev), col="red")
legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")

# Résultats  
summary(mite.spe.ca, display=NULL)

# Biplot
windows()
plot(mite.spe.ca, scaling=1, type="none",
     xlab=c("PC1 (%)", round((mite.spe.ca$CA$eig[1]/sum(mite.spe.ca$CA$eig))*100,2)),
     ylab=c("PC2 (%)", round((mite.spe.ca$CA$eig[2]/sum(mite.spe.ca$CA$eig))*100,2)))
points(scores(mite.spe.ca, display="sites", choices=c(1,2), scaling=1),
       pch=21, col="black", bg="steelblue", cex=1.2)
text(scores(mite.spe.ca, display="species", choices=c(1), scaling=1),
     scores(mite.spe.ca, display="species", choices=c(2), scaling=1),
     labels=rownames(scores(mite.spe.ca, display="species", scaling=1)),
     col="red", cex=0.8)
```

Et votre biplot devrait ressembler à celui-ci:

![](images//ca_mite_biplot.png){.align-center}


<!--chapter:end:07-analyse-de-correspondence.Rmd-->

#  Analyse en coordonnées principales

La PCA, comme la CA, impose une préservation des distances entre objets:
la distance euclidienne dans le cas de la PCA, et la distance de Chi2
dans la CA. Si l'objectif est d'ordonner les objets sur la base d'une
autre mesure de distance plus appropriée au problème, la PCoA constitue
une technique de choix. Dans une PCA, les données sont pivotées de façon
à ce que la première composante principale (correspondant à une
combinaison linéaire des descripteurs) explique la plus forte proportion
de variation possible; la contribution de chaque descripteur (espèces ou
variables environnementales) à chaque composante principale peut alors
être évaluée d'après son score. La PCoA est une seconde méthode
d'ordination sans contrainte dans laquelle les points sont ajoutés les
uns après les autres à l'espace d'ordination en utilisant la distance
euclidienne *ou n'importe quelle mesure de distance (dissimilarité)
métrique vous choisissez*. Un premier point est ainsi placé dans
l'espèce d'ordination, puis un second point placé à la valeur de
distance du premier, puis un troisième et ainsi de suite en ajoutant
autant d'axes (de dimensions) que nécessaire. Il est parfois difficile
de choisir entre effectuer une PCA ou une PCoA. La PCA permet toutefois
de réduire des données multivariables en un faible nombre de dimensions
tandis que la PCoA est utile pour visualiser les distances entre sites
(ou objets). La PCoA est aussi particulièrement adaptées pour les jeux
de données présentant plus de colonnes que de lignes. Par exemple, si
des centaines d'espèces ont été observées dans un petit nombree de
quadrats, une approche basée sur une PCoA utilisant la distance de
Bray-Curtis (voir ci-dessous) peut être plus adaptée.

PCoA avec DoubsSpe (transformé Hellinger):

```{r, echo = TRUE, eval = FALSE}
# En utilisant la fonction cmdscale()
?cmdscale
cmdscale(dist(spe.hel), k=(nrow(spe)-1), eig=TRUE)

# En utilisant la fonction pcoa()
?pcoa
spe.h.pcoa<-pcoa(dist(spe.hel))

# Extraction des résultats
spe.h.pcoa 

# Représentation graphique
biplot.pcoa(spe .h.pcoa, spe.hel, dir.axis2=-1)
```

Les résultats de cette PCoA sont:

![](images//pcoa_outputfr_1.png){width="800"}

![](images//pcoa_outputfr_2.png){width="800"}

Et le graphique:

![](images//pcoa_spe.png){width="500"}

Vous pouvez aussi exécuter cette PCoA avec une autre mesure de distance
(ex. Bray-Curtis):

```{r, echo = TRUE, eval = FALSE}
spe.bray.pcoa<-pcoa(spe.db) # il s'agit de la matrice de distances de Bray-Curtis qu'on a générée plus tôt 
spe.bray.pcoa
biplot.pcoa(spe.bray.pcoa, spe.hel, dir.axis2=-1)
# Le choix d'une mesure de distance est très important car ça influence les résultats! 
```

**Défi 5** Exécuter une PCoA sur les données d'abondance des *espèces
d'acariens* transformées Hellinger (données mite). Quels sont les axes
importants? Quels groupes de sites pouvez-vous identifier? Quelles
espèces sont liées à chaque groupe de sites? Comment les résultats de
cette PCoA se comparent-ils avec ceux de la PCA?

**Défi 5 - Solution** 

```{r, echo = TRUE, eval = FALSE}
mite.spe.h.pcoa<-pcoa(dist(mite.spe.hel))
mite.spe.h.pcoa
windows()
biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel, dir.axis2=-1)
```

Représentation graphique:

![](images//pcoa_mite.png){width="500"}

Les espèces 16 et 31 sont plus éloignées des autres espèces en termes de
distance, et donc leur distribution entre les sites est très différente
de celle des autres espèces d'acariens. Les sites dont les étiquettes
se chevauchent sont de bons exemples de sites à forte similarité en
termes de communautés d'acariens. 

<!--chapter:end:08-analyse-des-coordonnees-principales.Rmd-->

# Positionnement multidimensionnel non-métrique

Les méthodes d'ordination non contrainte présentées ci-dessus
permettent d'organiser les objets (ex. les sites) caractérisés par des
descripteurs (ex. les espèces) dans un espace comprenant l'ensemble des
dimensions décrites par l'ellipsoïde représentant le nuage des points de
données. En d'autres termes, la PCA, la CA et la PCoA calculent un
grand nombre d'axes d'ordination (nombre proportionnel au nombre de
descripteurs) représentant la variation des descripteurs entre sites et
préservant les distances entre objets (distance euclidienne dans une
PCA, distance de Chi2 dans une CA et distance définie par l'utilisateur
dans une PCoA). L'utilisateur peut ensuite sélectionner les axes
d'intérêt (généralement les deux premiers axes d'ordination) pour
représenter les objets dans un biplot. Le biplot produit représente
ainsi correctement les distances entre objets (ex. la similarité des
sites), mais ne permet pas de représenter l'ensemble des dimensions de
la variation dans l'espace d'ordinations (étant donnée que l'Axe 3,
l'Axe 4,\..., l'Axe n'apparaissent pas sur le biplot, mais
contribuent tout de même à expliquer la variation entre objets).

Dans certains cas, la priorité n'est pas de préserver la distance
exacte entre les objets, mais au contraire de représenter aussi
fidèlement que possible les relations entre objets selon un petit nombre
d'axes (généralement deux ou trois) spécifiés par l'utilisateur. Dans
de tels cas, le positionnement multidimensionnel non-métrique (NMDS) est
la solution. Si l'utilisateur définit un nombre d'axe égal à deux, le
biplot produit par le NMDS correspond à la meilleure solution graphique
pour représenter en deux dimensions la similarité entre objets (les
objets dissimilaires étant les plus éloignées, et les objets similaires
étant les plus proches). De plus, le NMDS permet à l'utilisateur de
choisir la mesure de distance qu'il souhaite pour ordonner les objets.

Afin de trouver la meilleure représentation des objets, le NMDS applique
une procédure itérative qui vise à positionner les objets dans le nombre
spécifié de dimensions de façon à minimiser une fonction de stress
(variant de 0 à 1) qui mesure la qualité de l'ajustement de la distance
entre objets dans l'espace d'ordination. Ainsi, plus la valeur du
stress sera faible, plus la représentation des objets dans l'espace
d'ordination sera exacte. Un second moyen d'évaluer l'exactitude
d'un NMDS consiste à construire un diagramme de Shepard qui représente
les distances entre objets sur le biplot d'ordination en fonction de
leurs distances réelles. Le R2 obtenu à partir de la régression entre
ces deux types de distance mesure la qualité de l'ajustement du NMDS.

```{r, echo = TRUE, eval = FALSE}
# NMDS
spe.nmds<-metaMDS(spe, distance='bray', k=2)
 
### Extraction des résultats
spe.nmds

### Évaluation de la qualité de l'ajustement et construction du diagramme de Shepard
spe.nmds$stress
stressplot(spe.nmds, main='Shepard plot')

# Construction du biplot
windows()
plot(spe.nmds, type="none", main=paste('NMDS/Bray - Stress=', round(spe.nmds$stress, 3)),
     xlab=c("NMDS1"),
     ylab=c("NMDS2"))
points(scores(spe.nmds, display="sites", choices=c(1,2)),
       pch=21, col="black", bg="steelblue", cex=1.2)
text(scores(spe.nmds, display="species", choices=c(1)),
     scores(spe.nmds, display="species", choices=c(2)),
     labels=rownames(scores(spe.nmds, display="species")),
     col="red", cex=0.8)
```

![](images//shepard_plot.png){.align-center}

Le diagramme de Shepard identifie une forte corrélation entre les
distances observées et les distances de l'ordination (R2 \> 0.95), et
donc une bonne qualité de l'ajustement du NMDS.

![](images//nmds_biplot.png){.align-center}

Le biplot du NMDS identifie un groupe de sites caractérisés par les
espèces BLA, TRU, VAI, LOC, CHA et OMB, tandis que les autres espèces
caractérisent un groupe de sites situés dans le coin supérieur droit du
biplot. Quatre sites situés dans le coin inférieur droit sont fortement
différents des autres.

**Défi 6** Exécuter un NMDS sur les données d'abondance des *espèces
d'acariens* (données mite) en deux dimensions à partir de distances de
Bray-Curtis. Évaluer la qualité de l'ajustement et interpréter le
biplot.

**Défi 5 - Solution** 

```{r, echo = TRUE, eval = FALSE}
### NMDS
mite.spe.nmds<-metaMDS(mite.spe, distance='bray', k=2)

### Extraction des résultats
mite.spe.nmds

### Évaluation de la qualité de l'ajustement
mite.spe.nmds$stress
stressplot(mite.spe.nmds, main='Shepard plot')

### Construction du biplot
windows()
plot(mite.spe.nmds, type="none", main=paste('NMDS/Bray - Stress=', round(mite.spe.nmds$stress, 3)),
     xlab=c("NMDS1"),
     ylab=c("NMDS2"))
points(scores(mite.spe.nmds, display="sites", choices=c(1,2)),
       pch=21, col="black", bg="steelblue", cex=1.2)
text(scores(mite.spe.nmds, display="species", choices=c(1)),
     scores(mite.spe.nmds, display="species", choices=c(2)),
     labels=rownames(scores(mite.spe.nmds, display="species")),
     col="red", cex=0.8)
```

![](images//nmds_mite_shepard.png){.align-center}

La corrélation entre distance observée et distance d'ordination (R2 \>
0.91) et la valeur de stress relativement faible identifient une bonne
qualité de l'ajustement du NMDS.

![](images//nmds_mite_biplot.png){.align-center}

Aucun groupe de sites ne peut être précisément identifié à partir du
biplot, ce qui montre que la plupart des espèces sont présentes dans la
plupart des sites, i.e. peu de sites présentent des communautés
distinctes. 

<!--chapter:end:09-echelle-multidimensionnelle-non-metrique.Rmd-->

# (PART\*) Considérations finales {-}

# Sommaire

L'ordination constitue une puissante méthode d'analyse pour étudier les
relations entre objets caractérisés par différents descripteurs (ex. des
sites décrits par leurs communautés biologiques, ou leurs variables
environnementales), mais de nombreuse méthodes d'ordination existent.
Ces méthodes diffèrent principalement par le type de distance qu'elles
préservent, le type de variables qu'elles autorisent, et le nombre de
dimensions de l'espace d'ordination. Pour mieux guider votre choix de
la méthode d'ordination à utiliser, le tableau ci-dessous identifie les
caractéristiques de chacune des quatre méthodes d'ordination présentées
lors de cet atelier.

![](images//resume_ordination.jpg){.align-center}

Lors du prochaine atelier, vous verrez comment identifier les relations
entre variables environnementales et communautés biologiques décrivant
un même ensemble de sites, à l'aide des méthodes d'analyses
canoniques.

# Resources additionnels

<!--chapter:end:10-considerations-finales.Rmd-->

`r if (knitr::is_html_output()) '
# Réferences
'`

<!--chapter:end:11-references-fr.Rmd-->

