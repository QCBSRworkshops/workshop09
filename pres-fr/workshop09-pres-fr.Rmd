---
title: "Atelier 9: Analyses multivari√©es"
subtitle: "S√©rie d'ateliers R"
author: "Centre de la Science de la Biodiversit√© du Qu√©bec"
output:
  xaringan::moon_reader:
    includes:
      in_header: qcbsR-header.html
    lib_dir: assets
    seal: true
    css: ["default", "qcbsR.css", "qcbsR-fonts.css"]
    nature:
      countIncrementalSlides: false
      beforeInit: "qcbsR-macros.js"
      highlightLines: true
---
class: inverse, center, middle

```{r setup, echo = FALSE, message=FALSE, warning=FALSE, include = FALSE}
library(knitr)

## Setup for your presentation
knitr::opts_chunk$set(
  eval = TRUE,
  cache = TRUE,
  comment = "#",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 5, fig.height = 5, fig.retina = 3,
  fig.align = 'center'
)

options(repos = structure(
  c(CRAN = "http://cran.r-project.org")
  )
  )

# Install xaringanExtra

if(nzchar(system.file(package = "xaringanExtra")) == FALSE) {remotes::install_github("gadenbuie/xaringanExtra", upgrade = "always", quiet = TRUE)}

# Include copy-to-clipboard icons

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fas fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r install_pkgs, message=FALSE, warning=FALSE, include=FALSE, results=0}
# Standard procedure to check and install packages and their dependencies, if needed.

list.of.packages <- c("ape", "gclus", "vegan", "GGally", "PlaneGeometry", "remotes", "matlib")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages) > 0) {
  install.packages(new.packages, dependencies = TRUE) 
  print(paste0("The following package was installed:", new.packages)) 
} else if(length(new.packages) == 0) {
    print("All packages were already installed previously")
}

# Load all required libraries at once
lapply(list.of.packages, require, character.only = TRUE, quietly = TRUE)
```

# √Ä propos de cet atelier

[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=repo&message=dev&color=6f42c1&logo=github)](https://github.com/QCBSRworkshops/workshop09)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=wiki&message=09&logo=wikipedia)](https://wiki.qcbs.ca/r_workshop9)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Slides&message=09&color=red&logo=html5)](https://qcbsrworkshops.github.io/workshop09/pres-fr/workshop09-pres-fr.html)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Slides&message=09&color=red&logo=adobe-acrobat-reader)](https://qcbsrworkshops.github.io/workshop09/pres-fr/workshop09-pres-fr.pdf)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=script&message=09&color=2a50b8&logo=r)](https://qcbsrworkshops.github.io/workshop09/book-fr/workshop09-script-fr.R)


---

<p style="font-size:75%">

.center[
**Contributeurs et contributrices au d√©veloppement de cet atelier**

en modifiant et en am√©liorant son contenu dans le cadre du <br> Prix d'apprentissage et de d√©veloppement (*Le*arning *a*nd *D*evelopment *A*ward)
]

.pull-left[
.right[

**2022** - **2021** - **2020**

[Pedro Henrique P. Braga]()

[Katherine H√©bert]()

[Mi Lin]()

[Linley Sherin]()

]
]

.pull-right[
.left[
**2019** - **2018** - **2017**

[Gabriel Mu√±oz](https://github.com/fgabriel1891)

[Marie H√©l√®ne-Brice]()

[Pedro Henrique P. Braga]()

<br>

**2016** - **2015** - **2014**

[B√©renger Bourgeois]()

[Xavier Giroux-Bougard]()

[Amanda Winegardner]()

[Emmanuelle Chr√©tien]()

[Monica Granados]()

]
]
</p>
---

# Mat√©riel requis

Pour suivre cet atelier, vous devez avoir t√©l√©charg√© et install√© les derni√®res versions de [RStudio](https://rstudio.com/products/rstudio/download/#download) et de [R](https://cran.rstudio.com/).

.pull-left[
Vous devez √©galement avoir install√© ces paquets:
* [ape](https://cran.r-project.org/package=ape)
* [gclus](https://cran.r-project.org/package=gclus)
* [vegan](https://cran.r-project.org/package=vegan)
* [GGally](https://cran.r-project.org/package=GGally)
* [PlaneGeometry](https://cran.r-project.org/package=PlaneGeometry)
* [remotes](https://cran.r-project.org/package=remotes)

]

.pull-right[


Pour les installer √† partir de CRAN, roulez:

```{r eval = FALSE}
install.packages(c("ape", 
                   "gclus", 
                   "vegan",
                   "GGally", 
                   "PlaneGeometry", 
                   "remotes"))
```
]


<br>

.pull-left2[
Au cours de cet atelier, il y aura une s√©rie de **d√©fis** que vous pouvez reconna√Ætre par ce cube de Rubix.

**Pendant ces d√©fis, n'h√©sitez pas √† collaborer!**
]

.pull-right2[

.center[
![:scale 45%](images/rubicub.png)

]
]

---

# Objectifs d'apprentissage

1. Apprendre les bases de l'analyse multivari√©e pour r√©v√©ler des mod√®les dans les donn√©es sur la composition des communaut√©s.

2. Utiliser `R` pour effectuer une ordination sans contrainte.

3. Apprendre les coefficients de similarit√© et de dissimilarit√© et les transformations des donn√©es.

4. Utiliser `R` pour cr√©er des dendrogrammes.

5. Apprendre les m√©thodes suivantes :

* Analyse de regroupement
* Analyse en composantes principales (ACP)
* Analyse en coordonn√©es principales (ACoP)
* √âchelle multidimensionnelle non m√©trique (PMnM).

---

class: inverse, center, middle

# 1. Pr√©ambule

---

# R√©capitulation : Analyses univari√©es

Nous avons appris une multitude d'analyses qui nous ont permis d'interpr√©ter des donn√©es √©cologiques en d√©crivant les effets d'_une ou plusieurs_ variables sur _une_ variable de r√©ponse.


--

.pull-left[
Nous pouvons rappeler les :

1. Mod√®les lin√©aires g√©n√©raux 
  1. `lm()` ;
  2. `anova()` ;
  3. `t.test()` ;
  4. `lmer()`.

2. Mod√®les lin√©aires g√©n√©ralis√©s
  1. `glm()` et `glmer()` avec plusieurs fonctions de liaison `family()`.

3. Mod√®les additifs g√©n√©ralis√©s
  1. `gam()`.
]

--

.pull-right[
Ces mod√®les nous ont permis de poser des questions telles que :

1. _Quels sont les effets des pr√©cipitations et de la temp√©rature sur la richesse des esp√®ces ?
2. 2. _Comment l'abondance des microbes change-t-elle entre les h√¥tes ?
3. Les poissons cooccurrents deviennent-ils plus agressifs apr√®s avoir √©t√© induits √† la peur ?
]

---

# Statistiques multivari√©es

Cependant, on peut √™tre int√©ress√© √† faire des inf√©rences √† partir de donn√©es √©cologiques contenant _plus d'une_ variable d√©pendante.

Cet int√©r√™t peut √™tre motiv√© par la validation d'hypoth√®ses et la mod√©lisation, mais peut aussi √™tre enti√®rement exploratoire. 

--

Par exemple, notre question de recherche pourrait √™tre la suivante

1. Comment la _composition bact√©rienne_ des feuilles d'√©rable change-t-elle le long du gradient d'altitude ?

2. Quelle est la _dissimilarit√© compositionnelle_ des communaut√©s de chauves-souris ?

3. Quel est le rapport entre les communaut√©s locales d'araign√©es et leur _composition_ ?

--

Dans tous ces cas, le r√©sultat est compos√© de plusieurs variables, _e.g._ g√©n√©ralement une matrice √©chantillon par esp√®ce ou √©chantillon par environnement.

--

Nous allons maintenant plonger dans les **statistiques multivari√©es**, un ensemble d'outils qui nous permettront de r√©pondre √† des questions n√©cessitant l'observation ou l'analyse simultan√©e de plus d'une variable de r√©sultat.

---

# Contexte des m√©thodes multivari√©es

1. Mesures et matrices d'association (ou de dissimilarit√©)

2. Analyse de classification (ou _cluster_)

3. Ordination sans contrainte

4. Ordination contrainte (ou canonique)

--

<br>

.center[
Mais, avant cela, il faut rappeler les bases de **l'alg√®bre matricielle**.
]

---

# Alg√®bre matricielle : un r√©sum√© tr√®s bref

L'alg√®bre matricielle est bien adapt√©e √† l'√©cologie, car la plupart (si ce n'est la totalit√©) des _ensembles de donn√©es_ avec lesquels nous travaillons sont dans un format _matriciel_.


.pull-left[
Les tableaux de donn√©es √©cologiques sont obtenus sous forme d'observations d'objets ou d'unit√©s d'√©chantillonnage, et sont souvent enregistr√©s comme tels :
<br>

| Objets | $y_1$     | $y_2$  | $\dots$           | $y_n$  |
| :-------------: |:-------------:| :-----:|:-----:|:-----:|
|  $x_1$        | $y_{1,1}$     | $y_{1,2}$  | $\dots$  | $y_{1,n}$  |
|  $x_2$        | $y_{2,1}$     | $y_{2,2}$  | $\dots$  | $y_{2,n}$  |
|  $\vdots$     | $\vdots$     | $\vdots$  | $\ddots$  | $\vdots$  |
|  $x_m$        | $y_{m,1}$     | $y_{m,2}$  | $\dots$  | $y_{m,n}$  |
<br>
o√π $x_m$ est l'unit√© d'√©chantillonnage $m$ ; et $y_n$ est le descripteur √©cologique qui peut √™tre, _e.g._, l'esp√®ce pr√©sente dans une unit√© d'√©chantillonnage ou une variable chimique.
]

--

.pull-right[

Le m√™me tableau de donn√©es √©cologiques peut √™tre repr√©sent√© en _notation matricielle_ de la mani√®re suivante :
$$Y = [y_{m,n}] =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}$$

o√π les lettres minuscules indiquent les _√©l√©ments_, et les lettres en indice indiquent la _position de ces √©l√©ments_ dans la matrice (et dans le tableau !).
]

---

# Alg√®bre matricielle : un r√©sum√© tr√®s bref

L'alg√®bre matricielle est bien adapt√©e √† l'√©cologie, car la plupart (si ce n'est la totalit√©) des _ensembles de donn√©es_ avec lesquels nous travaillons sont dans un format _matriciel_.

.pull-left[
De plus, tout sous-ensemble d'une matrice peut √™tre identifi√© !

<br>

.center[_une matrice de lignes_]
$$\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}$$

.center[_une matrice √† colonnes_]
$$\begin{bmatrix}
y_{1,1} \\ y_{2,2} \\ \vdots \\ y_{m,2}
\end{bmatrix}$$

]

.pull-right[
Le m√™me tableau de donn√©es √©cologiques peut √™tre repr√©sent√© en _notation matricielle_ de la mani√®re suivante :
$$Y = [y_{m,n}] =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}$$

o√π les lettres minuscules indiquent les _√©l√©ments_, et les lettres en indice indiquent la _position de ces √©l√©ments_ dans la matrice (et dans le tableau !).
]


---
# Alg√®bre matricielle : un r√©sum√© tr√®s bref

###### Matrices d'association

Deux matrices importantes peuvent √™tre d√©riv√©es de la matrice des donn√©es √©cologiques : la _**matrice d'association entre objets**_ et la _**matrice d'association entre descripteurs**_.

--

.pull-left[
En utilisant les donn√©es de notre matrice $Y$,


<div class="math">
\[ Y = 
\begin{array}{cc}
\begin{array}{ccc}
x_1 \rightarrow\\
x_2 \rightarrow\\
\vdots \\
x_m \rightarrow\\
\end{array}
&
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}
\end{array}
\]
</div>

on peut examiner la relation entre les deux premiers objets :

<div class="math">
\[x_1 \rightarrow \begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}
\]
</div>

<div class="math">
\[x_2 \rightarrow 
\begin{bmatrix}
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\end{bmatrix}
\]
</div>

<p>et obtenir \(a_{1,2}\). </p>

]

--

.pull-right[

Nous pouvons remplir la matrice d'association $A_{n,n}$ avec les relations entre tous les objets de $Y$ :

<div class="math">
\[A_{n,n} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{n,2} & \cdots & a_{n,n} 
\end{bmatrix}\]
</div>

<p>Parce que \(A_{n,n}\) a le m√™me nombre de lignes et de colonnes, elle est not√©e <i>matrice carr√©e</i>.</p> 

<p>Par cons√©quent, elle poss√®de \(n^2\) √©l√©ments.</p>

]


---
# Alg√®bre matricielle : un r√©sum√© tr√®s bref

###### Matrices d'association

Deux matrices importantes peuvent √™tre d√©riv√©es de la matrice des donn√©es √©cologiques : la _**matrice d'association entre objets**_ et la _**matrice d'association entre descripteurs**_.


Ces matrices sont la base des analyses **_mode Q_** et **_mode R_** en √©cologie. 

.pull-left[
$$Y_{m,n} = 
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n} 
\end{bmatrix}$$
]

.pull-right[

$$A_{m,m} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m} 
\end{bmatrix}$$

]

.pull-left[

$$A_{n,n} = 
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{m,2} & \cdots & a_{n,n} 
\end{bmatrix}$$

]

.pull-right[

.pull-left[

<br>

ü°ê Analyse **_mode R_** pour les descripteurs ou les esp√®ces

]

.pull-right[
ü†Ö Analyse **_mode Q_** pour les objets ou les sites
]
]

---
## Nos projets pour les prochaines √©tapes...

Nous allons nous plonger dans les analyses en **R-mode** et **Q-mode**, et nous allons explorer :

1. Les coefficients d'association : dissimilarit√© et similarit√©
2. La transformation des donn√©es de composition
3. Analyses de regroupement
4. Analyses d'ordination dans l'espace r√©duit

<br> 

.center[**Mais, tout d'abord, pr√©sentons un ensemble de donn√©es r√©elles.**]

.center[_Ce sera votre tour de vous salir les mains!_]

---
# Communaut√©s de poissons de la rivi√®re Doubs

.pull-left3[
Verneaux (1973) a propos√© d'utiliser les esp√®ces de poissons pour caract√©riser les zones √©cologiques le long des cours d'eau europ√©ens.

Il a recueilli des donn√©es dans **30 localit√©s** le long de la rivi√®re Doubs, qui coule pr√®s de la fronti√®re franco-suisse, dans les montagnes du Jura. Il a montr√© que les communaut√©s de poissons √©taient des indicateurs biologiques de ces masses d'eau. 
]

.pull.right3[
.center[![:scale 28%](images/DoubsRiver.png)]
]

--

.pull-left3[
Leurs donn√©es sont r√©parties en trois matrices :

1. L'abondance de 27 esp√®ces de poissons dans les communaut√©s ;
2. Les variables environnementales enregistr√©es sur chaque site ; et,
3. Les coordonn√©es g√©ographiques de chaque site.
]

.pull.right3[
.center[![:scale 30%](https://upload.wikimedia.org/wikipedia/commons/c/cc/Doubs_Laissey.jpg)]

<br>
]

.xsmall[Verneaux, J. (1973) _Cours d'eau de Franche-Comt√© (Massif du Jura). Recherches √©cologiques sur le r√©seau hydrographique du Doubs_. Essai de biotypologie. Th√®se d'√©tat, Besan√ßon. 1‚Äì257.] 

---
# Communaut√©s de poissons de la rivi√®re Doubs

.pull-left3[

Nous pouvons charger leurs donn√©es depuis le r√©pertoire `data/` dans cet atelier :

```{r}
spe <- read.csv("data/doubsspe.csv", 
                row.names = 1) 
env <- read.csv("data/doubsenv.csv", 
                row.names = 1)
```

]

--

.pull-right3[

Leurs donn√©es peuvent √©galement √™tre r√©cup√©r√©es dans le paquet `ade4` :
```{r eval=FALSE, echo = TRUE}
library (ade4)
data (doubs)
spe <- doubs$fish
env <- doubs$env
```

Alternativement, √† partir du paquet `codep` :
```{r eval=FALSE, echo = TRUE}
library (codep)
data (Doubs)
spe <- Doubs.fish
env <- Doubs.env
```
]

---
# Donn√©es environnementales de la rivi√®re Doubs

Nous pouvons alors explorer les objets contenant nos donn√©es nouvellement charg√©es. Jetons un coup d'oeil aux donn√©es `env` :

.pull-left[
```{r, echo = T}
str(env)
```

]

.pull-right[

|Variable |Description|
|:--:|:--|
|das|Distance from the source [km]  |
|alt|Altitude [m a.s.l.]  |
|pen|Slope [per thousand]  |
|deb|Mean min. discharge [m<sup>3</sup>s<sup>-1</sup>]  |
|pH|pH of water  |
|dur|Ca conc. (hardness) [mgL<sup>-1</sup>]  |
|pho|K conc. [mgL<sup>-1</sup>]  |
|nit|N conc. [mgL<sup>-1</sup>]  |
|amn|NH‚ÇÑ‚Å∫ conc. [mgL<sup>-1</sup>]  |
|oxy|Diss. oxygen [mgL<sup>-1</sup>]  |
|dbo|Biol. oxygen demand [mgL<sup>-1</sup>]  |

]
```{r, eval = F}
names(env)
dim(env) # dimensions
summary(env) # sommaire
head(env)
```

]

---
# Communaut√©s de poissons de la rivi√®re Doubs

Jetons un coup d'≈ìil aux donn√©es `spe` :

.pull-left[
```{r, echo = T}
head(spe)[, 1:8]
```

```{r, echo = T, output.lines=1:8}
str(spe)
```
]

.pull-right[

```{r, eval = F}
# Essayez-en quelques-uns !
names(spe)   # noms d'objets
dim(spe)     # dimensions
str(spe)     # structure des objets
summary(spe) # sommaire
head(spe)    # debut
```

]

---
# Mesures de (dis)similarit√©

La ressemblance des communaut√©s est frequement √©valu√©e sur la composition des esp√®ces, sous la forme d'un tableau de donn√©es site par esp√®ce $Y_{m,n}$.

On peut obtenir une matrice d'association $A_{m,m}$ sous la forme de distances ou de dissimilarit√©s par paires $D_{m,m}$ (ou de similarit√©s $S_{m,m}$), puis les analyser.

--

.pull-left[
Dans `R`, nous pouvons calculer des matrices de distance en utilisant `stats::dist()` :

```{r, eval = T}
dist(spe)
```

]

--

.pull-right[

Par mesure de simplification, nous avons fait cela sans sp√©cifier d'arguments.

Essayez `dist(spe)` de votre c√¥t√©, notez ce que vous observez et voyez ce que les commandes ci-dessous vous montrent :

```{r, eval = F}
class(dist(spe))
```

```{r, eval = F}
str(dist(spe))
```

```{r, eval = F}
as.matrix(dist(spe))
```

```{r, eval = F}
dim(as.matrix(dist(spe)))
```

]

??? 
Si possible, faites cette partie avec les participants.

Montrez-leur comment est structur√© un objet de classe dist. Soulignez qu'il s'agit d'une matrice triangulaire inf√©rieure, o√π les distances entre les m√™mes objets sont cach√©es.

---

# Fr√©quences des esp√®ces

Combien de z√©ros?

```{r}
sum(spe == 0)
```

Quelle proportion de z√©ros?

```{r}
sum(spe == 0)/(nrow(spe)*ncol(spe))
```

---
# Richesse totale en esp√®ce

Observer le nombre d'esp√®ces pr√©sentes dans chaque site :

```{r, fig.width=10, fig.height=5, echo=-1}
par(mar = c(4,4,1,.5), cex = 1.5)
site.pre <- rowSums(spe > 0)
barplot(site.pre, main = "Richesse sp√©cifique",
        xlab = "Sites", ylab = "Nombre d'esp√®ces",
        col = "grey ", las = 1)
```

---
# Comprenez vos donn√©es!

.center[...pour choisir la transformation et la distance appropri√©e]


- Y-a-t-il beaucoup de z√©ros?

- Que veulent-ils dire?


.alert[Une mesure de 0 (e.g 0mg/L, 0¬∞C) n'est pas √©quivalent √† un 0 repr√©sentant une absence d'observation.]


---
# Avant de transformer vos donn√©es de composition des communaut√©s...

.alert[Consid√©rations importantes:]

--
- abondances/comptes/pr√©sence-absence relatives?

--
- distributions asymm√©triques ?

--
- beaucoup d'esp√®ces rares?

--
- surabondance d'esp√®ces dominantes?

--
- probl√®me de double Z√©ro?

---
# Transformer les donn√©es de composition des communaut√©s

.center[
![](images/trans1.png)]

---
# Transformer les donn√©es de composition des communaut√©s

## Exemples

Transformer des comptes en pr√©sence - absence
```{r}
library(vegan)
spec.pa <- decostand(spe, method = "pa")
```

R√©duire le poids des esp√®ces rares

```{r}
spec.hel <- decostand(spe, method = "hellinger")
spec.chi <- decostand(spe, method = "chi.square")
```

R√©duire le poids des esp√®ces abondantes

```{r}
spe.pa <- decostand(spe, method = "log")
```

---
# Donn√©es sur l'environnement

```{r, eval = FALSE}
names(env) # Names of objects
dim(env) # dimensions
str(env) # structure of objects
summary(env) # summary statistics
head(env) # first 6 rows
```

```{r}
head(env) # first 6 rows
```

Explorer la colin√©arit√© en visualisant les corr√©lations entre les variables:

```{r, eval = F, purl = FALSE}
library(GGally)
ggpairs(env, main="Bivariate Plots of the Environmental Data")
```

---
# Donn√©es sur l'environnement

```{r, echo = F, fig.align='center', fig.height = 8, fig.width = 10}
library(GGally)
ggpairs(env, main="Graphiques bivari√©s des donn√©es environnementales")
```

---
# Standardisation

Standardiser les variables environnementales est indispensable car il est impossible de comparer des variables d'unit√©s diff√©rentes :

```{r, eval = -1}
?decostand
env.z <- decostand(env, method = "standardize")
```

<Br>
Cette fonction centre-r√©duit les donn√©es pour assurer la fiabilit√© des analyses:

```{r}
apply(env.z, 2, mean)
apply(env.z, 2, sd)
```



---
class: inverse, center, middle
# 3. Similarit√© / Dissimilarit√©


---
# Mesure d'association

L'alg√©bre matricielle est au coeur de plusieurs m√©thodes d'analyses multivari√©es

$$
\begin{bmatrix}
    a_{1,1} & a_{1,2} & a_{1,3} & \dots  & a_{1,n} \\
    a_{2,1} & a_{2,2} & a_{2,3} & \dots  & a_{2,n} \\
    a_{3,1} & a_{3,2} & a_{3,3} & \dots  & a_{3,n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & a_{m,3} & \dots  & a_{m,n}
\end{bmatrix}
$$

- Explorer diff√©rentes mesures de distance entre objets permet de mieux comprendre le fonctionnement de l'ordination

???
.center[![](images/MatrixAlgebra.png)]
---
# Au-d√©l√† de la 1√®re dimension

.pull-left[

- Les jeux de donn√©es √©cologiques correspondent souvent √† de grandes matrices.

- L'ordination calcule les relations entre esp√®ces, ou entre objets.

- Ces relations peuvent √™tre simplifi√©es par des mesures de dissimilarit√©s.
]

.pull-right[

![:scale 40%](images/PCAMatrix.png)

![:scale 40%](images/distMes.png)

![:scale 40%](images/distMat.png)
]


---
# Similarit√© / Dissimilarit√©

- Utile pour comprendre vos donn√©es
- Certains types d'ordination ou de groupement n√©cessitent des mesures appropri√©es

.center[
Similarit√©: S = 1 - D
Distance: D = 1-S]


![](images/similarity.png)

---
# Mesures de distance des communaut√©s

.pull-left[
- **Euclidienne**  
  $d_{jk} = \sqrt(\sum((x_{ij}-x_{ik})^2))$  
  Utilis√© en PCA et RDA.
  <br><br>
- **Manhattan**
  $d_{jk} = \sum(abs(x_{ij} - x_{ik}))$  
  Similaire √† l'Euclidienne, mais sensible au probl√®me de doubles z√©ros.
  <br><br>
- **Corde**  
  Distance Euclidienne calcul√©e pour une matrix standardis√©e par rang√©es.
  <br><br>
]

.pull-right[
- **Hellinger**  
  Racines carr√©es de probabilit√©s conditionnelles.  
  S'ex√©cute bien en ordination lin√©aire.
  <br><br>
- **Khi-carr√©**  
  $d_{jk} = \sum(\frac{(x_{ij}-x_{ik})^2}{(x_{ij}+x_{ik})} ) / 2$   
  Utilis√© en CA.
  <br><br>
- **Bray-Curtis** (Semim√©trique) 
  $d_{jk} = \frac{\sum(abs(x_{ij}-x_{ik}))}{(\sum (x_{ij}+x_{ik}))}$  
  Utilis√© en PCoA et NMDS.
]
<br>
.center[

*O√π $x_{ij}$ $x_{ik}$ sont le nombre d'esp√®ces (colonnes) $i$ dans les sites (rang√©es) $j$ et $k$.*]

--

.alert[Chacune de ces distances peut √™tre appropri√©e pour diff√©rentes situations.]

???
Notes: Ref: 
https://sites.google.com/site/mb3gustame/reference/dissimilarity
Euclidienne : Une m√©trique simple et sym√©trique utilisant la formule de Pythagore. Plus il y a de variables dans un ensemble de donn√©es, plus on peut s'attendre √† ce que les distances euclidiennes soient grandes. En plus, les doubles z√©ros entra√Ænent une diminution des distances. Cette propri√©t√© rend la distance euclidienne inadapt√©e √† de nombreux ensembles de donn√©es √©cologiques et des transformations √† motivation √©cologique doivent √™tre envisag√©es.  L'analyse en composantes principales et l'analyse de la redondance ordonnent les objets en utilisant les distances euclidiennes.

---
# Comparaison des sites de la rivi√®re Doubs

La fonction `vegdist()` comprend les mesures de distances communes :

```{r, eval = FALSE}
?vegdist
```
Comment la composition des communaut√©s diff√®re-t-elle entre les 30 sites de la rivi√®re Doubs?

```{r}
spe.db.pa <- vegdist(spe, method = "bray")
spe.db <- as.matrix(spe.db.pa)

#         1         2         3         4         5         6         7         9
# 1  0.0000000 0.6000000 0.6842105 0.7500000 0.8918919 0.7500000 0.6842105 1.0000000
# 2  0.6000000 0.0000000 0.1428571 0.3333333 0.6956522 0.3939394 0.1428571 0.6923077
# 3  0.6842105 0.1428571 0.0000000 0.1891892 0.6800000 0.2972973 0.1250000 0.7333333
#...

```

--
<br>  
- La diagonale est compos√©e de z√©ros (chaque site est compar√©e √† lui-m√™me).  
- Sites 3 et 7 sont les plus similaires (*plus petite distance*).  
- Sites 1 et 9 sont les plus dissimilaires (1 = *compl√®tement* dissimilaire).

---
exclude: true

# Comparaison des sites

.center[![](images/Doubs1.png)]

---
exclude: true 

# Comparaison des sites

.center[![](images/Doubs2.png)]

---
# Visualisation d'une matrice de distances

```{r, echo = FALSE}
# coldiss() function
# Color plots of a dissimilarity matrix, without and with ordering
#
# License: GPL-2 
# Author: Francois Gillet, 23 August 2012
#

"coldiss" <- function(D, nc = 4, byrank = TRUE, diag = FALSE)
{
	require(gclus)

	if (max(D)>1) D <- D/max(D)

	if (byrank) {
		spe.color <- dmat.color(1-D, cm.colors(nc))
	}
	else {
		spe.color <- dmat.color(1-D, byrank=FALSE, cm.colors(nc))
	}

	spe.o <- order.single(1-D)
	speo.color <- spe.color[spe.o, spe.o]
	
	op <- par(mfrow=c(1,2), pty="s")

	if (diag) {
		plotcolors(spe.color, rlabels=attributes(D)$Labels, 
			main="Dissimilarity Matrix", 
			dlabels=attributes(D)$Labels)
		plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o], 
			main="Ordered Dissimilarity Matrix", 
			dlabels=attributes(D)$Labels[spe.o])
	}
	else {
		plotcolors(spe.color, rlabels=attributes(D)$Labels, 
			main="Dissimilarity Matrix")
		plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o], 
			main="Ordered Dissimilarity Matrix")
	}

	par(op)
}

# Usage:
# coldiss(D = dissimilarity.matrix, nc = 4, byrank = TRUE, diag = FALSE)
# If D is not a dissimilarity matrix (max(D) > 1), then D is divided by max(D)
# nc 							number of colours (classes)
# byrank= TRUE		equal-sized classes
# byrank= FALSE		equal-length intervals
# diag = TRUE			print object labels also on the diagonal

# Example:
# coldiss(spe.dj, nc=9, byrank=F, diag=T)
```


```{r, fig.width = 10, fig.height = 8}
# Le code pour cr√©er la fonction coldiss() est dans le script de l'atelier.
coldiss(spe.db.pa)
```


---
# D√©fi #1 ![:cube]()

<br/>

Discuter en groupes:

<br/>

.center[**Comment savoir si deux objets caract√©ris√©s par des donn√©es multidimensionnelles sont similaires?**]

<br/>

- Faites une liste de vos suggestions


---
# Et qu'en est-il de l'ordination?

Avec des m√©thodes d'ordination, nous ordonnons vos objets (sites) en fonction de leur similarit√©

- Plus les sites sont similaires, plus ils sont proches dans l'espace d'ordination (plus petites distances)

- En √©cologie, on calcule habituellement la similarit√© entre sites en fonction de leur composition en esp√®ces ou de leur conditions environnementales.


---
# Analyse sch√©matique des analyses multivari√©es

.center[![:scale 120%](images/Schema2.png)]

???
Note: re-added the picture, rename it as Schema 2.

---
# Groupement

- Permet de mettre en lumi√®re des structures dans les donn√©es en partitionnant les objets

- Les r√©sultats sont repr√©sent√©s sous forme de dendrogramme (arbre)

- Pas une m√©thode statistique!

```{r, fig.width=10, echo = FALSE}
# D√©monstration d'un dendrogramme
spe.hel<-decostand(spe, method="hellinger")
spe.dhel<-vegdist(spe.hel,method="euclidean")
spe.dhel.ward<-hclust(spe.dhel, method="ward.D2")
spe.dhel.ward$height<-sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang=-1) # hang=-1 aligns all objets on the same line
```

???
.center[
![:scale 80%](images/cluster1.png)]

---
# Aper√ßu de 3 m√©thodes hi√©rarchiques

<br>

**1.** Groupement agglom√©ratif √† liens simples ("single").

<br>

**2.** Liens complets, groupement agglom√©ratif ("complete")

<br>

**3.** Groupement √† variance minimale de Ward ("ward.D2")

<br>

Les √©l√©ments de rang(s) inf√©rieur(s) sont imbriqu√©s dans des clusters de rang sup√©rieur. Par exemple, les esp√®ces sont regroup√©es par genre, famille, ordre, et ainsi de suite.

???
Dans le clustering agglom√©ratif √† liens simples (√©galement appel√© tri par plus proches voisins), les objets √† l'emplacement de l'objet sont class√©s par ordre de priorit√© et les objets les plus proches s'agglom√®rent.
Ceci g√©n√®re souvent de longues et fines grappes ou cha√Ænes d'objets. √Ä l'inverse, dans le clustering agglom√©ratif √† liens complets, un objet ne s'agglom√®re √† un groupe que lorsqu'il est li√© √† l'√©l√©ment le plus √©loign√© du groupe, qui le lie √† son tour √† tous les membres de ce groupe. Il formera de nombreux petits groupes s√©par√©s, et est plus appropri√© pour rechercher des contrastes, des discontinuit√©s dans les donn√©es.
Le clustering √† variance minimale de Ward diff√®re de ces deux m√©thodes en ce que
elle regroupe les objets en groupes en utilisant le crit√®re des moindres carr√©s
(similaire aux mod√®les lin√©aires). Son dendogramme montre par d√©faut les distances au carr√©. Pour comparer avec les autres m√©thodes, calculez d'abord la racine carr√©e des distances. 

---
# Groupement hi√©rarchique

√Ä partir d'une matrice de distances, on classe les objets en ordre croissant

![](images/Hierachic1.png)


---
# Groupement √† liens simples

.pull-left[

![:scale 50%](images/singleClust1.png)
--

]

.pull-right[

- Les deux objets les plus proches se regroupent

- Ensuite les deux objets les plus proches suivants

- et ainsi de suite.

![](images/singleClust2.png)

]

---
# Groupement √† liens complet

.pull-left[

![:scale 50%](images/compleClust1.png)

]

.pull-right[

- Les deux objets les plus proches se regroupent

- Ensuite les groupes se lient √† la distance √† laquelle les objets qu'ils contiennent sont tous li√©s

![](images/compleClust2.png)
]


---
# Comparaison

Cr√©er une matrice de distance √† partir des donn√©es de la rivi√®re Doubs transform√©es Hellinger et faire le groupement √† liens simples :

```{r, fig.width=7, echo = -1}
par(mar=c(.5,3.8,2,.5), cex = 1.5)
spe.dhe1 <- vegdist(spec.hel, method = "euclidean")
spe.dhe1.single <- hclust(spe.dhe1, method = "single")
plot(spe.dhe1.single)
```

---
# Comparaison

```{r,fig.width= 15, echo = -1}
par(mfrow=c(1,2), mar=c(.5,2.5,1.5,2.5), cex=1)
spe.dhe1 <- vegdist(spec.hel, method = "euclidean")
spe.dhe1.complete <- hclust(spe.dhe1, method = "complete")
plot(spe.dhe1.single, main="Single linkage clustering", hang =-1)
plot(spe.dhe1.complete, main="Complete linkage clustering", hang=-1)
```

.pull-left[

**Liens simples :**

Les objets ont tendance √† s'encha√Æner (e.g. 19,29,30,26).
]

.pull-right[

**Liens complets :**  

Les groupes sont plus distincts.
]

---
# Groupement de Ward

Le groupement de Ward utilise la m√©thode des moindres carr√©s pour lier les objets.
- Les groupes fusionnent de fa√ßon √† minimiser la variance intragroupe.
- C'est-√†-dire qu'√† chaque √©tape, la paire de groupes qui r√©sulte √† la plus petite augmentation de la somme des carr√©s des √©carts intra-groupes est celle qui fusionne.


---
# Groupement de Ward

Faire le groupement de Ward et dessiner le dendrogramme en utilisant la racine carr√©e des distances :

```{r, fig.width=9, echo = -1}
par(mar=c(.5,3.8,2,.5), cex = 1.5)
spe.dhel.ward <- hclust(spe.dhe1, method = "ward.D2")
spe.dhel.ward$height <- sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang = -1) # hang = -1 aligns objects at the same level
```

---
# Groupement de Ward

```{r, fig.width=9, echo = FALSE}
par(mar=c(.5,3.8,2,.5), cex = 1.5)
plot(spe.dhel.ward, hang = -1) # hang = -1 permet d'afficher les objets sur la m√™me ligne
```

Les objets ont tendance √† former des groupes plus homog√®nes (en termes du nombdre d'objets inclus dans chaque groupe).

---
# Comment choisir la bonne m√©thode ?

- Choisir une m√©thode appropri√©e d√©pend de votre objectif.
  - D√©montrer des gradients? des contrastes?
- Si plus d'une m√©thode semble ad√©quate, comparer les dendrogrammes.
- Encore une fois : ceci **n'est pas** une m√©thode statistique! 

Mais, le groupement permet de:
- D√©terminer un nombre de groupes optimal.
- Faire des tests statistiques sur le groupement r√©sultant.
- Combiner le groupement √† l'ordination pour distinguer des groupes de sites.

---

#### Et maintenant ?

Alors que l'**analyse de regroupements** remonte les *discontinuit√©s* dans un ensemble de donn√©es, l'**ordination** extrait les principales tendances sous la forme d'axes continus.

Nous allons examiner maintenant trois types de **m√©thodes d'ordination sans contrainte**...

--

.right[...**attendez**, qu'est-ce qu'on entend par **ordination sans contrainte** ? *Quelqu'un* ?]

--
<br>
.center[*Si personne ne s'exprime, choisissez une personne "volontaire" !*]

--

**Les ordinations non contraintes** √©valuent les relations chez un seul ensemble de variables. *Aucune tentative* n'est faite pour d√©finir la relation entre un ensemble de variables ind√©pendantes et une ou plusieurs variables d√©pendantes.

--

L'interpr√©tation des effets potentiels d'autres facteurs sur les patrons observ√©s ne peut se faire qu'indirectement, car ces facteurs ne sont *pas* explicitement inclus dans les analyses !

--

Ici, nous allons explorer :
.small[
.center[
.pull-left2[
**A**nalyse des **C**omposants **P**rincipales 

**A**nalyse des **Co**ordonn√©es **P**rincipales
]

.pull-right2[
**√â**chelonnement **M**ultidimensionnel **N**on-**M**√©trique
]
]
]


---
### Mais d'abord, faisons un *r√©capitulatif*...

Nous avons d√©j√† compris la signification de **variance** et **moyenne**, et comment les calculer :


.pull-left[
$$\sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$
]

.pull-right[
$$\mu_x = \frac{1}{n} \sum_{i=i}^{n} x_{i}$$
]

Elles sont tr√®s utiles pour comprendre le *centre* et la *dispersion* d'une variable ou d'une dimension donn√©e.

N√©anmoins, nous sommes souvent int√©ress√©s **par plus d'une dimension**, et nous voulons mesurer dans quelle mesure chaque dimension varie de la moyenne *par rapport aux autres*.

--

La **covariance** est une telle mesure, qui peut d√©crire comment **deux dimensions co-varient** :

.pull-left[
$$var_x = \sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$
]

.pull-right[
$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$
]

---

### Mais d'abord, faisons un *r√©capitulatif*...

Intuitivement, nous pouvons mesurer la **covariance** entre plus de deux variables. Disons, entre les variables $x$, $y$, et $z$ :

.pull-left[
$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$
]

.pull-right[
$$cov_{x,z}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(z_{i}-\bar{z})}{N-1}$$
]

$$cov_{z,y}=\frac{\sum_{i=1}^{N}(z_{i}-\bar{z})(y_{i}-\bar{y})}{N-1}$$
Nous pouvons repr√©senter ces calculs dans une **matrice de covariance** :

$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & cov_{y,y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & cov_{z,z}   
\end{array} \right]$$

--

**QUIZ TIME**: *Que sont les diagonales ?* *Et, que se passe-t-il si les variables sont ind√©pendantes ?*

---

##### Toujours en train de *r√©capituler*...

.center[***Que sont les diagonales ?***]

Si, $cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$, alors:

$$cov_{x,x}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(x_{i}-\bar{x})}{N} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2} {N} = var_x$$
--

Afin que :

$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & cov_{y,y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & cov_{z,z}   
\end{array} \right] = \left[ \begin{array}{ccc} 
var_{x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & var_{y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & var_{z}   
\end{array} \right]$$

 <br>
 
.center[La covariance d'une variable avec elle-m√™me est sa *variance* !]

---

##### Toujours en train de *r√©capituler*...

.center[***Que se passe-t-il si les variables sont ind√©pendantes ?***]

.pull-left[
```{r xyz-norm, echo=TRUE}
x <- rnorm(5000, mean = 0, sd = 1)
y <- rnorm(5000, mean = 0, sd = 1)
z <- rnorm(5000, mean = 0, sd = 1)

xyz <- data.frame(x, y, z)

GGally::ggpairs(xyz) 
```
]

--

.pull-right[
```{r xyz-cov, echo=TRUE, eval = FALSE}
cov(xyz)
```

```{r xyz-cov-2, echo=FALSE, eval = TRUE}
round(cov(xyz), digits = 5)
```

Si les variables sont parfaitement ind√©pendantes (ou non corr√©l√©es), la matrice de covariance $C(x, y, z)$ est :

$${C(x, y, z)} =  \left[ \begin{array}{ccc} 
var_{x} & 0 & 0 \\
0 & var_{y} & 0 \\
0 & 0 & var_{z}   
\end{array} \right]$$

*i*.*e*. une covariance plus proche de $1$ signifie que les variables sont *colin√©aires*.

Et ici, $var_{x} = var_{y} = var_{z} = 1$.

]

---
## Transformations lin√©aires

Nous sommes souvent int√©ress√©s √† l'observation des variables d'autres *formes*. 

Pour cela, nous cr√©ons une nouvelle variable, disons $x_{new}$, en utilisant des constantes pour modifier la variable originale $x$. Par exemple :

--

.pull-left[
On peut transformer des distances mesur√©es en kilom√®tres $d_{km}$ en miles :
$$d_{mi} = 0.62 \times d_{km}$$
]

.pull-right[
On peut aussi transformer des degr√©s Fahrenheit en degr√©s Celsius : 

$$T_{C} = 0.5556\times T_{Fahr} - 17.778$$
]

Ces exemples sont des **transformations lin√©aires** car les variables transform√©es sont li√©es lin√©airement aux variables d'origine et les formes de la distribution ne sont pas modifi√©es.

--

Deux types de transformations sont tr√®s importantes pour nous :

.pull-left[
**Centrage**, qui soustrait les valeurs d'un pr√©dicteur de la moyenne :

$$x' = x_i - \bar{x}$$
]

.pull-right[
**R√©duction**, qui divise chaque valeur d'une variable par son √©cart-type:

$$x'' = \frac{x_i}{\sigma_x}$$
]

??? 

Le centrage est essentiellement une technique o√π la moyenne des variables ind√©pendantes est soustraite de toutes les valeurs. Cela signifie que toutes les variables ind√©pendantes ont une moyenne nulle. La r√©duction est complementaire au centrage. Les variables pr√©dictives sont divis√©es par leur √©cart type.

Le pr√©sentateur doit mentionner ici que le centrage am√®ne la moyenne √† z√©ro et la mise √† l'√©chelle am√®ne l'√©cart type √† une unit√©. Ils doivent √©galement mentionner que les variables deviennent comparables lors de la mise √† l'√©chelle, car leur unit√© est perdue.

---

### D√©composition des matrices en √©l√©ments propres

**Les matrices carr√©es**, telles que la **matrice de covariance**, peuvent √™tre d√©compos√©es en *valeurs propres* et en *vecteurs propres*.

Pour une matrice carr√©e, $A_{n \times n}$, un vecteur $v$ est un *vecteur propre* de $A$ s'il y a un *scalaire*, $\lambda$, pour lequel :

.center[ 
$A_{n \times n} v_{n \times 1} = \lambda  v_{n \times 1}$, or $\left(\begin{matrix}a_{11}&\cdots&a_{1n}\\\vdots&\ddots&\vdots\\a_{1n}&\cdots&a_{nn}\\\end{matrix}\right)\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)=\lambda\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)$ 
]

la valeur de $\lambda$ √©tant la *valeur propre* correspondante.

--

C'est-√†-dire que la matrice $A$ *√©tire* effectivement le vecteur propre $v$ par la quantit√© sp√©cifi√©e par la valeur propre (*scalaire*) $\lambda$.

Un *vecteur propre* est un vecteur dont la direction reste inchang√©e lorsqu'on lui applique une **transformation lin√©aire**.

<br>

.center[Attendez ! Qu'entendons-nous par *direction inchang√©e* ?]

---

### D√©composition des matrices en √©l√©ments propres

.center[Attendez ! Qu'entendons-nous par *direction inchang√©e* ?]

<br>

Repr√©sentons cela avec cet exemple simple.

Nous pouvons transformer un carr√© en un parall√©logramme √† l'aide d'une **transformation affine** √† axe unique.

--

.pull-left[
Soit $S$ le carr√© de vertices $(0,0),\,(1,0),\,(1,1),\,(1,0)$ qui sera transform√© par cisaillement en parall√©logramme $P$ de vertices $(0,0),\,(1,0),\,(1,1.57),\,(1,0.57)$.

Nous pouvons voir qu'apr√®s la transformation lin√©aire, la fl√®che violette n'a pas chang√© de direction, c'est-√†-dire qu'elle est un *vecteur propre* de $S$.

En revanche, la fl√®che rouge a chang√© de direction, et *n'est donc pas* un *vecteur propre* de $S$.
]

.pull-right[
```{r echo = FALSE}
library(PlaneGeometry)

P <- c(0, 0) 
w <- c(1, 0) 

ratio <- 1 
angle <- 30

shear <- Shear$new(P, 
                   w, 
                   ratio, 
                   angle)

wt <- ratio * c(-w[2], w[1])

Q <- P + w
R <- Q + wt
S <- P + wt
A <- shear$transform(P)
B <- shear$transform(Q)
C <- shear$transform(R)
D <- shear$transform(S)


plot(0, 0, type = "n", asp = 1, xlim = c(0,1), ylim = c(0,2))

lines(rbind(P, Q, R, S, P), 
      lwd = 2) # unit square

lines(rbind(A, B, C, D, A), 
      lwd = 2, 
      col = "blue") # image by the shear

arrows(x0 = A[1], 
       y0 = A[2], 
       x1 = B[1],
       y1 = B[2],
       col = "red",
       lwd = 2)

arrows(x0 = A[1], 
       y0 = A[2], 
       x1 = D[1],
       y1 = D[2],
       col = "purple",
       lwd = 2)
```
]

---

##### D√©composition des matrices en √©l√©ments propres : implications

.center[
*Suivons avec la torture alg√©brique !* 
]

### Orthogonalit√©

Une propri√©t√© *fabuleuse* et *simple* des matrices *sym√©triques* que nous pouvons expliquer ici !

.pull-left[
Supposons que $x$ soit un vecteur propre de $A$ correspondant √† la valeur propre $Œª_1$ et $y$ un vecteur propre de $A$ correspondant √† la valeur propre $Œª_2$, avec $Œª_1‚â†Œª_2$.

$$Ax=\lambda_1x \\
Ay=\lambda_2y$$

Multiplions chacun d'eux par l'autre *vecteur propre* transpos√©.
$$y^{\intercal}Ax=\lambda_1y^{\intercal}x \\ x^{\intercal}A^{\intercal}y=\lambda_2x^{\intercal}y$$

]

--

.pull-right[

Maintenant, soustrayons la deuxi√®me √©quation de la premi√®re et utilisons la commutativit√© du produit scalaire :

$y^{\intercal}Ax-x^{\intercal}A^{\intercal}y=\lambda_1y^{\intercal}x - \lambda_2x^{\intercal}y \\ 0 = (\lambda_1 - \lambda_2)y^{\intercal}x$

Parce que nous savons que $\lambda_1-\lambda_2\neq 0$, alors 
$y^{\intercal}x = 0$, *c-*.*√†*.*-d*., $\mathbf{x}\perp\mathbf{y}$, *c-*.*√†*.*-d*. sont **orthogonaux** !

*Alors, que r√©v√®le la d√©composition de la matrice de variance-covariance en elements propres ?*
]

???

L'explication de cette partie est tr√®s utile et assez simple, afin que chacun puisse comprendre ce qu'est l'orthogonalit√©. Il s'agit d'op√©rations simples d'√©quations et de soustractions.  

---
##### D√©composition des matrices en √©l√©ments propres : implications

.center[
*Suivons avec la torture alg√©brique !* 
]

### Maximisation

.pull-left[
Si $v_i' v_i = 1$, alors $Av_i=\lambda_iv_i$ peut √™tre √©crite comme :
$$v_i' A v_i = \lambda_i$$
En effet, $v' A v$ 
est la variance d'une combinaison lin√©aire avec des poids en $v$, *i*.*e*. $\text{Var}(v_i'\,A)=v_i'\,\text{Var}(A)\,v_i$.

*Donc, on peut relier les points !*

]

--

.pull-right[
Rappelez-vous que les *valeurs propres* de notre *matrice de variance-covariance* $A$ sont directement li√©es √† la variance !

Pour trouver un vecteur $v$ qui maximise la variance, $v' A v$, il suffit de choisir le *vecteur propre* correspondant √† la plus grande *valeur propre* $\lambda_i$ !

De sorte que la variance maximale soit de $\lambda_1$ !
]

--

La *variance expliqu√©e* de chaque *vecteur propre* ob√©it √† l'ordre : $\lambda_1 > \lambda_2 > \dots > \lambda_k$.

Cela nous permet de condenser un plus grand nombre de variables originales en un ensemble plus petit de vecteurs s√©lectionn√©s avec une perte minimale d'informations (c'est-√†-dire **une r√©duction de la dimensionnalit√©**).

---
# M√©thodes d'ordination sans contrainte

Ceci est un bon point de d√©part pour nous mettre sur la voie des **m√©thodes d'ordination sans contrainte** que nous allons √©tudier aujourd'hui !

.pull-left[
Ils nous permettent de :
- √âvaluer les relations *dans* un ensemble de variables (esp√®ces ou variables environnementales);
- Trouver les composantes cl√©s de la variation entre les √©chantillons, les sites, les esp√®ces ;
- R√©duire le nombre de dimensions dans les donn√©es multivari√©es tout en limitant la perte substantielle d'informations ;
- Cr√©er de nouvelles variables √† utiliser dans des analyses ult√©rieures.
]

.pull-right[
Ici, nous apprendrons :

1. **P**rincipal **C**omponent **A**nalysis;

2. **P**rincipal **Co**ordinate **A**nalysis;

3. **N**on-Metric **M**ulti**d**imensional **S**caling;
]
---
# Analyse en Composantes Principales

.small[
L'analyse en composantes principales (ACP) est une technique de r√©duction de la dimensionnalit√© *lin√©aire*, c'est-√†-dire qu'elle r√©duit les donn√©es fortement corr√©l√©es.

En bref, l'ACP transforme *lin√©airement* des variables originales vers des nouvelles variables contenant des **composantes principales**, qui expliquent la plupart de la variance dans l'ensemble de donn√©es, c'est-√†-dire qui maximisent la s√©paration entre les donn√©es.
]

--
.pull-left[

.small[L'espace des *composants principaux* peut √™tre √©crit comme suit :]

$$Z_p = ‚àë_{j=1}^p œï_j * X_j$$
]
.small[
o√π, 
1. $Z_p$ est la composante principale $p$ ;
2. $œï_j$ est le vecteur de charge comprenant les $j$ charges pour la composante principale $p$, c'est-√†-dire les coefficients de la combinaison lin√©aire des variables originales √† partir desquelles les composantes principales sont construites ;
3. $X_j$ est le pr√©dicteur normalis√©, c'est-√†-dire avec une moyenne √©gale √† z√©ro et un √©cart-type √©gal √† un.
]

???

C'est-√†-dire que la reconstruction des donn√©es peut √™tre donn√©e par une simple combinaison lin√©aire des composantes.


Les futures versions de ce document devraient inclure une figure (telle qu'une carte thermique) avec les donn√©es normalis√©es originales sur le c√¥t√© gauche, et sur le c√¥t√© droit, une carte thermique des chargements fois une carte thermique des composants (qui peut √™tre consid√©r√©e comme √©gale √† la somme des matrices de rang un) √©tant √©gale √† la carte thermique des donn√©es reconstruites.

---
# Analyse en Composantes Principales

L'ACP peut √™tre calcul√©e d'au moins *quatre* fa√ßons diff√©rentes.

Pour des raisons de simplicit√©, nous nous concentrerons ici sur la fa√ßon d'obtenir des composantes principales √† partir d'une matrice de corr√©lation.

Nous apprendrons √† le faire √† partir de rien, puis √† utiliser les paquets `R` pour calculer les composantes principales.

---

### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;
]

.pull-right3[
Dans `R`, √† partir de z√©ro !

```{r}
data(varechem)

str(varechem)
```

]

---

#### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;
]

.pull-right3[
Dans `R`, √† partir de z√©ro !

```{r}
data(varechem)

# √âtape 1 
Y <- varechem[, 1:2]

head(Y)
```

]

---
#### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient √† centrer, comme dans $y_c = [y_i - \bar{y}]$, puis √† mettre √† l'√©chelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;
]

.pull-right3[
Dans `R`, √† partir de z√©ro !

```{r}
data(varechem)

# √âtape 1 
Y <- varechem[, 1:2]

# √âtape 2
Y_std <- as.matrix(scale(Y))

head(Y_std)

round(apply(Y_std, 2, mean))
round(apply(Y_std, 2, sd))

```

]

---

#### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient √† centrer, comme dans $y_c = [y_i - \bar{y}]$, puis √† mettre √† l'√©chelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;
]

.pull-right3[
Dans `R`, √† partir de z√©ro !

```{r}
data(varechem)

# √âtape 1 
Y <- varechem[, 1:2]

# √âtape 2
Y_std <- as.matrix(scale(Y))

# √âtape 3
(Y_R <- cov(Y_std))
```
]

---

#### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient √† centrer, comme dans $y_c = [y_i - \bar{y}]$, puis √† mettre √† l'√©chelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;

4. Effectuer la d√©composition de la matrice de covariance pour obtenir la matrice $U$ des vecteurs propres, contenant les *composantes principales* ;
]

.pull-right3[
Dans `R`, √† partir de z√©ro !

```{r}
data(varechem)

# √âtape 1
Y <- varechem[, 1:2]

# √âtape 2
Y_std <- as.matrix(scale(Y))

# √âtape 3
Y_R <- cov(Y_std)

# √âtape 4
(Eigenvalues <- eigen(Y_R)$values)

(Eigenvectors <- eigen(Y_R)$vectors)
```
]

---

### Analyse en composantes principales : √©tape par √©tape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, √† chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[

Nous pouvons repr√©senter les distances des observations au premier vecteur propre (`PC1`, en rouge).

La premi√®re composante principale est dessin√©e de fa√ßon √† ce que la variation des valeurs le long de sa ligne soit maximale. 

Les fl√®ches sur les composantes principales sont obtenues en multipliant leurs *valeurs propres* par les *vecteurs propres*.
]

.pull-right[
```{r echo=FALSE}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

plot(N ~ P, 
     col = as.factor(rownames(Y_std)), 
     main="Distances to PC1", 
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0, 
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0, 
#        y0 = 0, 
#        x1 = Eigenvalues[2]*Eigenvectors[1,2], 
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange", 
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0, 
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  #finds endpoint for a perpendicular segment from the point (x0,y0) to the line1
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'purple')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
                     pos = 1, 
          cex=1.4))
```
]

???

Dans cette premi√®re repr√©sentation, nous pouvons observer qu'une premi√®re direction (ou la premi√®re composante lin√©aire) est trac√©e en essayant de maximiser la variance des donn√©es.

Les participants peuvent vous demander en quoi cela est diff√©rent d'une r√©gression lin√©aire. L'une des principales diff√©rences r√©side dans la fa√ßon dont les carr√©s d'erreurs sont minimis√©s perpendiculairement √† la ligne droite (90 degr√©s, ce qui la rend orthogonale), alors que dans la r√©gression lin√©aire, les carr√©s d'erreurs sont minimis√©s dans la direction des ordonn√©es.

---

### Analyse en composantes principales : √©tape par √©tape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, √† chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[
Nous pouvons alors repr√©senter les distances des observations au deuxi√®me vecteur propre (`PC2`, en orange).

La deuxi√®me composante principale est √©galement dessin√©e en maximisant la variance des donn√©es.

Notez comment les composantes principales sont orthogonales !
]

.pull-right[

```{r echo = FALSE}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

plot(N ~ P, 
     col = as.factor(rownames(Y_std)), 
     main="Distances to PC2", 
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0, 
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[2]*Eigenvectors[1,2], 
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange", 
       lwd = 2)


line2 <- c(0, 
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line2)

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'orange')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
                     pos = 1, 
          cex=1.4)
     )
```

]

--
.pull-left[
*Nous avons repr√©sent√© les vecteurs propres, c'est-√†-dire les composantes principales !* 

*Mais, quelle est l'utilit√© des valeurs propres ?*
]

???

Ici, la deuxi√®me direction (ou la deuxi√®me composante lin√©aire) est trac√©e de mani√®re √† ce que la variance des donn√©es soit maximis√©e par rapport √† cette deuxi√®me composante.

---

#### Analyse en composantes principales : √©tape par √©tape

Nous avons vu que les *valeurs propres* repr√©sentent la magnitude (la variance) des composantes principales.

.pull-left[
En fait, la somme de toutes les *valeurs propres* est √©gale √† la somme des variances, qui sont repr√©sent√©es sur la diagonale de la matrice de variance-covariance.
]

.pull-right[
```{r}
sum(diag(cov(Y_std)))
sum(eigen(cov(Y_std))$values)
```
]

--

Intuitivement, on peut obtenir l'influence relative de chaque *vecteur propre* $v_{k}$ (ou $\text{PC}_{k}$) en divisant leurs valeurs par la somme de toutes les *valeurs propres*.

$$\text{Variance expliqu√©e de}~v_{k} = \frac{\lambda_{v_k}}{\sum^p_{i=1}{\lambda_{v}}}$$

En faisant cela, nous pouvons dire que le $\text{PC}1$ explique `r round(eigen(cov(Y_std))$values[1]/sum(eigen(cov(Y_std))$values) * 100)`% de la variance dans les donn√©es, tandis que $\text{PC}2$ explique `r round(eigen(cov(Y_std))$values[2]/sum(eigen(cov(Y_std))$values) * 100)`% de la variance.

--

Enfin, nous pouvons proc√©der √† la derni√®re √©tape de notre analyse des composantes principales !

---

#### Analyse en composantes principales : √©tape par √©tape

.pull-left3[
1. Point de d√©part : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribu√©es ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient √† centrer, comme dans $y_c = [y_i - \bar{y}]$, puis √† mettre √† l'√©chelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;

4. Effectuer la d√©composition de la matrice de covariance pour obtenir la matrice $U$ des vecteurs propres, contenant les *composantes principales* ;

5. Obtenir la *matrice des coordonn√©es* $F$ en multipliant $U$ par la matrice normalis√©e $Y_{std}$.
]

.pull-right3[
En `R`, √† partir de z√©ro !

```{r}
# √âtape 1 
Y <- varechem[, 1:2]

# √âtape 2
Y_std <- as.matrix(scale(Y))

# √âtape 3
Y_R <- cov(Y_std)

# √âtape 4 
Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

# √âtape 5
F_PrComps <- Y_std %*% Eigenvectors
head(F_PrComps)
```
]

---

### Analyse en composantes principales : √©tape par √©tape

La *matrice des coordonn√©es*, $F$, (objet `F_PrComps`) permet de *rotationner* le nouvel espace de donn√©es, afin qu'il soit repr√©sent√© par rapport aux composantes principales.

.pull-left[
.center[ 
$\text{N}$ ~ $\text{P}$ 
]

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

op <- par(mfrow = c(2, 1),     # 2x2 layout
    oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
    mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
    )       

plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0, 
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0, 
#        y0 = 0, 
#        x1 = Eigenvalues[2]*Eigenvectors[1,2], 
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange", 
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0, 
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'purple')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
                     pos = 1, 
          cex=1.4))


plot(N ~ P, 
     col = as.factor(rownames(Y_std)),
     pch = 19, 
     xlim=c(-2.2, 2.2), 
     ylim = c(-2.2,2.2), 
     data = as.data.frame(Y_std))

abline(v=0 , h=0, 
       col = "dark gray")

#Overlap pertinent evectors

abline(0, 
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0, 
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0, 
       y0 = 0, 
       x1 = Eigenvalues[2]*Eigenvectors[1,2], 
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange", 
       lwd = 2)


line2 <- c(0, 
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0, 
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P, 
                         Y_std$N, 
                         line2)

segments(x0 = ss$x0, 
         x1 = ss$x1, 
         y0 = ss$y0, 
         y1 = ss$y1, 
         col = 'orange')

points(N ~ P, 
       col = as.factor(rownames(Y_std)), 
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P, 
          labels = as.factor(rownames(Y_std)),
                     pos = 1, 
          cex=1.4)
     )

title(xlab = "N",
      ylab = "P",
      outer = TRUE, line = 3)

par(op)
```
]

.pull-right[
.center[ 
$\text{PC}1$ ~ $\text{PC}2$ 
]

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5}
score <- as.data.frame(F_PrComps)

colnames(score) <- c("PC1", "PC2")

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)


plot(PC2 ~ PC1, 
     col = as.factor(rownames(score)), 
     pch = 19, 
     xlim = c(-2.2, 2.2), ylim = c(-2.2,2.2), xlab='PC1', ylab='PC2',data = score)

abline(h = 0, col = 'purple')
abline(v = 0, col='orange')


perp.segment.horiz <- function(x0, y0){
  x1 <- x0
  y1 <- 0
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1 <- perp.segment.horiz(score[,1], score[,2])

segments(x0 = ss1$x0, x1 = ss1$x1, y0 = ss1$y0, y1 = ss1$y1, col='purple')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)
with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))


plot(PC2 ~ PC1, col=as.factor(rownames(score)), 
     pch = 19, xlim=c(-2.2, 2.2), ylim = c(-2.2,2.2),
     xlab='PC1', ylab='PC2',data=score)

abline(h = 0, col = 'purple')
abline(v = 0, col ='orange')


perp.segment.vert <- function(x0, y0){
  x1 <- 0
  y1 <- y0
  
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1a <- perp.segment.vert(score[,1], score[,2])
segments(x0 = ss1a$x0, x1 = ss1a$x1, y0 = ss1a$y0, y1 = ss1a$y1, col='orange')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)

with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))

title(xlab = "PC1",
      ylab = "PC2",
      outer = TRUE, 
      line = 3)

par(op)
```
]

???
Les titres des axes ne se font pas imprimer. Je les ai inclus en haut de chaque graphique, mais ce probl√®me n'est pas r√©solu.

Le pr√©sentateur ou la pr√©sentatrice devrait souligner la rotation et parler de ce que sont les points. Vous pouvez survoler les points et leur montrer quelle √©tait la position des points dans les graphiques " sans rotation ", et maintenant dans les graphiques " avec rotation ", en soulignant que maintenant, les " nouveaux axes " sont PC1 et PC2.

Cette compr√©hension sera utile lorsque les participants utiliseront les fonctions PCA impl√©ment√©es dans R.

---

### Analyse en composantes principales : √©tape par √©tape

L'ACP peut aussi √™tre calcul√©e en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare √†]

```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`stats::prcomp()`?
```{r}
PCA_prcomp <- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp <- prcomp(Y_std)

head(PCA_prcomp$x)
```
]

---

### Analyse en composantes principales : √©tape par √©tape

L'ACP peut aussi √™tre calcul√©e en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare √†]

```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`stats::princomp()` ?
```{r}
PCA_princomp <- princomp(Y_std)

head(PCA_princomp$scores)
```
]

---

### Analyse en composantes principales : √©tape par √©tape

L'ACP peut aussi √™tre calcul√©e en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare √†]

```{r}
data(varechem)

Y <- varechem[, 1:2] 
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`vegan::rda()` ?
```{r}
PCA_vegan_rda <- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
```

`vegan::rda()` utilise des mises √† l'√©chelle alternatives. Vous pouvez √©tudier la `vignette("decision-vegan")`.
]

???

Dites aux participants que le nom `rda` fait r√©f√©rence √† un type diff√©rent de technique d'ordination sous contrainte, mais que si nous ex√©cutons `rda()` avec une seule variable, il ex√©cutera une ACP.

---

# Analyse en composantes principales

Nous avons impl√©ment√© l'ACP sur un ensemble de donn√©es √† deux variables pour plus de simplicit√©.

Avan√ßons et appliquons-la √† notre jeu de donn√©es sur les esp√®ces de poissons.


Pour cela, nous allons utiliser la fonction `vegan::rda()` sur les donn√©es de poissons *transform√©es par Hellinger* et r√©sumer les r√©sultats :

```{r}
spe.h.pca <- rda(spec.hel)

# summary(spe.h.pca)
```

---

# Analyse en composantes principales

.pull-left[
Les premi√®res lignes de `summary.rda()` nous renseignent sur la *variance totale* et la *variance sans contrainte* de notre mod√®le.
]

.pull-right[

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[5:8])
```
]

--

.pull-left2[

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(12:16, 21:24)])
```

]

.pull-right2[

Viennent ensuite les *valeurs propres*, et leur contribution √† la variance.


En fait, si nous additionnons toutes nos *valeurs propres*, nous obtiendrons la quantit√© de variance sans contrainte expliqu√©e par l'analyse !

```{r}
sum(spe.h.pca$CA$eig)
```

]

???

Puisque nous n'avons pas contraint notre ordination, la variance proportionnelle non contrainte est √©gale √† la variance totale.

Prenez un moment pour expliquer la proportion expliqu√©e, et montrez que la proportion cumul√©e sera √©gale √† 1 au 27√®me PC.

---

# Analyse en composantes principales

Les informations suivantes sont li√©es √† la *mise √† l'√©chelle* (`scaling`), aux *scores d'esp√®ces*, et aux *scores de sites*.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(26:29, 31:32, 34:40, 63:64, 66:72)])
```
]

---

# Analyse en composantes principales

.pull-left[
`Species` fait r√©f√©rence √† vos descripteurs (c'est-√†-dire les colonnes de votre ensemble de donn√©es), qui sont ici les esp√®ces de poissons.

`Scores` font r√©f√©rence √† la position de chaque esp√®ce le long des composantes principales.
]

.pull-right[
```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(32, 34:40)])
```
]

--


.pull-left2[
```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(64, 66:72)])
```
]

.pull-right2[
`Sites` repr√©sentent les lignes de votre jeu de donn√©es, qui sont ici les diff√©rents sites le long de la rivi√®re *Doubs*.
]

--

<br>

.pull-left[
Cette information peut √™tre obtenue avec la fonction `score()`:
]

.pull-right[

```{r, eval = FALSE, echo = TRUE}
scores(spe.h.pca,
       display = "species" or "sites")
```

]

---
### Analyse en composantes principales : condensation des donn√©es

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour s√©lectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les donn√©es.

--

#### Crit√®re de Kaiser-Guttman

.pull-left[
Nous pouvons s√©lectionner les composantes principales qui capturent plus de variance que l'explication moyenne de toutes les composantes principales. Nous le faisons en

1. Extraire les *valeurs propres* associ√©es aux composantes principales ;

2. S√©lectionner les *valeurs propres* au-dessus de la *valeur propre* moyenne :

```{r}
ev <- spe.h.pca$CA$eig
# ev[ev > mean(ev)]
```
]

.pull-right[
```{r, echo = -1, fig.width=10, fig.height = 5.5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```
]

---
### Analyse en composantes principales : condensation des donn√©es

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour s√©lectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les donn√©es.

#### Mod√®le *broken-stick*

.pull-left[
Le mod√®le *broken-stick* retient les composantes qui expliquent plus de variance que ce qui serait attendu en divisant al√©atoirement la variance en $p$ parties.

```{r}
head(bstick(spe.h.pca))
```
]

.pull-right[
```{r, echo = TRUE, fig.width=4.5, fig.height = 4.5}
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```
]

---
## Analyse en composantes principales 

Il ne reste plus qu'√† discuter de la *mise √† l'√©chelle* et √† *visualiser* nos r√©sultats.

Pratiquons et calculons une ACP sur les variables environnementales standardis√©es pour le m√™me ensemble de donn√©es.

```{r}
env.pca <- rda(env.z)
# summary(env.pca, scaling  = 2)
```

--

D√©terminons notre ensemble de *valeurs propres* et leurs *vecteurs propres* correspondants :

.pull-left[
```{r}
ev <- env.pca$CA$eig
```

```{r}
ev[ev>mean(ev)]
```
]

--

.pull-right[
```{r, echo = FALSE, fig.width=8, fig.height = 5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```
]

---

# Analyse en composantes principales : `plot()`

L'information calcul√©e par l'ACP peut √™tre repr√©sent√©e par des *biplots*.

Nous pouvons produire un biplot rapide de l'ACP en utilisant la fonction `plot()` en base `R`.

```{r, echo = -1}
par(mar=c(4,4, 0.1,0.1), cex = 1.5)
plot(spe.h.pca)
```


---

# Analyse en composantes principales : `plot()`


`biplot()` de `base` `R` permet une meilleure interpr√©tation.

.pull-left2[
```{r, echo = -1, fig.height=6, fig.width=6.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca)
```
]

.pull-right2[

Les fl√®ches sont trac√©es pour montrer la directionnalit√© et l'angle des descripteurs dans l'ordination.

Les descripteurs situ√©s √† 180 degr√©s les uns des autres sont corr√©l√©s n√©gativement ;

Les descripteurs situ√©s √† 90 degr√©s l'un de l'autre ont une corr√©lation nulle ;

Les descripteurs situ√©s √† 0 degr√© l'un de l'autre sont positivement corr√©l√©s.

]

---
# Analyse en composantes principales : *mise √† l'√©chelle*

.small[
*Mise √† l'√©chelle de type 2* (`scaling = 2`, d√©faut) : les distances entre les objets ne sont pas des approximations des distances euclidiennes ; les angles entre les vecteurs des descripteurs (esp√®ces) refl√®tent leurs corr√©lations.
]

.small[
*Mise √† l'√©chelle de type 1* (`scaling = 1`) : tente de pr√©server la distance euclidienne (dans un espace multidimensionnel) entre les objets (sites) : les angles entre les vecteurs des descripteurs (esp√®ces) ne sont pas pertinents.
]

.pull-left[
```{r, echo = -1, fig.height=3.5, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 1)
```
]

.pull-right[
```{r, echo = -1, fig.height=3.5, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 2)
```
]

???
2 : **Meilleur pour l'interpr√©tation des relations entre les descripteurs (esp√®ces)**.


1 : **Meilleur pour l'interpr√©tation des relations entre les objets (sites)!**

---
# D√©fi # 3 ![:cube]()

En utilisant tout ce que vous avez appris, calculez une ACP sur les donn√©es d'abondance des esp√®ces d'acariens.

```{r}
data(mite)
```

Soyez pr√™t √† discuter et √† r√©pondre :
- Quelles sont les composantes principales *les plus pertinentes*, c'est-√†-dire les sous-ensembles ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'esp√®ces sont li√©s √† ces groupes de sites ?

---

# Solution #3

Calculer l'ACP sur les donn√©es d'esp√®ces transform√©es par Hellinger.

```{r}
mite.spe.hel <- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca <- rda(mite.spe.hel)
```

--

.pull-left[
Appliquer le crit√®re de Kaiser-Guttman

```{r, eval = F}
ev <- mite.spe.h.pca$CA$eig
ev[ev>mean(ev)]
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, 
       col = "red3", bty = "n")
```
]

.pull-right[
```{r, echo = F, fig.width=5, fig.height=4}
par(mar=c(4,4,2,1), cex = 1.2)
ev <- mite.spe.h.pca$CA$eig
n <- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, col = "red3", 
       bty = "n")
```
]

---
# Solution #3

```{r, echo = -1, fig.height=6.5, fig.width=7}
par(mar = c(4,4,0.05,0.05), cex = 1.5)
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

---
# Analyse des coordonn√©es principales

La **ACoP** (*PCoA*, en anglais) est similaire dans son esprit √† l'ACP, mais elle prend des *dissimilarit√©s* comme donn√©es d'entr√©e ! 

Elle vise √† repr√©senter fid√®lement les distances avec l'espace dimensionnel le plus bas possible.

Elle commence par le (i) calcul d'une matrice de distance pour les $p$ √©l√©ments, puis (ii) le centrage de la matrice par lignes et colonnes, et enfin, la (iii) *decomposition* de la matrice de distance centr√©e en √©l√©ments propres.

--

Pour calculer une ACoP, nous pouvons utiliser les fonctions `cmdscale()` ou `pcoa()` des paquets `stats` et `ape` :

```{r}
library(ape)
spe.h.pcoa <- pcoa(dist(spec.hel))
summary(spe.h.pcoa)
```

---

# Analyse des coordonn√©es principales

```{r}
head(spe.h.pcoa$values)
```

---

# Analyse des coordonn√©es principales

Nous pouvons √©galement voir les *vecteurs propres* associ√©s √† chaque *valeur propre* contenant les coordonn√©es dans l'espace euclidien pour chaque site.


```{r}
head(spe.h.pcoa$vectors)[, 1:5]
```

---

# Analyse des coordonn√©es principales : `biplot.pcoa()`

Nous pouvons afficher les distances entre les sites en utilisant la fonction `biplot.pcoa()`, ainsi que repr√©senter les esp√®ces associ√©es √† chaque site.

```{r, fig.height=5.5, fig.width=8}
biplot.pcoa(spe.h.pcoa, spec.hel)
```

---

##### Analyse des coordonn√©es principales : distances non m√©triques

La ACoP peut √©galement √™tre utilis√©e pour capturer les informations contenues dans les distances non m√©triques, telles que la populaire distance de Bray-Curtis. Essayons :


.pull-left[
```{r}
spe.bray.pcoa <- pcoa(spe.db.pa)
```

```{r}
spe.bray.pcoa$values$Eigenvalues
```
]

.pull-right[
Notez les valeurs propres n√©gatives ! 

Cela est d√ª au fait que les distances non m√©triques ne peuvent pas √™tre repr√©sent√©es dans l'espace euclidien sans corrections (*voir* Legendre & Legendre 2012 pour plus de d√©tails √† ce sujet) :

```{r}
spe.bray.pcoa <- pcoa(spe.db.pa, 
                      correction = "cailliez")
```
]

---
##### Analyse des coordonn√©es principales : distances non m√©triques

.pull-left[

Les valeurs propres corrig√©es sont maintenant sur une nouvelle colonne !

```{r}
spe.bray.pcoa$values$Corr_eig
```

]

.pull-right[
Utilisez un biplot sans les esp√®ces pour la repr√©senter !

```{r, fig.width=6, fig.height=5.5, echo = -1}
par(mar=c(3,3,.5,1), cex = 1.2)
biplot.pcoa(spe.bray.pcoa)
```

]

---
# D√©fi #5 ![:cube]()

Calculez une ACoP sur les donn√©es d'abondance des esp√®ces d'acariens transform√©es par Hellinger.

Soyez pr√™t √† r√©pondre :

- Quels sont les *vecteurs propres* et les *valeurs propres* significatifs ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'esp√®ces sont li√©s √† ces groupes de sites ?
- Comment les r√©sultats de la ACoP se comparent-ils √† ceux de l'ACP ?

---
# Solution #5

- Transformation de Hellinger des donn√©es sur les esp√®ces
```{r}
mite.spe <- mite
mite.spe.hel <- decostand(mite.spe, method = "hellinger")
```

- Calcul de la ACoP

```{r}
mite.spe.h.pcoa <- pcoa(dist(mite.spe.hel))
```

---
# Solution #5

- Construisez un biplot pour visualiser les donn√©es :
```{r, fig.width=6, fig.height=6}
biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel)
```


---
# Positionnement multidimensionnel non-m√©trique

- Dans l'ACP et la ACoP, les objets sont ordonn√©s selon un petit nombre de dimensions (g√©n√©ralement > 2) ;

- Les biplots 2D peuvent ne pas repr√©senter toute la variation au sein de l'ensemble de donn√©es ;

- Parfois, nous cherchons √† repr√©senter les donn√©es dans un plus petit nombre de dimensions sp√©cifi√© ;

- Comment pouvons-nous tracer l'espace d'ordination pour repr√©senter la plus grande variation possible dans les donn√©es ?

--

Nous pouvons essayer d'utiliser le *positionnement multidimensionnel non-m√©trique*!

* Le PMnM (en anglais, *nMDS* de *non-metric multidimensional scaling*) est la contrepartie non-m√©trique de ACoP ;
* Il utilise un algorithme d'optimisation it√©ratif pour trouver la meilleure repr√©sentation des distances dans un espace r√©duit ;

---
# Positionnement multidimensionnel non-m√©trique

- Le PMnM (ou nMDS) applique une proc√©dure it√©rative qui tente de positionner les objets dans le nombre de dimensions demand√© de mani√®re √† minimiser une fonction de contrainte (√©chelonn√©e de $0$ √† $1$), qui mesure la qualit√© de l'ajustement de la distance dans la configuration de l'espace r√©duit.

- Par cons√©quent, plus la valeur de stress est faible, meilleure est la repr√©sentation des objets dans l'espace d'ordination.

- nMDS est impl√©ment√© dans `vegan` comme `metaMDS()` o√π :
  - `distance` sp√©cifie la m√©trique de distance √† utiliser ;
  - `k` sp√©cifie le nombre de dimensions.

```{r, eval = FALSE, echo = TRUE}
spe.nmds <- metaMDS(spe, distance = 'bray', k = 2)
```

```{r include=FALSE}
spe.nmds <- metaMDS(spe, distance = 'bray', k = 2)
```

---
# Positionnement multidimensionnel non-m√©trique: *qualit√© de l'ajustement*

Le diagramme de *Shepard* et les valeurs de contraintes peuvent √™tre obtenus avec `stressplot()` :

```{r, eval = FALSE}
spe.nmds$stress # [1] 0.07376229
stressplot(spe.nmds, main = "Shepard plot")
```

.pull-left[
```{r, echo = FALSE, warning=FALSE, message=FALSE, purl = FALSE}
stressplot(spe.nmds, main = "Shepard plot")
```
]
.pull-right[
<Br><Br>
Le graphique de Shepard identifie une forte corr√©lation entre la dissimilarit√© observ√©e et la distance d'ordination $(R^2 > 0.95)$, ce qui met en √©vidence la qualit√© √©lev√©e de l'ajustement du PMnM.

]

---
# Positionnement multidimensionnel non-m√©trique : `biplot()`

Construire un biplot

```{r, eval = F}
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```

---
##### Positionnement multidimensionnel non-m√©trique : `biplot()`

.pull-left[

Le biplot du PMnM montre un groupe de sites ferm√©s caract√©ris√©s par les esp√®ces BLA, TRU, VAI, LOC, CHA et OMB, tandis que les autres esp√®ces forment un groupe de sites dans la partie sup√©rieure droite du graphique. 

Quatre sites dans la partie inf√©rieure du graphique sont fortement diff√©rents des autres.
]

.pull-right[
```{r echo=FALSE}
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```
]

---
# D√©fi #6 ![:cube]()

<br>

Ex√©cutez le PMnM de l'abondance des esp√®ces d'acariens en 2 dimensions bas√© sur une distance de Bray-Curtis.

√âvaluez la qualit√© de l'ajustement de l'ordination et interpr√©tez le biplot.

---
# Solution #6

.pull-left[
![](images/SheplSol6.png)
]

.pull-right[
La corr√©lation entre la dissimilarit√© observ√©e et la distance d'ordination $(R^2 > 0,91)$ et la valeur de contrainte relativement faible, montrant ensemble une bonne pr√©cision de l'ordination NMDS.
]

---
# Solution #6

.pull-left[
![](images/NMDSSol61.png)
]

.pull-right[
Aucun groupe de sites ne peut √™tre d√©fini pr√©cis√©ment √† partir du biplot NMDS montrant que la plupart des esp√®ces sont pr√©sentes dans la plupart des sites, c'est-√†-dire que quelques sites abritent des communaut√©s sp√©cifiques.
]

---
# Conclusion

.alert[De nombreuses techniques d'ordination existent, mais leur sp√©cificit√© doit guider vos choix sur les m√©thodes √† utiliser]

| | Distance pr√©serv√©e | Variables | Nombre maximal d'axes |
|---|---------|--------------|------|
|PCA| Euclidienne | Donn√©es quantitatives, relations lin√©aires | p|
|CA| Chi2 | Non n√©gatif, donn√©es quantitatives homog√®nes, donn√©es binaires | p-1 |
|PCoA| D√©finie par l'utilisateur | Donn√©es quantitatives, semi-quantitatives, mixtes| p-1|
|NMDS| D√©finie par l'utilisateur | Donn√©es quantitatives, semi-quantitatives et mixtes | D√©finie par l'utilisateur |

---
# C'est l'heure du quiz !

.alert[Que signifie ACP?]

--

Analyse en composantes principales

--

.alert[Laquelle de ces m√©thodes est la meilleure pour visualiser les *distances* entre composition des communaut√©s de diff√©rents sites?]

--

Analyse en coordonn√©es principales (ACoP)

--

.alert[Que repr√©sente une valeur propre dans une ACP ?]

--

La proportion de variance captur√©e par une composante principale

---
# C'est l'heure du quiz !

Trouvez l'erreur!

![:scale 90%](images/Chall7.png)

--
.alert[
- Donn√©es non centr√©es, beurk!
]

---
# C'est l'heure du quiz !

Trouvez l'erreur!

![:scale 90%](images/Chall7_2.png)

--

.alert[
- Les 2 premiers axes capturent 100% de la variation
]

---
class: inverse, center, bottom

# Merci pour votre participation √† cet atelier!

![:scale 50%](images/qcbs_logo.png)
