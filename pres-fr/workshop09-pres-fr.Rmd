---
title: "Atelier 9: Analyses multivariées"
subtitle: "Série d'ateliers R"
author: "Centre de la Science de la Biodiversité du Québec"
output:
  xaringan::moon_reader:
    includes:
      in_header: qcbsR-header.html
    lib_dir: assets
    seal: true
    css: ["default", "qcbsR.css", "qcbsR-fonts.css"]
    nature:
      countIncrementalSlides: false
      beforeInit: "qcbsR-macros.js"
      highlightLines: true
---
class: inverse, center, middle

```{r setup, echo = FALSE, message=FALSE, warning=FALSE, include = FALSE}
library(knitr)

## Setup for your presentation
knitr::opts_chunk$set(
  eval = TRUE,
  cache = TRUE,
  comment = "#",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 5, fig.height = 5, fig.retina = 3,
  fig.align = 'center'
)

options(repos = structure(
  c(CRAN = "http://cran.r-project.org")
  )
  )

# Install xaringanExtra

if(nzchar(system.file(package = "xaringanExtra")) == FALSE) {remotes::install_github("gadenbuie/xaringanExtra", upgrade = "always", quiet = TRUE)}

# Include copy-to-clipboard icons

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fas fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r output-lines, echo = FALSE}
# sometimes cache needs to be set to true in the knitr setup chunk for this to take effect
# in xaringan::infinite_moon_reader()
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```


```{r install_pkgs, message=FALSE, warning=FALSE, include=FALSE, results=0}
# Standard procedure to check and install packages and their dependencies, if needed.

list.of.packages <- c("ape", "ade4", "codep", "gclus", "vegan", "GGally", "PlaneGeometry", "remotes", "matlib")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages) > 0) {
  install.packages(new.packages, dependencies = TRUE)
  print(paste0("The following package was installed:", new.packages))
} else if(length(new.packages) == 0) {
  print("All packages were already installed previously")
}

# Load all required libraries at once
lapply(list.of.packages, require, character.only = TRUE, quietly = TRUE)
```

# À propos de cet atelier

[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Présentation&message=09&color=BF616A)](https://r.qcbs.ca/workshop09/pres-fr/workshop09-pres-fr.html)    [![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Script&message=09&color=D08770&logo=r)](https://r.qcbs.ca/workshop09/book-fr/workshop09-script-fr.R) 
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Livre&message=09&color=EBCB8B)](https://r.qcbs.ca/workshop09/book-fr/index.html)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=Site&message=r.qcbs.ca&color=A3BE8C)](https://r.qcbs.ca/fr/workshops/r-workshop-09/)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&label=GitHub&message=09&color=B48EAD&logo=github)](https://github.com/QCBSRworkshops/workshop09) 

---

<p style="font-size:75%">

.center[
**Membres du CSBQ qui ont contribué à cet atelier**

en modifiant et en améliorant son contenu dans le cadre du <br> Prix d'apprentissage et de développement (*Le*arning *a*nd *D*evelopment *A*ward)
]

.pull-left[
.right[

**2022** - **2021** - **2020**

Pedro Henrique P. Braga

Katherine Hébert

Mi Lin

Linley Sherin


**2019** - **2018** - **2017**

Gabriel Muñoz

Marie Hélène-Brice

Pedro Henrique P. Braga

<br>
]
]

.pull-right[
.left[

**2016** - **2015** - **2014**

Bérenger Bourgeois

Xavier Giroux-Bougard

Amanda Winegardner

Emmanuelle Chrétien

Monica Granados

]
]
</p>

<br>
.center[
__Si vous voulez contribuer aussi__, visitez [r.qcbs.ca/fr/contributing](https://r.qcbs.ca/fr/contributing/) <br> et n'hésitez pas à [nous contacter](mailto:csbq.qcbs.r@gmail.com)!
]

---

# Matériel requis

Cet atelier requiert les dernières versions de [RStudio](https://rstudio.com/products/rstudio/download/#download) et de [R](https://cran.rstudio.com/).

.pull-left[
Vous devez également avoir installé ces paquets:
* [ape](https://cran.r-project.org/package=ape)
* [ade4](https://cran.r-project.org/package=ade4)
* [codep](https://cran.r-project.org/package=codep)
* [gclus](https://cran.r-project.org/package=gclus)
* [vegan](https://cran.r-project.org/package=vegan)
* [GGally](https://cran.r-project.org/package=GGally)
* [PlaneGeometry](https://cran.r-project.org/package=PlaneGeometry)
* [remotes](https://cran.r-project.org/package=remotes)

]

.pull-right[


Pour les installer à partir de CRAN, roulez:

```{r eval = FALSE}
install.packages(c("ape",
                   "ade4",
                   "codep",
                   "gclus",
                   "vegan",
                   "GGally",
                   "PlaneGeometry",
                   "remotes"))
```
]


<br>

.pull-left2[
Au cours de cet atelier, il y aura une série de **défis** que vous pouvez reconnaître par ce cube de Rubik.

**Pendant ces défis, n'hésitez pas à collaborer!**
]

.pull-right2[

.center[
![:scale 45%](images/rubicub.png)

]
]

---

# Objectifs d'apprentissage

1. Apprendre les bases de l'analyse multivariée pour révéler des modèles dans les données sur la composition des communautés.

2. Utiliser `R` pour effectuer une ordination sans contrainte.

3. Apprendre les coefficients de similarité et de dissimilarité et les transformations des données.

4. Utiliser `R` pour créer des dendrogrammes.

5. Apprendre les méthodes suivantes :

* Analyse de regroupement
* Analyse en composantes principales (ACP)
* Analyse en coordonnées principales (ACoP)
* Échelle multidimensionnelle non métrique (PMnM).

---

class: inverse, center, middle

# 1. Préambule

---

# Récapitulation : Analyses univariées

Nous avons appris une multitude d'analyses qui nous ont permis d'interpréter des données écologiques en décrivant les effets d'_une ou plusieurs_ variables sur _une_ variable de réponse.


--

.pull-left[
Nous pouvons rappeler les :

1. Modèles linéaires généraux
  1. `lm()` ;
  2. `anova()` ;
  3. `t.test()` ;
  4. `lmer()`.

2. Modèles linéaires généralisés
  1. `glm()` et `glmer()` avec plusieurs fonctions de liaison `family()`.

3. Modèles additifs généralisés
  1. `gam()`.
]

--

.pull-right[
Ces modèles nous ont permis de poser des questions telles que :

1. Quels sont les effets des précipitations et de la température sur la richesse des espèces ?
2. Comment l'abondance des microbes change-t-elle entre les hôtes ?
3. Les poissons cooccurrents deviennent-ils plus agressifs après avoir été induits à la peur ?
]

---

# Statistiques multivariées

Cependant, on peut être intéressé à faire des inférences à partir de données écologiques contenant _plus d'une_ variable dépendante.

Cet intérêt peut être motivé par la validation d'hypothèses et la modélisation, mais peut aussi être entièrement exploratoire.

--

Par exemple, notre question de recherche pourrait être la suivante

1. Comment la _composition bactérienne_ des feuilles d'érable change-t-elle le long du gradient d'altitude ?

2. Quelle est la _dissimilarité compositionnelle_ des communautés de chauves-souris ?

3. Quel est le rapport entre les communautés locales d'araignées et leur _composition_ ?

--

Dans tous ces cas, le résultat est composé de plusieurs variables, _e.g._ généralement une matrice échantillon par espèce ou échantillon par environnement.

---
# Statistiques multivariées

Nous allons maintenant plonger dans les **statistiques multivariées**, un ensemble d'outils qui nous permettront de répondre à des questions nécessitant l'observation ou l'analyse simultanée de plus d'une variable de résultat.

---

# Contexte des méthodes multivariées

1. Mesures et matrices d'association (ou de dissimilarité)

2. Analyse de classification (ou _cluster_)

3. Ordination sans contrainte

4. Ordination contrainte (ou canonique)

--

<br>

.center[
Mais, avant cela, il faut rappeler les bases de **l'algèbre matricielle**.
]

---

# Algèbre matricielle : un résumé très bref

L'algèbre matricielle est bien adaptée à l'écologie, car la plupart des _ensembles de données_ avec lesquels nous travaillons sont dans un format _matriciel_.


.pull-left[
Les tableaux de données écologiques sont obtenus sous forme d'observations d'objets ou d'unités d'échantillonnage, et sont souvent enregistrés comme ceci :
<br>

| Objets | $y_1$     | $y_2$  | $\dots$           | $y_n$  |
| :-------------: |:-------------:| :-----:|:-----:|:-----:|
|  $x_1$        | $y_{1,1}$     | $y_{1,2}$  | $\dots$  | $y_{1,n}$  |
|  $x_2$        | $y_{2,1}$     | $y_{2,2}$  | $\dots$  | $y_{2,n}$  |
|  $\vdots$     | $\vdots$     | $\vdots$  | $\ddots$  | $\vdots$  |
|  $x_m$        | $y_{m,1}$     | $y_{m,2}$  | $\dots$  | $y_{m,n}$  |
<br>
où $x_m$ est l'unité d'échantillonnage $m$ ; et $y_n$ est le descripteur écologique qui peut être, _e.g._, l'espèce présente ou une variable chimique.
]

--

.pull-right[

Le même tableau de données écologiques peut être représenté en _notation matricielle_ de la manière suivante :
$$Y = [y_{m,n}] =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n}
\end{bmatrix}$$

où les lettres minuscules indiquent les _éléments_, et les lettres en indice indiquent la _position de ces éléments_ dans la matrice (et dans le tableau !).
]

---

# Algèbre matricielle : un résumé très bref

L'algèbre matricielle est bien adaptée à l'écologie, car la plupart des _ensembles de données_ avec lesquels nous travaillons sont dans un format _matriciel_.

.pull-left[
De plus, tout sous-ensemble d'une matrice peut être identifié !

<br>

.center[_une matrice de lignes_]
$$\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}$$

.center[_une matrice à colonnes_]
$$\begin{bmatrix}
y_{1,1} \\ y_{2,2} \\ \vdots \\ y_{m,2}
\end{bmatrix}$$

]

.pull-right[
Le même tableau de données écologiques peut être représenté en _notation matricielle_ de la manière suivante :
$$Y = [y_{m,n}] =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n}
\end{bmatrix}$$

où les lettres minuscules indiquent les _éléments_, et les lettres en indice indiquent la _position de ces éléments_ dans la matrice (et dans le tableau !).
]


---
# Algèbre matricielle : un résumé très bref

###### Matrices d'association

Deux matrices importantes peuvent être dérivées de la matrice des données écologiques : la _**matrice d'association entre objets**_ et la _**matrice d'association entre descripteurs**_.

--

.pull-left[
En utilisant les données de notre matrice $Y$,


<div class="math">
\[ Y =
\begin{array}{cc}
\begin{array}{ccc}
x_1 \rightarrow\\
x_2 \rightarrow\\
\vdots \\
x_m \rightarrow\\
\end{array}
&
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n}
\end{bmatrix}
\end{array}
\]
</div>

on peut examiner la relation entre les deux premiers objets :

<div class="math">
\[x_1 \rightarrow \begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
\end{bmatrix}
\]
</div>

<div class="math">
\[x_2 \rightarrow
\begin{bmatrix}
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\end{bmatrix}
\]
</div>

<p>et obtenir \(a_{1,2}\). </p>

]

--

.pull-right[

Nous pouvons remplir la matrice d'association $A_{n,n}$ avec les relations entre tous les objets de $Y$ :

<div class="math">
\[A_{n,n} =
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}
\end{bmatrix}\]
</div>

<p>Parce que \(A_{n,n}\) a le même nombre de lignes et de colonnes, elle est notée <i>matrice carrée</i>.</p>

<p>Par conséquent, elle possède \(n^2\) éléments.</p>

]


---
# Algèbre matricielle : un résumé très bref

###### Matrices d'association

Deux matrices importantes peuvent être dérivées de la matrice des données écologiques : la _**matrice d'association entre objets**_ et la _**matrice d'association entre descripteurs**_.


Ces matrices sont la base des analyses **_mode Q_** et **_mode R_** en écologie.

.pull-left[
$$Y_{m,n} =
\begin{bmatrix}
y_{1,1} & y_{1,2} & \cdots & y_{1,n} \\
y_{2,1} & y_{2,2} & \cdots & y_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
y_{m,1} & y_{m,2} & \cdots & y_{m,n}
\end{bmatrix}$$
]

.pull-right[

$$A_{m,m} =
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m}
\end{bmatrix}$$

]

.pull-left[

$$A_{n,n} =
\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n,1} & a_{m,2} & \cdots & a_{n,n}
\end{bmatrix}$$

]

.pull-right[

.pull-left[

<br>

$\leftarrow$ Analyse **_mode R_** pour les descripteurs ou les espèces

]

.pull-right[
$\uparrow$ Analyse **_mode Q_** pour les objets ou les sites
]
]

---
## Nos projets pour les prochaines étapes...

Nous allons nous plonger dans les analyses en **R-mode** et **Q-mode**, et nous allons explorer :

1. Les coefficients d'association : dissimilarité et similarité
2. La transformation des données de composition
3. Analyses de regroupement
4. Analyses d'ordination dans l'espace réduit

<br>

.center[**Mais, tout d'abord, présentons un ensemble de données réelles.**]

.center[_Ce sera votre tour de vous salir les mains!_]

---
# Communautés de poissons de la rivière Doubs

.pull-left3[
Verneaux (1973) a proposé d'utiliser les espèces de poissons pour caractériser les zones écologiques le long des cours d'eau européens.

Il a recueilli des données dans **30 sites** le long de la rivière Doubs près de la frontière franco-suisse, dans les montagnes du Jura. Il a montré que les communautés de poissons étaient des indicateurs biologiques de ces masses d'eau.
]

.pull.right3[
.center[![:scale 25%](images/DoubsRiver.png)]
]

--

.pull-left3[
Leurs données sont réparties en trois matrices :

1. L'abondance de 27 espèces de poissons dans les communautés ;
2. Les variables environnementales enregistrées sur chaque site ; et,
3. Les coordonnées géographiques de chaque site.
]

.pull.right3[
.center[![:scale 30%](https://upload.wikimedia.org/wikipedia/commons/c/cc/Doubs_Laissey.jpg)]

.xsmall[Verneaux, J. (1973) _Cours d'eau de Franche-Comté (Massif du Jura). Recherches écologiques sur le réseau hydrographique du Doubs_. Essai de biotypologie. Thèse d'état, Besançon. 1–257.]
]

---
# Communautés de poissons de la rivière Doubs

.pull-left3[

Vous pouvez télécharger les jeux de données sur [r.qcbs.ca/fr/workshops/r-workshop-09](http://r.qcbs.ca/fr/workshops/r-workshop-09/).

```{r}
spe <- read.csv("data/doubsspe.csv",
                row.names = 1)
env <- read.csv("data/doubsenv.csv",
                row.names = 1)
```

]

--

.pull-right3[

Leurs données peuvent également être récupérées dans le paquet `ade4` :
```{r eval=FALSE, echo = TRUE}
library (ade4)
data (doubs)
spe <- doubs$fish
env <- doubs$env
```

Alternativement, à partir du paquet `codep` :
```{r eval=FALSE, echo = TRUE}
library (codep)
data (Doubs)
spe <- Doubs.fish
env <- Doubs.env
```
]


---
# Communautés de poissons de la rivière Doubs

Jetons un coup d'œil aux données `spe` :

.pull-left[
```{r, echo = T}
head(spe)[, 1:8]
```

```{r, echo = T, output.lines=1:8}
str(spe)
```
]

.pull-right[

```{r, eval = F}
# Essayez-en quelques-uns !
names(spe)   # noms d'objets
dim(spe)     # dimensions
str(spe)     # structure des objets
summary(spe) # sommaire
head(spe)    # debut
```

```{r}
# Nous devrons enlever des sites avec 
# aucune spèce

# Obtenir les lignes avec qui ont des 
# valeurs differents de zero
row_sub <- apply(spe, 
                 1, function(row) any(row !=0 ))

# Garder ces lignes dans `spe`
spe <- spe[row_sub, ]
```

]

---
# Données environnementales de la rivière Doubs

Nous pouvons alors explorer les objets contenant nos données nouvellement chargées. Jetons un coup d'oeil aux données `env` :

.pull-left[
```{r, echo = T}
# Garder les lignes avec != 0 dans `env`
env <- env[row_sub,]

str(env)
```

]

.pull-right[

|Variable |Description|
|:--:|:--|
|das|Distance de la source [km]  |
|alt|Altitude [m a.s.l.]  |
|pen|Pente [par millier m.]  |
|deb|Decharge min. moy. [m<sup>3</sup>s<sup>-1</sup>]  |
|pH|pH de l'eau  |
|dur|Conc. Ca (hardness) [mgL<sup>-1</sup>]  |
|pho|Conc. K [mgL<sup>-1</sup>]  |
|nit|Conc. N [mgL<sup>-1</sup>]  |
|amn|Conc. NH₄⁺ [mgL<sup>-1</sup>]  |
|oxy|Diss. d'oxygen [mgL<sup>-1</sup>]  |
|dbo|Demande biol. d'oxygen [mgL<sup>-1</sup>]  |

]
```{r, eval = F}
names(env)
dim(env) # dimensions
summary(env) # sommaire
head(env)
```


]

---
# Mesures de (dis)similarité

La ressemblance des communautés est frequement évaluée sur la composition des espèces, sous la forme d'un tableau de données site par espèce $Y_{m,n}$.

On peut obtenir une matrice d'association $A_{m,m}$ sous la forme de distances ou de dissimilarités par paires $D_{m,m}$ (ou de similarités $S_{m,m}$), puis les analyser.

--

.pull-left[
Dans `R`, nous pouvons calculer des matrices de distance en utilisant `stats::dist()` :

```{r, eval = T}
dist(spe)
```

]

--

.pull-right[

Par mesure de simplification, nous avons fait cela sans spécifier d'arguments.

Essayez `dist(spe)` de votre côté, notez ce que vous observez et voyez ce que les commandes ci-dessous vous montrent :

```{r, eval = F}
class(dist(spe))
```

```{r, eval = F}
str(dist(spe))
```

```{r, eval = F}
as.matrix(dist(spe))
```

```{r, eval = F}
dim(as.matrix(dist(spe)))
```

]

???
Si possible, faites cette partie avec les participants.

Montrez-leur comment est structuré un objet de classe dist. Soulignez qu'il s'agit d'une matrice triangulaire inférieure, où les distances entre les mêmes objets sont cachées.

---
# Standardisation

La standardisation des variables environnementales est cruciale car vous ne pouvez pas comparer les effets de variables ayant des unités différentes :

```{r, eval = -1}
?decostand
env.z <- decostand(env, method = "standardize")
```

<Br>
Cela centre et réduit les variables pour rendre votre analyse en aval plus appropriée:

```{r}
apply(env.z, 2, mean)
apply(env.z, 2, sd)
```

---

# Mesures de (dis)similarité

Il existe trois groupes de coefficients de distance : _métriques_, _..._ , _..._ .

Le premier groupe est constitué de *métriques*, et ses coefficients satisfont aux propriétés suivantes :

.pull-left[
1. minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ;
2. positivité : si $a \neq b$, alors $D(a,b) > 0$ ;
3. la symétrie : $D(a,b) = D(b,a)$ ;
4. inégalité triangulaire : $D(a,b) + D(b,c) \geq D(a,c)$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien est égale ou supérieure au troisième côté.
]

.pull-right[

Nous pouvons repérer toutes ces propriétés ci-dessous :

```{r, eval = T}
as.matrix(dist(spe))[1:3, 1:3]
```

]

???

Soulignez la nature triangulaire de la classe `dist`, le fait qu'elle peut être convertie en matrice, et les dimensions (en notant qu'elle a le même nombre d'espèces).

---
# Mesures de (dis)similarité

Il existe trois groupes de coefficients de distance : _métriques_, _semimétriques_, _..._ .

Le deuxième groupe est constitué de *semimétriques*, et ils violent la propriété d'_inégalité des triangles_ :

.pull-left[
1. minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ;
2. positivité : si $a \neq b$, alors $D(a,b) > 0$ ;
3. la symétrie : $D(a,b) = D(b,a)$ ;
4. ~~inégalité triangulaire~~ : ${D(a,b) + D(b,c) \geq ou < D(a,c)}$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien n'est _pas_ égale ou supérieure au troisième côté.
]

.pull-right[


]

???

Soulignez la nature triangulaire de la classe `dist`, le fait qu'elle peut être convertie en matrice, et les dimensions (en notant qu'elle a le même nombre d'espèces).

---
# Mesures de (dis)similarité

Il existe trois groupes de coefficients de distance : _métriques_, _semimétriques_, _nonmétriques_.

Le troisième groupe est constitué de coefficients *non métriques*, et ils violent la propriété de _positivité_ des distances métriques :

.pull-left[
1. minimum 0 : si l'espèce $a$ est égale à l'espèce $b$, alors $D(a,b)=0$ ;
2. ~~positivité:~~ si $a \neq b$, alors $D(a,b) > ou < 0$ ;
3. la symétrie : $D(a,b) = D(b,a)$ ;
4. ~~inégalité triangulaire~~ : ${D(a,b) + D(b,c) \geq ou < D(a,c)}$. La somme de deux côtés d'un triangle tracé dans l'espace euclidien n'est _pas_ égale ou supérieure au troisième côté.
]

.pull-right[


]

---

# Mesures de (dis)similarité : Distances euclidiennes

La mesure de distance métrique la plus courante est la _distance euclidienne_.

.pull-left4[
Elle est calculée à l'aide de la formule de Pythagore :
$$D_{1} (x_1,x_2) = \sqrt{\sum_{j=1}^p(y_{1j} - y_{2j})^2}$$

```{r echo=FALSE}
#setting up the plot
xlim <- c(0,4)
ylim <- c(-1,5)
par(mar=c(1,1,1,1)+.1)
plot(xlim,
     ylim, type="n",
     xlab="X1",
     ylab="X2",
     asp=1)
grid()
# define some vectors
a=c(4, 0)
b=c(0, 4)
# plot the vectors
vectors(a, labels="D(y21, y11)", pos.lab=3, frac.lab=.5, col="grey")
vectors(a + b, labels="D1(x1,x2)", pos.lab=4, frac.lab=.5, col="red")
# vector a+b starting from a is equal to b.
vectors(a + b, labels="D(y12, y22)", pos.lab=4, frac.lab=.5, origin=a, col="grey")
points(x = 4, y = 0, type = "p")
text(x=4.1, y=-0.2, labels="")
points(x = 0, y = 0, type = "p")
text(x=-0.1, y=-0.2, labels="x1")
points(x = 4, y = 4, type = "p")
text(x=4.1, y=4.2, labels="x2")
```


]

.pull-right4[
En utilisant `stats::dist()`, nous pouvons la calculer avec :

```{r, eval = T}
spe.D.Euclid <- dist(x = spe,
                     method = "euclidean")
```

Et, nous pouvons tester si une distance est euclidienne en utilisant :

```{r, eval = T}
is.euclid(spe.D.Euclid)
```

]

???

La figure sur le côté doit être terminée. Elle est censée représenter la distance euclidienne entre les sites x1 et x2 dans l'espace cartésien 2D. Les axes sont les descripteurs y1 et y2, respectivement. La flèche en bas représente la distance entre la position de y21 et celle de y11.

Vous pouvez l'utiliser pour vous rappeler le théorème de Pythagore que tout le monde apprend au lycée, où la longueur de l'hypoténuse et a et b désignent les deux longueurs des branches d'un triangle rectangle.

`method = "euclidean"` est le paramètre par défaut.


---
Comparaison des sites Doubs

La fonction `vegdist()` contient toutes les distances couramment utilisées.

```{r, eval = F}
?vegdist
```

--
Quelle est la différence de composition des communautés entre les 30 sites de la rivière Doubs?

```{r}
spe.db.pa <- vegdist(spe, method = "bray")
spe.db <- as.matrix(spe.db.pa)
```

--
- La diagonale est égale à zéro, car les sites sont comparés les uns aux autres.
- Les sites 3 et 7 sont les plus similaires (*plus petite distance*).
- Les sites 1 et 9 sont les plus différents (1 = ils sont *complètement* différents).

---

# Défi #1  ![:cube]()

**Votre tour!** En utilisant la fonction `dist()`, calculez la matrice de distance euclidienne $D_{hmm}$ pour les abondances des espèces par matrice de site $Y_{hmm}$ ci-dessous :

.pull-left[
```{r, eval = T}
Y.hmm <- data.frame(
  y1 = c(0, 0, 1),
  y2 = c(4, 1, 0),
  y3 = c(8, 1, 0))
```
]

.pull-right[

| Sites | $y_1$ | $y_2$ | $y_3$ |
|:----: | :----:| :---: | :---: |
| $s_1$ | 0 | 4 | 8 |
| $s_2$ | 0 | 1 | 1 |
| $s_3$ | 1 | 0 | 0 |

]

Après cela, examinez les chiffres, réfléchissez-y de manière critique et soyez prêts à discuter ! (_5 à 10 minutes_)

--

.pull-left[

**Solution:**

```{r, eval = F}
Y.hmm.DistEu <- dist(x = Y.hmm,
                     method = "euclidean")
as.matrix(Y.hmm.DistEu)
```

]

.pull-right[

**Résultat**:

```{r echo=FALSE}
Y.hmm.DistEu <- dist(x = Y.hmm,
                     method = "euclidean")
as.matrix(Y.hmm.DistEu)
```

]

<br>

.center[
Suggestion : Examinez la composition et les distances entre les sites $s_2$ et $s_3$ et entre $s_1$ et $s_2$.
]

???

---
## Défi #1 ![:cube]()

.pull-left[

**Solution:**

```{r, eval = T}
Y.hmm
```

]

.pull-right[

**Résultat**:

```{r echo=TRUE}
as.matrix(Y.hmm.DistEu)
```

]

La distance euclidienne entre les sites $s_2$ et $s_3$, qui n'ont aucune espèce en commun, est plus petite que la distance entre $s_1$ et $s_2$, qui partagent les espèces $y_2$ et $y_3$ ( !).

D'un point de vue écologique, _cette évaluation de la relation entre les sites est problématique.

--

Ce problème est comme le **paradoxe du double zéro**, _i.e._ les doubles zéros sont traités de la même manière que les doubles présences, de sorte que les doubles zéros réduisent la distance entre deux sites.

Les distances euclidiennes ( $D_1$ ) ne devraient donc _pas_ être utilisées pour comparer des sites sur la base de l'abondance des espèces.

---

# Mesures de (dis)similarité : la distance des cordes

Orlóci (1967) a proposé la _distance de corde_ pour analyser la composition des communautés.

.pull-left[
Elle se constitue de :

1\. Normaliser les données, _c.-à-d. _ mettre à l'échelle les vecteurs de site à la longueur 1 en divisant les abondances des espèces dans un échantillon donné par la somme à racine carrée des abondances carrées dans tous les échantillons, comme suit

$$y'_{Uj}=y_{Uj}/\sum^s_{j=1}{y^2_{Uj}}$$
]

.pull-right[
2\. Calculer les distances euclidiennes sur ces données normalisées :

$$D_{3} (x_1,x_2) = \sqrt{\sum_{j=1}^p(y'_{1j} - y'_{2j})^2}$$
]

---

# Mesures de (dis)similarité : la distance des cordes

Nous pouvons utiliser `vegan::vegdist()` pour cela :

```{r, eval = T}
spe.D.Ch <- vegdist(spe,
                    method = "chord")
as.matrix(spe.D.Ch)[1:3, 1:3]
```

Lorsque deux sites partagent les mêmes espèces dans les mêmes proportions du nombre d'individus, la valeur de $D_3$ est de $0$, et lorsqu'aucune espèce n'est partagée, sa valeur est de $\sqrt{2}$.


---
# Mesures de (dis)similarité : la distance des cordes

Il existe de nombreuses autres distances métriques d'intérêt, que nous pouvons utiliser :

Que se passe-t-il si nous calculons les distances d'accord dans la même matrice site-espèce $Y_{hmm}$ ?
.pull-left[

```{r, eval = T}
Y.hmm
```

```{r echo=TRUE}
as.matrix(Y.hmm.DistEu)
```

]

.pull-right[

```{r, eval = T}
Y.hmm.DistCh <- vegdist(Y.hmm,
                    method = "chord")
```

```{r echo=TRUE}
as.matrix(Y.hmm.DistCh)
```
]

--

L'ajout d'un nombre quelconque de doubles zéros à une paire de sites ne modifie pas la valeur de $D_3$.

Par conséquent, les _distances de cordes_ peuvent être utilisées pour comparer des sites décrits par des abondances d'espèces !


---
# Mesures de (dis)similarité : Jaccard

Un autre coefficient d'association populaire est le _coefficient de similarité de Jaccard_ (1900).

Il n'est approprié que pour les **données binaires**, et son coefficient de distance est défini par la taille de l'intersection divisée par la taille de l'union des ensembles d'échantillons

$$D_{7}(x_1,x_2)=1-{ {|x_1 \cap x_2|}\over{|x_1 \cup x_2|} } = 1-{ {|x_1 \cap x_2|}\over{|x_1| + |x_2| - |x_1 \cap x_2|} } = 1-\frac{a}{a+b+c}$$
où,

- $a$ est le nombre d'espèces partagées entre $x_1$ et $x_2$ qui sont codées $1$ ;
- $b$ est le nombre d'occurrences où l'on sait que $x_1$ et $x_2$ sont différents ;
- $c$ est le nombre d'absences communes entre $x_1$ et $x_2$, _c.-à-d._ toutes deux $0$.

--

.pull-left[
Par exemple, pour les sites $x_1$ et $x_2$ :

| $x_1,x_2$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|:----: | :----:| :---: | :---: | :---: | :---: |
| $x_1$ | 0 | 1 | 0 | 1 | 0 |
| $x_2$ | 0 | 1 | 1 | 1 | 1 |
]

.pull-right[
.pull-left[
Alors:
- $a$ = 1 + 1 = 2
- $b$ = 1 + 1 = 2
- $c$ = 1

]
.pull-right[
<br>
$D_{7}(x_1,x_2) =$

$1-\frac{2}{2+2+1}=$

$0.6$
]
]

---
# Mesures de (dis)similarité : Sørensen

Tous les paramètres du _coefficient de similarité de Jaccard_ ont le même poids.

$$D_{7}(x_1,x_2)=1-\frac{a}{a+b+c}$$

Cependant, vous pouvez envisager que la présence d'une espèce soit plus informative que son absence.

La distance correspondant au _coefficient de similarité de Sørensen_ (1948) donne du poids aux doubles présences :

$$D_{13}(x_1,x_2)=1-\frac{2a}{2a+b+c}=\frac{b+c}{2a+b+c}$$

où,
- $a$ est le nombre d'espèces partagées entre $x_1$ et $x_2$ qui sont codées $1$ ;
- $b$ est le nombre d'occurrences où l'on sait que $x_1$ et $x_2$ sont différents ;
- $c$ est le nombre d'absences communes entre $x_1$ et $x_2$, _c.-à-d. _ toutes deux $0$.

---
# Mesures de (dis)similarité : Bray-Curtis

Le _coefficient de dissimilarité de Bray-Curtis_ est une version modifiée de l'indice de Sørensen et tient compte de l'abondance des espèces :

.pull-left[
$$D_{14}(x_1,x_2)=\frac{\sum{\vert y_{1j}-y_{2j}\vert} }{\sum{( y_{1j}+y_{2j})} }=$$

$$D_{14}(x_1,x_2)=1 - \frac{2W}{A+B}$$
où,
- $W$ est la somme des plus faibles abondances de chaque espèce trouvées entre les sites $x_1$ et $x_2$ ;
- $A$ est la somme de toutes les abondances dans $x_1$ ; et,
- $B$ est la somme de toutes les abondances dans $x_2$.
]

--

.pull-right[
Par exemple, pour les sites $x_1$ et $x_2$ :

| $x_1,x_2$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|:----: | :----:| :---: | :---: | :---: | :---: |
| $x_1$ | _2_ | _1_ | _0_ | 5 | 2 |
| $x_2$ | 5 | _1_ | 3 | _1_ | 1 |

<br>

Alors:
- $W = 2 + 1 + 0 + 1 + 1 = 5$
- $A = 2 + 1 + 0 + 5 + 2 = 10$
- $B = 5 + 1 + 3 + 1 + 1 = 10$

$$D_{14}(x_1,x_2) = 1-\frac{2 \times 5}{10+10}=$$
$$D_{14}(x_1,x_2) = 0.5$$
]

---
##### Mesures de (dis)similarité : Coefficients de Jaccard et de Sørensen

Dans `R`, vous pouvez utiliser la fonction `vegan::vegdist()` pour calculer les indices de Jaccard et de Sørensen :

.pull-left[

```{r}
spe.D.Jac <- vegdist(spe,
                     method = "jaccard",
                     binary = TRUE)
```

]

.pull-right[

```{r}
spe.D.Sor <- vegdist(spe,
                     method = "bray",
                     binary = TRUE)
```
]
<br>
> Comme la méthode de Jaccard et celle de Sørensen ne sont appropriées que pour les données de présence-absence, vous devez effectuer une transformation binaire des données d'abondance en utilisant `binary = TRUE` dans `vegdist()`.

##### Coefficient de dissimilarité de Bray-Curtis

Pour calculer le _coefficient de dissimilarité de Bray-Curtis_, qui peut tenir compte des abondances, vous devez définir `binary = FALSE`.


```{r}
spe.D.Bray <- vegdist(spe,
                      method = "bray",
                      binary = FALSE)
```

---
### Mesures de (dis)similarité : Distance de Mahalanobis




---
# Mesures de (dis)similarité : représentation

Nous pouvons créer des représentations graphiques des matrices d'association en utilisant la fonction `coldiss()` :

```{r, echo = FALSE}
# coldiss() function
# Color plots of a dissimilarity matrix, without and with ordering
#
# License: GPL-2
# Author: Francois Gillet, 23 August 2012
#
"coldiss" <- function(D, nc = 4, byrank = TRUE, diag = FALSE)
{
  require(gclus)

  if (max(D)>1) D <- D/max(D)

  if (byrank) {
    spe.color <- dmat.color(1-D, cm.colors(nc))
  }
  else {
    spe.color <- dmat.color(1-D, byrank=FALSE, cm.colors(nc))
  }

  spe.o <- order.single(1-D)
  speo.color <- spe.color[spe.o, spe.o]

  op <- par(mfrow=c(1,2), pty="s")

  if (diag) {
    plotcolors(spe.color, rlabels=attributes(D)$Labels,
               main="Dissimilarity Matrix",
               dlabels=attributes(D)$Labels)
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o],
               main="Ordered Dissimilarity Matrix",
               dlabels=attributes(D)$Labels[spe.o])
  }
  else {
    plotcolors(spe.color, rlabels=attributes(D)$Labels,
               main="Dissimilarity Matrix")
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o],
               main="Ordered Dissimilarity Matrix")
  }

  par(op)
}
# Usage:
# coldiss(D = dissimilarity.matrix, nc = 4, byrank = TRUE, diag = FALSE)
# If D is not a dissimilarity matrix (max(D) > 1), then D is divided by max(D)
# nc 							number of colours (classes)
# byrank= TRUE		equal-sized classes
# byrank= FALSE		equal-length intervals
# diag = TRUE			print object labels also on the diagonal
# Example:
# coldiss(spe.dj, nc=9, byrank=F, diag=T)
```


```{r, fig.height = 8, fig.width = 16}
coldiss(spe.D.Jac)
```

---
# Transformations pour les données de composition des communautés

Les communautés échantillonnées dans des conditions environnementales homogènes ou courtes peuvent avoir des compositions d'espèces avec peu de zéros, de sorte que les distances euclidiennes pourraient suffire à les caractériser.

Néanmoins, c'est rarement la réalité.

Les espèces peuvent être très fréquentes lorsque les conditions sont favorables, ou être absentes de nombreux sites. Parfois, cette asymétrie peut introduire des problèmes parasites dans nos analyses.

Nous pouvons alors être amenés à transformer nos données de composition pour les analyser de manière appropriée.

--

Dans `R`, nous pouvons compter sur `vegan::decostand()` pour de nombreux types de transformations.

Jetez un coup d'oeil à l'aide de cette fonction pour voir les options disponibles :

```
?decostand()
```

Voyons-en quelques-uns.

---
## Transformations : présence-absence

Nous pouvons changer l'argument `method` en `"pa"` pour transformer nos données d'abondance en données de présence-absence :

.pull-left[
Rappelons notre ensemble de données `spe` :

```{r}
spe[1:6, 1:6]
```
]

Transformons les abondances `spe` en présence-absence :

```{r}
spe.pa <- decostand(spe, method = "pa")
spe.pa[1:6, 1:6]
```

---
## Transformations : profils d'espèces

Parfois, on souhaite éliminer les effets des unités très abondantes. Nous pouvons transformer les données en profils d'abondance relative des espèces grâce à l'équation suivante :

$$y'_{ij} = \frac{y_{ij}}{y_{i+}}$$

.pull-left[
Rappelons notre jeu de données `spe` :

```{r}
spe[1:5, 1:6]
```
]

Avec `decostand()`:

```{r}
spe.total <- decostand(spe,
                       method = "total")
spe.total[1:5, 1:6]
```


???

yi+ est utilisé pour indiquer le nombre total de l'échantillon pour toutes les espèces j=1,...,m, pour le ième échantillon.

---
## Transformations : Hellinger

Nous pouvons prendre la racine carrée de la _transformation du profil de l'espèce_ et obtenir la _transformation de Hellinger_, qui a de très bonnes propriétés mathématiques et permet de réduire les effets des valeurs de $y_{ij}$ qui sont extrêmement grandes.

$$y'_{ij} = \sqrt{\frac{y_{ij}}{y_{i+}}}$$

.pull-left[
Rappelons notre jeu de données `spe` :

```{r}
spe[1:5, 1:6]
```
]

Avec `decostand()`:

```{r}
spe.total <- decostand(spe,
                       method = "hellinger")
spe.total[1:5, 1:6]
```

---
exclude: true
# Richesse totale en espèces

Visualisez le nombre d'espèces présentes sur chaque site :

```{r, fig.width=10, fig.height=5, echo=-1}
par(mar = c(4,4,1,.5), cex = 1.5)
site.pre <- rowSums(spe > 0)
barplot(site.pre, main = "Species richness",
        xlab = "Sites",
        ylab = "Number of species",
        col = "grey ", las = 1)
```

---
exclude: true

# Visualisation des matrices de distance

```{r, echo = FALSE}
# coldiss() function
# Color plots of a dissimilarity matrix, without and with ordering
#
# License: GPL-2
# Author: Francois Gillet, 23 August 2012
#
"coldiss" <- function(D, nc = 4, byrank = TRUE, diag = FALSE)
{
  require(gclus)

  if (max(D)>1) D <- D/max(D)

  if (byrank) {
    spe.color <- dmat.color(1-D, cm.colors(nc))
  }
  else {
    spe.color <- dmat.color(1-D, byrank=FALSE, cm.colors(nc))
  }

  spe.o <- order.single(1-D)
  speo.color <- spe.color[spe.o, spe.o]

  op <- par(mfrow=c(1,2), pty="s")

  if (diag) {
    plotcolors(spe.color, rlabels=attributes(D)$Labels,
               main="Dissimilarity Matrix",
               dlabels=attributes(D)$Labels)
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o],
               main="Ordered Dissimilarity Matrix",
               dlabels=attributes(D)$Labels[spe.o])
  }
  else {
    plotcolors(spe.color, rlabels=attributes(D)$Labels,
               main="Dissimilarity Matrix")
    plotcolors(speo.color, rlabels=attributes(D)$Labels[spe.o],
               main="Ordered Dissimilarity Matrix")
  }

  par(op)
}
# Usage:
# coldiss(D = dissimilarity.matrix, nc = 4, byrank = TRUE, diag = FALSE)
# If D is not a dissimilarity matrix (max(D) > 1), then D is divided by max(D)
# nc 							number of colours (classes)
# byrank= TRUE		equal-sized classes
# byrank= FALSE		equal-length intervals
# diag = TRUE			print object labels also on the diagonal
# Example:
# coldiss(spe.dj, nc=9, byrank=F, diag=T)
```


```{r, fig.width = 10, fig.height = 8, eval = T}
# the code for the coldiss() function is in the workshop script.
coldiss(spe.db.pa)
```

---
class: inverse, middle, center

# _Clustering_ ou regroupement

---
# Regroupement

Une application des matrices d'association est le _regroupement hierarchique._

Le regroupement met en évidence les structures des données en partitionnant soit les objets, soit les descripteurs.

Un objectif des écologistes pourrait être de diviser un ensemble de sites en groupes en fonction de leurs conditions environnementales ou de la composition de leur communauté.

Ces résultats peuvent être représentés sous forme de dendrogrammes (diagrammes en forme d'arbre), qui décrivent la proximité des observations.

```{r, fig.width=10, echo = FALSE}
# Demonstration of a cluster dendrogram
spe.hel<-decostand(spe, method="hellinger")
spe.dhel <- vegdist(spe.hel,method="euclidean")
spe.dhel.ward <- hclust(spe.dhel, method="ward.D2")
spe.dhel.ward$height<-sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang=-1) # hang=-1 aligns all objets on the same line
```

???
.center[![:scale 80%](images/cluster1_revised.jpg)]

---
# Méthodes de regroupement hiérarchique

Les algorithmes de regroupement s'appuient généralement sur cette série d'étapes :

1. Calculer une matrice d'association avec toutes les similitudes par paires entre tous les objets ;
2. Réunir les paires d'objets qui sont les plus similaires (ou dissemblables) ;
3. Recalculer la matrice de similarité pour ce groupe _par rapport à tous les objets restants ;
4. Répétez les étapes 2 et 3 jusqu'à ce que tous les objets soient réunis.

--
<br>

Parmi les nombreux algorithmes de clustering hiérarchique existants, nous allons explorer :

1. Le regroupement agglomératif à liaison unique ;
2. Liaison complète, regroupement agglomératif ;
3. Regroupement à variance minimale de Ward.

--

<br>

Dans `R`, nous utiliserons les fonctions `hclust()` et `agnes()` pour construire nos dendrogrammes.

???

Dans le cas d'un regroupement agglomératif par liaison simple (également appelé tri par
voisins), les objets les plus proches s'agglomèrent.
ce qui génère souvent de longues et fines grappes ou chaînes d'objets. À l'inverse, dans le clustering agglomératif à liens complets, un objet ne s'agglomère à un groupe que lorsqu'il est lié à l'élément le plus éloigné du groupe, qui le lie à son tour à tous les membres de ce groupe. Il formera de nombreux petits groupes séparés, et est plus approprié pour rechercher des contrastes, des discontinuités dans les données.
Le clustering à variance minimale de Ward diffère de ces deux méthodes en ce que
elle regroupe les objets en groupes en utilisant le critère des moindres carrés
(similaire aux modèles linéaires). Son dendogramme montre par défaut les distances au carré. Pour comparer avec d'autres méthodes, calculez d'abord la racine carrée des distances.

---

# Regroupement complet de liens

.pull-left4[

![:scale 50%](images/compleClust1.png)

]

.pull-right4[

- Les objets sont répartis en petits groupes (1-2, 3-4, 5)

- Relier les petits groupes en utilisant la plus grande distance entre leurs éléments
* (1-3=0,15, 2-4=0,35, 2-3=0,6, choisir 0,6 pour relier le groupe 1-2 et 3-4)


![](images/compleClust2.png)
]


---
# Comparison

Créez une matrice de distance à partir des données sur les rivières du Doubs transformées par Hellinger et calculez le regroupement par liaison simple :

```{r, fig.width=7, echo = -1}
par(mar=c(.5,3.8,2,.5), cex = 1.5)
spe.dhe1 <- vegdist(spe.hel, method = "euclidean")
spe.dhe1.single <- hclust(spe.dhe1, method = "single")
plot(spe.dhe1.single)
```


---
# Comparaison entre methodes

```{r,fig.width= 15, echo = -1}
par(mfrow=c(1,2), mar=c(.5,2.5,1.5,2.5), cex=1)
spe.dhe1 <- vegdist(spe.hel, method = "euclidean")
spe.dhe1.complete <- hclust(spe.dhe1, method = "complete")
plot(spe.dhe1.single, main="Single linkage clustering", hang =-1)
plot(spe.dhe1.complete, main="Complete linkage clustering", hang=-1)
```

.pull-left[

**Single linkage:**

Des chaînes d'objets se produisent  (e.g. 19, 29, 30, 26)
]

.pull-right[

**Complete linkage:**
Les groupes contrastés sont formés d'objets se produisent
]

???
![](images/comparison.png)

---
# Méthode de variance minimale de Ward

.pull-left[
- Utilise le critère des moindres carrés pour regrouper les objets en groupes.

- À chaque étape, la paire de clusters qui fusionne est celle qui entraîne l'augmentation minimale de la somme totale des carrés à l'intérieur du groupe.

- Les groupes générés par cette méthode ont tendance à être plus sphériques et à contenir un nombre similaire d'objets.
]

---
# Méthode de variance minimale de Ward

Calculez le regroupement à variance minimale de Ward et tracez le dendrogramme en utilisant la racine carrée des distances :

```{r, fig.height=4, echo=-1}
par(mar=c(.5,2.5,1.5,.5), cex = 1)
spe.dhel.ward <- hclust(spe.dhe1, method = "ward.D2")
spe.dhel.ward$height <- sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang = -1) # hang = -1 aligns objects at 0
```

> L'algorithme de clustering de Ward ne peut être appliqué qu'avec des distances euclidiennes. Les distances de Bray-Curtis ne sont pas euclidiennes, elles doivent donc être racinées au carré avant.

---
# Questions importantes à aborder

- Le choix de la méthode "appropriée" dépend de votre objectif ;
- Voulez-vous mettre en évidence des gradients ou des contrastes ?
- Il ne s'agit *pas* d'une méthode statistique et vous devrez peut-être vous appuyer sur des méthodes complémentaires pour étayer vos hypothèses.

<br>

Cependant, le clustering nous permet de :
- Déterminer le nombre optimal de clusters interprétables ;
- Calculer les statistiques de clustering ;
- Combiner le clustering à l'ordination pour distinguer des groupes de sites.

---

#### Et maintenant ?

Alors que l'**analyse de regroupements** remonte les *discontinuités* dans un ensemble de données, l'**ordination** extrait les principales tendances sous la forme d'axes continus.

Nous allons examiner maintenant trois types de **méthodes d'ordination sans contrainte**...

--

.right[...**attendez**, qu'est-ce qu'on entend par **ordination sans contrainte** ? *Quelqu'un* ?]

--
<br>
.center[*Si personne ne s'exprime, choisissez une personne "volontaire" !*]

--

**Les ordinations non contraintes** évaluent les relations chez un seul ensemble de variables. *Aucune tentative* n'est faite pour définir la relation entre un ensemble de variables indépendantes et une ou plusieurs variables dépendantes.

--

L'interprétation des effets potentiels d'autres facteurs sur les patrons observés ne peut se faire qu'indirectement, car ces facteurs ne sont *pas* explicitement inclus dans les analyses !

--

Ici, nous allons explorer :
.small[
.center[
.pull-left2[
**A**nalyse des **C**omposants **P**rincipales

**A**nalyse des **Co**ordonnées **P**rincipales
]

.pull-right2[
**É**chelonnement **M**ultidimensionnel **N**on-**M**étrique
]
]
]


---
### Mais d'abord, faisons un *récapitulatif*...

Nous avons déjà compris la signification de **variance** et **moyenne**, et comment les calculer :


.pull-left[
$$\sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$
]

.pull-right[
$$\mu_x = \frac{1}{n} \sum_{i=i}^{n} x_{i}$$
]

Elles sont très utiles pour comprendre le *centre* et la *dispersion* d'une variable ou d'une dimension donnée.

Néanmoins, nous sommes souvent intéressés **par plus d'une dimension**, et nous voulons mesurer dans quelle mesure chaque dimension varie de la moyenne *par rapport aux autres*.

--

La **covariance** est une telle mesure, qui peut décrire comment **deux dimensions co-varient** :

.pull-left[
$$var_x = \sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$
]

.pull-right[
$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$
]

---

### Mais d'abord, faisons un *récapitulatif*...

Intuitivement, nous pouvons mesurer la **covariance** entre plus de deux variables. Disons, entre les variables $x$, $y$, et $z$ :

.pull-left[
$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$
]

.pull-right[
$$cov_{x,z}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(z_{i}-\bar{z})}{N-1}$$
]

$$cov_{z,y}=\frac{\sum_{i=1}^{N}(z_{i}-\bar{z})(y_{i}-\bar{y})}{N-1}$$
Nous pouvons représenter ces calculs dans une **matrice de covariance** :

$${C(x, y, z)} = \left[ \begin{array}{ccc}
cov_{x,x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & cov_{y,y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & cov_{z,z}
\end{array} \right]$$

--

**QUIZ**: *Que sont les diagonales ?* *Et, que se passe-t-il si les variables sont indépendantes ?*

---

##### Toujours en train de *récapituler*...

.center[***Que sont les diagonales ?***]

Si, $cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$, alors:

$$cov_{x,x}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(x_{i}-\bar{x})}{N} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2} {N} = var_x$$
--

Afin que :

$${C(x, y, z)} = \left[ \begin{array}{ccc}
cov_{x,x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & cov_{y,y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & cov_{z,z}
\end{array} \right] = \left[ \begin{array}{ccc}
var_{x} & cov_{y,x} & cov_{z,x} \\
cov_{x,y} & var_{y} & cov_{z,y} \\
cov_{x,z} & cov_{y,z} & var_{z}
\end{array} \right]$$

 <br>

.center[La covariance d'une variable avec elle-même est sa *variance* !]

---

##### Toujours en train de *récapituler*...

.center[***Que se passe-t-il si les variables sont indépendantes ?***]

.pull-left[
```{r xyz-norm, echo=TRUE}
x <- rnorm(5000, mean = 0, sd = 1)
y <- rnorm(5000, mean = 0, sd = 1)
z <- rnorm(5000, mean = 0, sd = 1)

xyz <- data.frame(x, y, z)

GGally::ggpairs(xyz)
```
]

--

.pull-right[
```{r xyz-cov, echo=TRUE, eval = FALSE}
cov(xyz)
```

```{r xyz-cov-2, echo=FALSE, eval = TRUE}
round(cov(xyz), digits = 5)
```

Si les variables sont parfaitement indépendantes (ou non corrélées), la matrice de covariance $C(x, y, z)$ est :

$${C(x, y, z)} =  \left[ \begin{array}{ccc}
var_{x} & 0 & 0 \\
0 & var_{y} & 0 \\
0 & 0 & var_{z}
\end{array} \right]$$

*i*.*e*. une covariance plus proche de $1$ signifie que les variables sont *colinéaires*.

Et ici, $var_{x} = var_{y} = var_{z} = 1$.

]

---
## Transformations linéaires

Nous sommes souvent intéressés à l'observation des variables d'autres *formes*.

Pour cela, nous créons une nouvelle variable, disons $x_{new}$, en utilisant des constantes pour modifier la variable originale $x$. Par exemple :

--

.pull-left[
On peut transformer des distances mesurées en kilomètres $d_{km}$ en miles :
$$d_{mi} = 0.62 \times d_{km}$$
]

.pull-right[
On peut aussi transformer des degrés Fahrenheit en degrés Celsius :

$$T_{C} = 0.5556\times T_{Fahr} - 17.778$$
]

Ces exemples sont des **transformations linéaires** car les variables transformées sont liées linéairement aux variables d'origine et les formes de la distribution ne sont pas modifiées.

--

Deux types de transformations sont très importantes pour nous :

.pull-left[
**Centrage**, qui soustrait les valeurs d'un prédicteur de la moyenne :

$$x' = x_i - \bar{x}$$
]

.pull-right[
**Réduction**, qui divise chaque valeur d'une variable par son écart-type:

$$x'' = \frac{x_i}{\sigma_x}$$
]

???

Le centrage est essentiellement une technique où la moyenne des variables indépendantes est soustraite de toutes les valeurs. Cela signifie que toutes les variables indépendantes ont une moyenne nulle. La réduction est complementaire au centrage. Les variables prédictives sont divisées par leur écart type.

Le présentateur doit mentionner ici que le centrage amène la moyenne à zéro et la mise à l'échelle amène l'écart type à une unité. Ils doivent également mentionner que les variables deviennent comparables lors de la mise à l'échelle, car leur unité est perdue.

---

### Décomposition des matrices en éléments propres

**Les matrices carrées**, telles que la **matrice de covariance**, peuvent être décomposées en *valeurs propres* et en *vecteurs propres*.

Pour une matrice carrée, $A_{n \times n}$, un vecteur $v$ est un *vecteur propre* de $A$ s'il y a un *scalaire*, $\lambda$, pour lequel :

.center[
$A_{n \times n} v_{n \times 1} = \lambda  v_{n \times 1}$, or $\left(\begin{matrix}a_{11}&\cdots&a_{1n}\\\vdots&\ddots&\vdots\\a_{1n}&\cdots&a_{nn}\\\end{matrix}\right)\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)=\lambda\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)$
]

la valeur de $\lambda$ étant la *valeur propre* correspondante.

--

c.-à-d. que la matrice $A$ *étire* effectivement le vecteur propre $v$ par la quantité spécifiée par la valeur propre (*scalaire*) $\lambda$.

Un *vecteur propre* est un vecteur dont la direction reste inchangée lorsqu'on lui applique une **transformation linéaire**.

<br>

.center[Attendez ! Qu'entendons-nous par *direction inchangée* ?]

---

### Décomposition des matrices en éléments propres

.center[Attendez ! Qu'entendons-nous par *direction inchangée* ?]

<br>

Représentons cela avec cet exemple simple.

Nous pouvons transformer un carré en un parallélogramme à l'aide d'une **transformation affine** à axe unique.

--

.pull-left[
Soit $S$ le carré de vertices $(0,0),\,(1,0),\,(1,1),\,(1,0)$ qui sera transformé par cisaillement en parallélogramme $P$ de vertices $(0,0),\,(1,0),\,(1,1.57),\,(1,0.57)$.

Nous pouvons voir qu'après la transformation linéaire, la flèche violette n'a pas changé de direction, c.-à-d. qu'elle est un *vecteur propre* de $S$.

En revanche, la flèche rouge a changé de direction, et *n'est donc pas* un *vecteur propre* de $S$.
]

.pull-right[
```{r echo = FALSE}
library(PlaneGeometry)

P <- c(0, 0)
w <- c(1, 0)

ratio <- 1
angle <- 30

shear <- Shear$new(P,
                   w,
                   ratio,
                   angle)

wt <- ratio * c(-w[2], w[1])

Q <- P + w
R <- Q + wt
S <- P + wt
A <- shear$transform(P)
B <- shear$transform(Q)
C <- shear$transform(R)
D <- shear$transform(S)


plot(0, 0, type = "n", asp = 1, xlim = c(0,1), ylim = c(0,2))

lines(rbind(P, Q, R, S, P),
      lwd = 2) # unit square

lines(rbind(A, B, C, D, A),
      lwd = 2,
      col = "blue") # image by the shear

arrows(x0 = A[1],
       y0 = A[2],
       x1 = B[1],
       y1 = B[2],
       col = "red",
       lwd = 2)

arrows(x0 = A[1],
       y0 = A[2],
       x1 = D[1],
       y1 = D[2],
       col = "purple",
       lwd = 2)
```
]

---

##### Décomposition des matrices en éléments propres : implications

.center[
*Suivons avec la torture algébrique !*
]

### Orthogonalité

Une propriété *fabuleuse* et *simple* des matrices *symétriques* que nous pouvons expliquer ici !

.pull-left[
Supposons que $x$ soit un vecteur propre de $A$ correspondant à la valeur propre $λ_1$ et $y$ un vecteur propre de $A$ correspondant à la valeur propre $λ_2$, avec $λ_1≠λ_2$.

$$Ax=\lambda_1x \\
Ay=\lambda_2y$$

Multiplions chacun d'eux par l'autre *vecteur propre* transposé.
$$y^{\intercal}Ax=\lambda_1y^{\intercal}x \\ x^{\intercal}A^{\intercal}y=\lambda_2x^{\intercal}y$$

]

--

.pull-right[

Maintenant, soustrayons la deuxième équation de la première et utilisons la commutativité du produit scalaire :

$y^{\intercal}Ax-x^{\intercal}A^{\intercal}y=\lambda_1y^{\intercal}x - \lambda_2x^{\intercal}y \\ 0 = (\lambda_1 - \lambda_2)y^{\intercal}x$

Parce que nous savons que $\lambda_1-\lambda_2\neq 0$, alors
$y^{\intercal}x = 0$, *c-*.*à*.*-d*., $\mathbf{x}\perp\mathbf{y}$, *c-*.*à*.*-d*. sont **orthogonaux** !

*Alors, que révèle la décomposition de la matrice de variance-covariance en elements propres ?*
]

???

L'explication de cette partie est très utile et assez simple, afin que chacun puisse comprendre ce qu'est l'orthogonalité. Il s'agit d'opérations simples d'équations et de soustractions.

---
##### Décomposition des matrices en éléments propres : implications

.center[
*Suivons avec la torture algébrique !*
]

### Maximisation

.pull-left[
Si $v_i' v_i = 1$, alors $Av_i=\lambda_iv_i$ peut être écrite comme :
$$v_i' A v_i = \lambda_i$$
En effet, $v' A v$
est la variance d'une combinaison linéaire avec des poids en $v$, *i*.*e*. $\text{Var}(v_i'\,A)=v_i'\,\text{Var}(A)\,v_i$.

*Donc, on peut relier les points !*

]

--

.pull-right[
Rappelez-vous que les *valeurs propres* de notre *matrice de variance-covariance* $A$ sont directement liées à la variance !

Pour trouver un vecteur $v$ qui maximise la variance, $v' A v$, il suffit de choisir le *vecteur propre* correspondant à la plus grande *valeur propre* $\lambda_i$ !

De sorte que la variance maximale soit de $\lambda_1$ !
]

--

La *variance expliquée* de chaque *vecteur propre* obéit à l'ordre : $\lambda_1 > \lambda_2 > \dots > \lambda_k$.

Cela nous permet de condenser un plus grand nombre de variables originales en un ensemble plus petit de vecteurs sélectionnés avec une perte minimale d'informations (c.-à-d. **une réduction de la dimensionnalité**).

---
# Méthodes d'ordination sans contrainte

Ceci est un bon point de départ pour nous mettre sur la voie des **méthodes d'ordination sans contrainte** que nous allons étudier aujourd'hui !

.pull-left[
Ils nous permettent de :
- Évaluer les relations *dans* un ensemble de variables (espèces ou variables environnementales);
- Trouver les composantes clés de la variation entre les échantillons, les sites, les espèces ;
- Réduire le nombre de dimensions dans les données multivariées tout en limitant la perte substantielle d'informations ;
- Créer de nouvelles variables à utiliser dans des analyses ultérieures.
]

.pull-right[
Ici, nous apprendrons :

1. **P**rincipal **C**omponent **A**nalysis;

2. **P**rincipal **Co**ordinate **A**nalysis;

3. **N**on-Metric **M**ulti**d**imensional **S**caling;
]
---
# Analyse en Composantes Principales

.small[
L'analyse en composantes principales (ACP) est une technique de réduction de la dimensionnalité *linéaire*, c.-à-d. qu'elle réduit les données fortement corrélées.

En bref, l'ACP transforme *linéairement* des variables originales vers des nouvelles variables contenant des **composantes principales**, qui expliquent la plupart de la variance dans l'ensemble de données, c.-à-d. qui maximisent la séparation entre les données.
]

--
.pull-left[

.small[L'espace des *composants principaux* peut être écrit comme suit :]

$$Z_p = ∑_{j=1}^p ϕ_j * X_j$$
]
.small[
où,
1. $Z_p$ est la composante principale $p$ ;
2. $ϕ_j$ est le vecteur de charge comprenant les $j$ charges pour la composante principale $p$, c.-à-d. les coefficients de la combinaison linéaire des variables originales à partir desquelles les composantes principales sont construites ;
3. $X_j$ est le prédicteur normalisé, c.-à-d. avec une moyenne égale à zéro et un écart-type égal à un.
]

???

c.-à-d. que la reconstruction des données peut être donnée par une simple combinaison linéaire des composantes.


Les futures versions de ce document devraient inclure une figure (telle qu'une carte thermique) avec les données normalisées originales sur le côté gauche, et sur le côté droit, une carte thermique des chargements fois une carte thermique des composants (qui peut être considérée comme égale à la somme des matrices de rang un) étant égale à la carte thermique des données reconstruites.

---
# Analyse en Composantes Principales

L'ACP peut être calculée d'au moins *quatre* façons différentes.

Pour des raisons de simplicité, nous nous concentrerons ici sur la façon d'obtenir des composantes principales à partir d'une matrice de corrélation.

Nous apprendrons à le faire à partir de rien, puis à utiliser les paquets `R` pour calculer les composantes principales.

---

### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;
]

.pull-right3[
Dans `R`, à partir de zéro !

```{r}
data(varechem)

str(varechem)
```

]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;
]

.pull-right3[
Dans `R`, à partir de zéro !

```{r}
data(varechem)

# Étape 1
Y <- varechem[, 1:2]

head(Y)
```

]

---
#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient à centrer, comme dans $y_c = [y_i - \bar{y}]$, puis à mettre à l'échelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;
]

.pull-right3[
Dans `R`, à partir de zéro !

```{r}
data(varechem)

# Étape 1
Y <- varechem[, 1:2]

# Étape 2
Y_std <- as.matrix(scale(Y))

head(Y_std)

round(apply(Y_std, 2, mean))
round(apply(Y_std, 2, sd))

```

]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient à centrer, comme dans $y_c = [y_i - \bar{y}]$, puis à mettre à l'échelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;
]

.pull-right3[
Dans `R`, à partir de zéro !

```{r}
data(varechem)

# Étape 1
Y <- varechem[, 1:2]

# Étape 2
Y_std <- as.matrix(scale(Y))

# Étape 3
(Y_R <- cov(Y_std))
```
]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient à centrer, comme dans $y_c = [y_i - \bar{y}]$, puis à mettre à l'échelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;

4. Effectuer la décomposition de la matrice de covariance pour obtenir la matrice $U$ des vecteurs propres, contenant les *composantes principales* ;
]

.pull-right3[
Dans `R`, à partir de zéro !

```{r}
data(varechem)

# Étape 1
Y <- varechem[, 1:2]

# Étape 2
Y_std <- as.matrix(scale(Y))

# Étape 3
Y_R <- cov(Y_std)

# Étape 4
(Eigenvalues <- eigen(Y_R)$values)

(Eigenvectors <- eigen(Y_R)$vectors)
```
]

---

### Analyse en composantes principales : étape par étape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, à chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[

Nous pouvons représenter les distances des observations au premier vecteur propre (`PC1`, en rouge).

La première composante principale est dessinée de façon à ce que la variation des valeurs le long de sa ligne soit maximale.

Les flèches sur les composantes principales sont obtenues en multipliant leurs *valeurs propres* par les *vecteurs propres*.
]

.pull-right[
```{r echo=FALSE}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

plot(N ~ P,
     col = as.factor(rownames(Y_std)),
     main="Distances to PC1",
     pch = 19,
     xlim=c(-2.2, 2.2),
     ylim = c(-2.2,2.2),
     data = as.data.frame(Y_std))

abline(v=0 , h=0,
       col = "dark gray")

#Overlap pertinent evectors

abline(0,
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0,
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0,
#        y0 = 0,
#        x1 = Eigenvalues[2]*Eigenvectors[1,2],
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange",
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0,
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  #finds endpoint for a perpendicular segment from the point (x0,y0) to the line1
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0,
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P,
                         Y_std$N,
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0,
         x1 = ss$x1,
         y0 = ss$y0,
         y1 = ss$y1,
         col = 'purple')

points(N ~ P,
       col = as.factor(rownames(Y_std)),
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P,
          labels = as.factor(rownames(Y_std)),
                     pos = 1,
          cex=1.4))
```
]

???

Dans cette première représentation, nous pouvons observer qu'une première direction (ou la première composante linéaire) est tracée en essayant de maximiser la variance des données.

Les participants peuvent vous demander en quoi cela est différent d'une régression linéaire. L'une des principales différences réside dans la façon dont les carrés d'erreurs sont minimisés perpendiculairement à la ligne droite (90 degrés, ce qui la rend orthogonale), alors que dans la régression linéaire, les carrés d'erreurs sont minimisés dans la direction des ordonnées.

---

### Analyse en composantes principales : étape par étape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, à chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[
Nous pouvons alors représenter les distances des observations au deuxième vecteur propre (`PC2`, en orange).

La deuxième composante principale est également dessinée en maximisant la variance des données.

Notez comment les composantes principales sont orthogonales !
]

.pull-right[

```{r echo = FALSE}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

plot(N ~ P,
     col = as.factor(rownames(Y_std)),
     main="Distances to PC2",
     pch = 19,
     xlim=c(-2.2, 2.2),
     ylim = c(-2.2,2.2),
     data = as.data.frame(Y_std))

abline(v=0 , h=0,
       col = "dark gray")

#Overlap pertinent evectors

abline(0,
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0,
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[2]*Eigenvectors[1,2],
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange",
       lwd = 2)


line2 <- c(0,
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0,
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P,
                         Y_std$N,
                         line2)

segments(x0 = ss$x0,
         x1 = ss$x1,
         y0 = ss$y0,
         y1 = ss$y1,
         col = 'orange')

points(N ~ P,
       col = as.factor(rownames(Y_std)),
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P,
          labels = as.factor(rownames(Y_std)),
                     pos = 1,
          cex=1.4)
     )
```

]

--
.pull-left[
*Nous avons représenté les vecteurs propres, c.-à-d. les composantes principales !*

*Mais, quelle est l'utilité des valeurs propres ?*
]

???

Ici, la deuxième direction (ou la deuxième composante linéaire) est tracée de manière à ce que la variance des données soit maximisée par rapport à cette deuxième composante.

---

#### Analyse en composantes principales : étape par étape

Nous avons vu que les *valeurs propres* représentent la magnitude (la variance) des composantes principales.

.pull-left[
En fait, la somme de toutes les *valeurs propres* est égale à la somme des variances, qui sont représentées sur la diagonale de la matrice de variance-covariance.
]

.pull-right[
```{r}
sum(diag(cov(Y_std)))
sum(eigen(cov(Y_std))$values)
```
]

--

Intuitivement, on peut obtenir l'influence relative de chaque *vecteur propre* $v_{k}$ (ou $\text{PC}_{k}$) en divisant leurs valeurs par la somme de toutes les *valeurs propres*.

$$\text{Variance expliquée de}~v_{k} = \frac{\lambda_{v_k}}{\sum^p_{i=1}{\lambda_{v}}}$$

En faisant cela, nous pouvons dire que le $\text{PC}1$ explique `r round(eigen(cov(Y_std))$values[1]/sum(eigen(cov(Y_std))$values) * 100)`% de la variance dans les données, tandis que $\text{PC}2$ explique `r round(eigen(cov(Y_std))$values[2]/sum(eigen(cov(Y_std))$values) * 100)`% de la variance.

--

Enfin, nous pouvons procéder à la dernière étape de notre analyse des composantes principales !

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice $Y$ de $n$ observations et $p$ variables continues normalement distribuées ;

2. Normalisation des observations, comme dans $Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}$ ; ce qui revient à centrer, comme dans $y_c = [y_i - \bar{y}]$, puis à mettre à l'échelle, comme dans $y_s = \frac{y_i}{\sigma_y}$ ;

3. Calculer la matrice de variance-covariance $R = cov(Y_{std})$ ;

4. Effectuer la décomposition de la matrice de covariance pour obtenir la matrice $U$ des vecteurs propres, contenant les *composantes principales* ;

5. Obtenir la *matrice des coordonnées* $F$ en multipliant $U$ par la matrice normalisée $Y_{std}$.
]

.pull-right3[
En `R`, à partir de zéro !

```{r}
# Étape 1
Y <- varechem[, 1:2]

# Étape 2
Y_std <- as.matrix(scale(Y))

# Étape 3
Y_R <- cov(Y_std)

# Étape 4
Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

# Étape 5
F_PrComps <- Y_std %*% Eigenvectors
head(F_PrComps)
```
]

---

### Analyse en composantes principales : étape par étape

La *matrice des coordonnées*, $F$, (objet `F_PrComps`) permet de *rotationner* le nouvel espace de données, afin qu'il soit représenté par rapport aux composantes principales.

.pull-left[
.center[
$\text{N}$ ~ $\text{P}$
]

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5}
Eigenvectors. <- as.data.frame(Eigenvectors)
row.names(Eigenvectors.) <- c("P", "N")
colnames(Eigenvectors.) <- c("PC1", "PC2")
Eigenvectors.[, 1] <- Eigenvectors.[, 1]*-1
Y_std <- as.data.frame(Y_std)

op <- par(mfrow = c(2, 1),     # 2x2 layout
    oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
    mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
    )

plot(N ~ P,
     col = as.factor(rownames(Y_std)),
     pch = 19,
     xlim=c(-2.2, 2.2),
     ylim = c(-2.2,2.2),
     data = as.data.frame(Y_std))

abline(v=0 , h=0,
       col = "dark gray")

#Overlap pertinent evectors

abline(0,
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
# abline(0,
#        Eigenvectors[1, 2]/Eigenvectors[2, 2],
#        col='orange')

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

# arrows(x0 = 0,
#        y0 = 0,
#        x1 = Eigenvalues[2]*Eigenvectors[1,2],
#        y1 = Eigenvalues[2]*Eigenvectors[2, 2],
#        col = "orange",
#        lwd = 2)

# Plot the lines from first evector to points

line1 <- c(0,
           Eigenvectors[2, 1]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line1){
  a <- line1[1]  #intercept
  b <- line1[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0,
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P,
                         Y_std$N,
                         line1)
# do.call(segments, ss)
# which is the same as:

segments(x0 = ss$x0,
         x1 = ss$x1,
         y0 = ss$y0,
         y1 = ss$y1,
         col = 'purple')

points(N ~ P,
       col = as.factor(rownames(Y_std)),
       pch = 19,
       data = Y_std)
with(Y_std,
     text(N ~ P,
          labels = as.factor(rownames(Y_std)),
                     pos = 1,
          cex=1.4))


plot(N ~ P,
     col = as.factor(rownames(Y_std)),
     pch = 19,
     xlim=c(-2.2, 2.2),
     ylim = c(-2.2,2.2),
     data = as.data.frame(Y_std))

abline(v=0 , h=0,
       col = "dark gray")

#Overlap pertinent evectors

abline(0,
       Eigenvectors[2, 1]/Eigenvectors[1, 1],
       col='purple')
abline(0,
       Eigenvectors[1, 2]/Eigenvectors[2, 2],
       col='orange')

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[1]*Eigenvectors[1, 1],
       y1 = Eigenvalues[1]*Eigenvectors[2, 1],
       col = "purple",
       lwd = 2)

arrows(x0 = 0,
       y0 = 0,
       x1 = Eigenvalues[2]*Eigenvectors[1,2],
       y1 = Eigenvalues[2]*Eigenvectors[2, 2],
       col = "orange",
       lwd = 2)


line2 <- c(0,
           Eigenvectors[1, 2]/Eigenvectors[1, 1])

perp.segment.coord <- function(x0, y0, line2){
  a <- line2[1]  #intercept
  b <- line2[2]  #slope
  x1 <- (x0 + b * y0 - a * b)/(1 + b^2)
  y1 <- a + b * x1
  list(x0 = x0, y0 = y0,
       x1 = x1, y1 = y1)
}

ss <- perp.segment.coord(Y_std$P,
                         Y_std$N,
                         line2)

segments(x0 = ss$x0,
         x1 = ss$x1,
         y0 = ss$y0,
         y1 = ss$y1,
         col = 'orange')

points(N ~ P,
       col = as.factor(rownames(Y_std)),
       pch = 19,
       data = Y_std)

with(Y_std,
     text(N ~ P,
          labels = as.factor(rownames(Y_std)),
                     pos = 1,
          cex=1.4)
     )

title(xlab = "N",
      ylab = "P",
      outer = TRUE, line = 3)

par(op)
```
]

.pull-right[
.center[
$\text{PC}1$ ~ $\text{PC}2$
]

```{r echo=FALSE, fig.height = 6.5, fig.width = 3.5}
score <- as.data.frame(F_PrComps)

colnames(score) <- c("PC1", "PC2")

op <- par(mfrow = c(2, 1),     # 2x2 layout
          oma = c(2, 2, 0, 0), # two rows of text at the outer left and bottom margin
          mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
          mgp = c(2, 1, 0)    # axis label at 2 rows distance, tick labels at 1 row
)


plot(PC2 ~ PC1,
     col = as.factor(rownames(score)),
     pch = 19,
     xlim = c(-2.2, 2.2), ylim = c(-2.2,2.2), xlab='PC1', ylab='PC2',data = score)

abline(h = 0, col = 'purple')
abline(v = 0, col='orange')


perp.segment.horiz <- function(x0, y0){
  x1 <- x0
  y1 <- 0
  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1 <- perp.segment.horiz(score[,1], score[,2])

segments(x0 = ss1$x0, x1 = ss1$x1, y0 = ss1$y0, y1 = ss1$y1, col='purple')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)
with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))


plot(PC2 ~ PC1, col=as.factor(rownames(score)),
     pch = 19, xlim=c(-2.2, 2.2), ylim = c(-2.2,2.2),
     xlab='PC1', ylab='PC2',data=score)

abline(h = 0, col = 'purple')
abline(v = 0, col ='orange')


perp.segment.vert <- function(x0, y0){
  x1 <- 0
  y1 <- y0

  list(x0 = x0, y0 = y0, x1 = x1, y1 = y1)
}

ss1a <- perp.segment.vert(score[,1], score[,2])
segments(x0 = ss1a$x0, x1 = ss1a$x1, y0 = ss1a$y0, y1 = ss1a$y1, col='orange')


points(PC2 ~ PC1, col=as.factor(rownames(score)), pch = 19, xlab='V1', ylab='V2',data=score)

with(score,text(PC2 ~ PC1, labels=as.factor(rownames(score)), pos = 3, cex=1.4))

title(xlab = "PC1",
      ylab = "PC2",
      outer = TRUE,
      line = 3)

par(op)
```
]

???
Les titres des axes ne se font pas imprimer. Je les ai inclus en haut de chaque graphique, mais ce problème n'est pas résolu.

Le présentateur ou la présentatrice devrait souligner la rotation et parler de ce que sont les points. Vous pouvez survoler les points et leur montrer quelle était la position des points dans les graphiques " sans rotation ", et maintenant dans les graphiques " avec rotation ", en soulignant que maintenant, les " nouveaux axes " sont PC1 et PC2.

Cette compréhension sera utile lorsque les participants utiliseront les fonctions PCA implémentées dans R.

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]

```{r}
data(varechem)

Y <- varechem[, 1:2]
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`stats::prcomp()`?
```{r}
PCA_prcomp <- prcomp(Y,
                     center = TRUE,
                     scale = TRUE)

# or PCA_prcomp <- prcomp(Y_std)

head(PCA_prcomp$x)
```
]

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]

```{r}
data(varechem)

Y <- varechem[, 1:2]
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`stats::princomp()` ?
```{r}
PCA_princomp <- princomp(Y_std)

head(PCA_princomp$scores)
```
]

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]

```{r}
data(varechem)

Y <- varechem[, 1:2]
Y_std <- as.matrix(scale(Y))
Y_R <- cov(Y_std)

Eigenvalues <- eigen(Y_R)$values
Eigenvectors <- eigen(Y_R)$vectors

F_PrComps <- Y_std %*% Eigenvectors

head(F_PrComps)
```
]

.pull-right[
<br>
`vegan::rda()` ?
```{r}
PCA_vegan_rda <- rda(Y_std)

scores(PCA_vegan_rda,
       display = "sites",
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
```

`vegan::rda()` utilise des mises à l'échelle alternatives. Vous pouvez étudier la `vignette("decision-vegan")`.
]

???

Dites aux participants que le nom `rda` fait référence à un type différent de technique d'ordination sous contrainte, mais que si nous exécutons `rda()` avec une seule variable, il exécutera une ACP.

---

# Analyse en composantes principales

Nous avons implémenté l'ACP sur un ensemble de données à deux variables pour plus de simplicité.

Avançons et appliquons-la à notre jeu de données sur les espèces de poissons.


Pour cela, nous allons utiliser la fonction `vegan::rda()` sur les données de poissons *transformées par Hellinger* et résumer les résultats :

```{r}
spe.h.pca <- rda(spe.hel)

# summary(spe.h.pca)
```

---

# Analyse en composantes principales

.pull-left[
Les premières lignes de `summary.rda()` nous renseignent sur la *variance totale* et la *variance sans contrainte* de notre modèle.
]

.pull-right[

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[5:8])
```
]

--

.pull-left2[

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(12:16, 21:24)])
```

]

.pull-right2[

Viennent ensuite les *valeurs propres*, et leur contribution à la variance.


En fait, si nous additionnons toutes nos *valeurs propres*, nous obtiendrons la quantité de variance sans contrainte expliquée par l'analyse !

```{r}
sum(spe.h.pca$CA$eig)
```

]

???

Puisque nous n'avons pas contraint notre ordination, la variance proportionnelle non contrainte est égale à la variance totale.

Prenez un moment pour expliquer la proportion expliquée, et montrez que la proportion cumulée sera égale à 1 au 27ème PC.

---

# Analyse en composantes principales

Les informations suivantes sont liées à la *mise à l'échelle* (`scaling`), aux *scores d'espèces*, et aux *scores de sites*.

```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(26:29, 31:32, 34:40, 63:64, 66:72)])
```
]

---

# Analyse en composantes principales

.pull-left[
`Species` fait référence à vos descripteurs (c.-à-d. les colonnes de votre ensemble de données), qui sont ici les espèces de poissons.

`Scores` font référence à la position de chaque espèce le long des composantes principales.
]

.pull-right[
```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(32, 34:40)])
```
]

--


.pull-left2[
```{r echo=FALSE}
paste(capture.output(summary(spe.h.pca))[c(64, 66:72)])
```
]

.pull-right2[
`Sites` représentent les lignes de votre jeu de données, qui sont ici les différents sites le long de la rivière *Doubs*.
]

--

<br>

.pull-left[
Cette information peut être obtenue avec la fonction `score()`:
]

.pull-right[

```{r, eval = FALSE, echo = TRUE}
scores(spe.h.pca,
       display = "species" or "sites")
```

]

---
### Analyse en composantes principales : condensation des données

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

--

#### Critère de Kaiser-Guttman

.pull-left[
Nous pouvons sélectionner les composantes principales qui capturent plus de variance que l'explication moyenne de toutes les composantes principales. Nous le faisons en

1. Extraire les *valeurs propres* associées aux composantes principales ;

2. Sélectionner les *valeurs propres* au-dessus de la *valeur propre* moyenne :

```{r}
ev <- spe.h.pca$CA$eig
# ev[ev > mean(ev)]
```
]

.pull-right[
```{r, echo = -1, fig.width=10, fig.height = 5.5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```
]

---
### Analyse en composantes principales : condensation des données

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

#### Modèle *broken-stick*

.pull-left[
Le modèle *broken-stick* retient les composantes qui expliquent plus de variance que ce qui serait attendu en divisant aléatoirement la variance en $p$ parties.

```{r}
head(bstick(spe.h.pca))
```
]

.pull-right[
```{r, echo = TRUE, fig.width=4.5, fig.height = 4.5}
screeplot(spe.h.pca,
          bstick = TRUE, type = "lines")
```
]

---
## Analyse en composantes principales

Il ne reste plus qu'à discuter de la *mise à l'échelle* et à *visualiser* nos résultats.

Pratiquons et calculons une ACP sur les variables environnementales standardisées pour le même ensemble de données.

```{r}
env.pca <- rda(env.z)
# summary(env.pca, scaling  = 2)
```

--

Déterminons notre ensemble de *valeurs propres* et leurs *vecteurs propres* correspondants :

.pull-left[
```{r}
ev <- env.pca$CA$eig
```

```{r}
ev[ev>mean(ev)]
```
]

--

.pull-right[
```{r, echo = FALSE, fig.width=8, fig.height = 5}
par(mar=c(4,4,2.5,.5), cex = 1.5)
n <- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```
]

---

# Analyse en composantes principales : `plot()`

L'information calculée par l'ACP peut être représentée par des *biplots*.

Nous pouvons produire un biplot rapide de l'ACP en utilisant la fonction `plot()` en base `R`.

```{r, echo = -1}
par(mar=c(4,4, 0.1,0.1), cex = 1.5)
plot(spe.h.pca)
```


---

# Analyse en composantes principales : `plot()`


`biplot()` de `base` `R` permet une meilleure interprétation.

.pull-left2[
```{r, echo = -1, fig.height=6, fig.width=6.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca)
```
]

.pull-right2[

Les flèches sont tracées pour montrer la directionnalité et l'angle des descripteurs dans l'ordination.

Les descripteurs situés à 180 degrés les uns des autres sont corrélés négativement ;

Les descripteurs situés à 90 degrés l'un de l'autre ont une corrélation nulle ;

Les descripteurs situés à 0 degré l'un de l'autre sont positivement corrélés.

]

---
# Analyse en composantes principales : *mise à l'échelle*

.small[
*Mise à l'échelle de type 2* (`scaling = 2`, défaut) : les distances entre les objets ne sont pas des approximations des distances euclidiennes ; les angles entre les vecteurs des descripteurs (espèces) reflètent leurs corrélations.
]

.small[
*Mise à l'échelle de type 1* (`scaling = 1`) : tente de préserver la distance euclidienne (dans un espace multidimensionnel) entre les objets (sites) : les angles entre les vecteurs des descripteurs (espèces) ne sont pas pertinents.
]

.pull-left[
```{r, echo = -1, fig.height=3.5, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 1)
```
]

.pull-right[
```{r, echo = -1, fig.height=3.5, fig.width=4.5}
par(mar = c(4,4,0.05,0.05), cex = 1.2)
biplot(spe.h.pca, scaling = 2)
```
]

???
2 : **Meilleur pour l'interprétation des relations entre les descripteurs (espèces)**.


1 : **Meilleur pour l'interprétation des relations entre les objets (sites)!**

---
# Défi #2 ![:cube]()

En utilisant tout ce que vous avez appris, calculez une ACP sur les données d'abondance des espèces d'acariens.

```{r}
data(mite)
```

Soyez prêt à discuter et à répondre :
- Quelles sont les composantes principales *les plus pertinentes*, c.-à-d. les sous-ensembles ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?

---

# Défi #2: Solution

Calculer l'ACP sur les données d'espèces transformées par Hellinger.

```{r}
mite.spe.hel <- decostand(mite,
                          method = "hellinger")

mite.spe.h.pca <- rda(mite.spe.hel)
```

--

.pull-left[
Appliquer le critère de Kaiser-Guttman

```{r, eval = F}
ev <- mite.spe.h.pca$CA$eig
ev[ev>mean(ev)]
n <- length(ev)
barplot(ev, main = "Eigenvalues",
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright",
       "Average eigenvalue",
       lwd = 2,
       col = "red3", bty = "n")
```
]

.pull-right[
```{r, echo = F, fig.width=5, fig.height=4}
par(mar=c(4,4,2,1), cex = 1.2)
ev <- mite.spe.h.pca$CA$eig
n <- length(ev)
barplot(ev, main = "Eigenvalues",
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright",
       "Average eigenvalue",
       lwd = 2, col = "red3",
       bty = "n")
```
]

---
# Défi #2: Solution

```{r, echo = -1, fig.height=6.5, fig.width=7}
par(mar = c(4,4,0.05,0.05), cex = 1.5)
biplot(mite.spe.h.pca,
       col = c("red3", "grey15"))
```

---
# Analyse des coordonnées principales

La **ACoP** (*PCoA*, en anglais) est similaire dans son esprit à l'ACP, mais elle prend des *dissimilarités* comme données d'entrée !

Elle vise à représenter fidèlement les distances avec l'espace dimensionnel le plus bas possible.

Elle commence par le (i) calcul d'une matrice de distance pour les $p$ éléments, puis (ii) le centrage de la matrice par lignes et colonnes, et enfin, la (iii) *decomposition* de la matrice de distance centrée en éléments propres.

--

Pour calculer une ACoP, nous pouvons utiliser les fonctions `cmdscale()` ou `pcoa()` des paquets `stats` et `ape` :

```{r}
library(ape)
spe.h.pcoa <- pcoa(dist(spe.hel))
summary(spe.h.pcoa)
```

---

# Analyse des coordonnées principales

```{r}
head(spe.h.pcoa$values)
```

---

# Analyse des coordonnées principales

Nous pouvons également voir les *vecteurs propres* associés à chaque *valeur propre* contenant les coordonnées dans l'espace euclidien pour chaque site.


```{r}
head(spe.h.pcoa$vectors)[, 1:5]
```

---

# Analyse des coordonnées principales : `biplot.pcoa()`

Nous pouvons afficher les distances entre les sites en utilisant la fonction `biplot.pcoa()`, ainsi que représenter les espèces associées à chaque site.

```{r, fig.height=5.5, fig.width=8}
biplot.pcoa(spe.h.pcoa, spe.hel)
```

---

##### Analyse des coordonnées principales : distances non métriques

La ACoP peut également être utilisée pour capturer les informations contenues dans les distances non métriques, telles que la populaire distance de Bray-Curtis. Essayons :


.pull-left[
```{r}
spe.bray.pcoa <- pcoa(spe.db.pa)
```

```{r}
spe.bray.pcoa$values$Eigenvalues
```
]

.pull-right[
Notez les valeurs propres négatives !

Cela est dû au fait que les distances non métriques ne peuvent pas être représentées dans l'espace euclidien sans corrections (*voir* Legendre & Legendre 2012 pour plus de détails à ce sujet) :

```{r}
spe.bray.pcoa <- pcoa(spe.db.pa,
                      correction = "cailliez")
```
]

---
##### Analyse des coordonnées principales : distances non métriques

.pull-left[

Les valeurs propres corrigées sont maintenant sur une nouvelle colonne !

```{r}
spe.bray.pcoa$values$Corr_eig
```

]

.pull-right[
Utilisez un biplot sans les espèces pour la représenter !

```{r, fig.width=6, fig.height=5.5, echo = -1}
par(mar=c(3,3,.5,1), cex = 1.2)
biplot.pcoa(spe.bray.pcoa)
```

]

---
# Défi #3 ![:cube]()

Calculez une ACoP sur les données d'abondance des espèces d'acariens transformées par Hellinger.

Soyez prêt à répondre :

- Quels sont les *vecteurs propres* et les *valeurs propres* significatifs ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?
- Comment les résultats de la ACoP se comparent-ils à ceux de l'ACP ?

---
# Défi #3: Solution

- Transformation de Hellinger des données sur les espèces
```{r}
mite.spe <- mite
mite.spe.hel <- decostand(mite.spe, method = "hellinger")
```

- Calcul de la ACoP

```{r}
mite.spe.h.pcoa <- pcoa(dist(mite.spe.hel))
```

---
# Défi #3: Solution

- Construisez un biplot pour visualiser les données :
```{r, fig.width=6, fig.height=6}
biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel)
```


---
# Positionnement multidimensionnel non-métrique

- Dans l'ACP et la ACoP, les objets sont ordonnés selon un petit nombre de dimensions (généralement > 2) ;

- Les biplots 2D peuvent ne pas représenter toute la variation au sein de l'ensemble de données ;

- Parfois, nous cherchons à représenter les données dans un plus petit nombre de dimensions spécifié ;

- Comment pouvons-nous tracer l'espace d'ordination pour représenter la plus grande variation possible dans les données ?

--

Nous pouvons essayer d'utiliser le *positionnement multidimensionnel non-métrique*!

* Le PMnM (en anglais, *nMDS* de *non-metric multidimensional scaling*) est la contrepartie non-métrique de ACoP ;
* Il utilise un algorithme d'optimisation itératif pour trouver la meilleure représentation des distances dans un espace réduit ;

---
# Positionnement multidimensionnel non-métrique

- Le PMnM (ou nMDS) applique une procédure itérative qui tente de positionner les objets dans le nombre de dimensions demandé de manière à minimiser une fonction de contrainte (échelonnée de $0$ à $1$), qui mesure la qualité de l'ajustement de la distance dans la configuration de l'espace réduit.

- Par conséquent, plus la valeur de stress est faible, meilleure est la représentation des objets dans l'espace d'ordination.

- nMDS est implémenté dans `vegan` comme `metaMDS()` où :
  - `distance` spécifie la métrique de distance à utiliser ;
  - `k` spécifie le nombre de dimensions.

```{r, eval = FALSE, echo = TRUE}
spe.nmds <- metaMDS(spe, distance = 'bray', k = 2)
```

```{r include=FALSE}
spe.nmds <- metaMDS(spe, distance = 'bray', k = 2)
```

---
# Positionnement multidimensionnel non-métrique: *qualité de l'ajustement*

Le diagramme de *Shepard* et les valeurs de contraintes peuvent être obtenus avec `stressplot()` :

```{r, eval = FALSE}
spe.nmds$stress # [1] 0.07376229
stressplot(spe.nmds, main = "Shepard plot")
```

.pull-left[
```{r, echo = FALSE, warning=FALSE, message=FALSE, purl = FALSE}
stressplot(spe.nmds, main = "Shepard plot")
```
]
.pull-right[
<Br><Br>
Le graphique de Shepard identifie une forte corrélation entre la dissimilarité observée et la distance d'ordination $(R^2 > 0.95)$, ce qui met en évidence la qualité élevée de l'ajustement du PMnM.

]

---
# Positionnement multidimensionnel non-métrique : `biplot()`

Construire un biplot

```{r, eval = F}
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```

---
##### Positionnement multidimensionnel non-métrique : `biplot()`

.pull-left[

Le biplot du PMnM montre un groupe de sites fermés caractérisés par les espèces BLA, TRU, VAI, LOC, CHA et OMB, tandis que les autres espèces forment un groupe de sites dans la partie supérieure droite du graphique.

Quatre sites dans la partie inférieure du graphique sont fortement différents des autres.
]

.pull-right[
```{r echo=FALSE}
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```
]

---
# Défi #4 ![:cube]()

<br>

Exécutez le PMnM de l'abondance des espèces d'acariens `mite.spe` en 2 dimensions basé sur une distance de Bray-Curtis.

Évaluez la qualité de l'ajustement de l'ordination et interprétez le biplot.

Rappelez-vous de ces fonctions utiles:

```{r, eval = F}
?metaMDS
?stressplot
```

---
# Défi #4: Solution

Exécutez le PMnM de l'abondance des espèces d'acariens `mite.spe` en 2 dimensions basé sur une distance de Bray-Curtis.

```{r, results='hide'}
mite.nmds <- metaMDS(mite.spe, distance = 'bray', k = 2)
```

---
# Défi #4: Solution

```{r, eval = F}
plot(mite.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(mite.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")
points(scores(mite.nmds, display = "sites",
              choices = c(1,2)),
              pch = 21,
              col = "black",
              bg = "steelblue",
              cex = 1.2)
text(scores(mite.nmds, display = "species", choices = c(1)),
     scores(mite.nmds, display = "species", choices = c(2)),
     labels = rownames(scores(mite.nmds, display = "species")),
     col = "red", cex = 0.8)
```

---
# Défi #4: Solution

```{r, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 7}
plot(mite.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(mite.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")
points(scores(mite.nmds, display = "sites",
              choices = c(1,2)),
              pch = 21,
              col = "black",
              bg = "steelblue",
              cex = 1.2)
text(scores(mite.nmds, display = "species", choices = c(1)),
     scores(mite.nmds, display = "species", choices = c(2)),
     labels = rownames(scores(mite.nmds, display = "species")),
     col = "red", cex = 0.8)
```

Il n'y a pas vraiment de regroupement évident de sites dans le biplot NMDS. Cela nous indique que les espèces sont présentes dans la plupart des sites. Seuls quelques sites contiennent des communautés spécifiques.

---
# Défi #4: Solution

```{r}
stressplot(mite.nmds)
```

La dissimilarité observée est fortement corrélée avec la distance d'ordination, et la valeur de stress NMDS est relativement faible, ce qui nous indique que l'ordination NMDS est relativement précise.

---
# Conclusion

.alert[De nombreuses techniques d'ordination existent, mais leur spécificité doit guider vos choix sur les méthodes à utiliser.]

| | Distance préservée | Variables | Nombre maximal d'axes |
|---|---------|--------------|------|
|PCA| Euclidienne | Données quantitatives, relations linéaires | p|
|CA| Chi2 | Non négatif, données quantitatives homogènes, données binaires | p-1 |
|PCoA| Définie par l'utilisateur | Données quantitatives, semi-quantitatives, mixtes| p-1|
|NMDS| Définie par l'utilisateur | Données quantitatives, semi-quantitatives et mixtes | Définie par l'utilisateur |

---
# C'est l'heure du quiz !

.alert[Que signifie ACP?]

--

Analyse en composantes principales

--

.alert[Laquelle de ces méthodes est la meilleure pour visualiser les *distances* entre composition des communautés de différents sites?]

--

Analyse en coordonnées principales (ACoP)

--

.alert[Que représente une valeur propre dans une ACP ?]

--

La proportion de variance capturée par une composante principale

---
# C'est l'heure du quiz!

Trouvez l'erreur dans cette figure!

![:scale 90%](images/Chall7.png)

--
.alert[
- Les données ne sont pas centrées. Beurk!
]

--

.alert[
- Les deux premiers axes capturent 100% de la variation.
]

---
class: inverse, center, bottom

# Merci pour votre participation à cet atelier!

![:scale 50%](images/qcbs_logo.png)
