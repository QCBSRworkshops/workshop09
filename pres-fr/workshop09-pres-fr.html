<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Atelier 9: Analyses multivariées</title>
    <meta charset="utf-8" />
    <meta name="author" content="Centre de la Science de la Biodiversité du Québec" />
    <link href="assets/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script src="assets/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="assets/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="assets/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fas fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"<i class=\"fa fa-times-circle\" style=\"color: #F94144\"><\/i>"})</script>
    <link href="assets/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
    <link href="assets/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
    <link rel="stylesheet" href="qcbsR.css" type="text/css" />
    <link rel="stylesheet" href="qcbsR-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Atelier 9: Analyses multivariées
## Série d’ateliers R
### Centre de la Science de la Biodiversité du Québec

---

class: inverse, center, middle





# À propos de cet atelier

[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=repo&amp;message=dev&amp;color=6f42c1&amp;logo=github)](https://github.com/QCBSRworkshops/workshop09)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=wiki&amp;message=09&amp;logo=wikipedia)](https://wiki.qcbs.ca/r_workshop9)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=09&amp;color=red&amp;logo=html5)](https://qcbsrworkshops.github.io/workshop09/pres-fr/workshop09-pres-fr.html)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=09&amp;color=red&amp;logo=adobe-acrobat-reader)](https://qcbsrworkshops.github.io/workshop09/pres-fr/workshop09-pres-fr.pdf)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=script&amp;message=09&amp;color=2a50b8&amp;logo=r)](https://qcbsrworkshops.github.io/workshop09/book-fr/workshop09-script-fr.R)


---

&lt;p style="font-size:75%"&gt;

.center[
**Contributeurs et contributrices au développement de cet atelier**

en modifiant et en améliorant son contenu dans le cadre du &lt;br&gt;Prix d'apprentissage et du développement (*Le*arning *a*nd *D*evelopment *A*ward*).
]

.pull-left[
.right[

 **2022** - **2021** - **2020**

[Pedro Henrique P. Braga]()

[Katherine Hébert]()

[Mi Lin]()

[Linley Sherin]()

]
]

.pull-right[
.left[
**2019** - **2018** - **2017**

[Gabriel Muñoz](https://github.com/fgabriel1891)

[Marie Hélène-Brice]()

[Pedro Henrique P. Braga]()

&lt;br&gt;

**2016** - **2015** - **2014**

[Bérenger Bourgeois]()

[Xavier Giroux-Bougard]()

[Amanda Winegardner]()

[Emmanuelle Chrétien]()

[Monica Granados]()

]
]
&lt;/p&gt;

---
&lt;p style="font-size:75%"&gt;

.center[
**Contributeurs et contributrices au développement de cet atelier**

en modifiant et en améliorant son contenu dans le cadre du &lt;br&gt; Prix d'apprentissage et de développement (*Le*arning *a*nd *D*evelopment *A*ward)
]


.pull-left[
.right[

 **2022** - **2021** - **2020**



]
]

.pull-right[
.left[
**2019** - **2018** - **2017**



**2016** - **2015** - **2014**



]

]
&lt;/p&gt;

---

# Matériel requis

Pour suivre cet atelier, vous devez avoir téléchargé et installé les dernières versions de [RStudio](https://rstudio.com/products/rstudio/download/#download) et de [R](https://cran.rstudio.com/).

.pull-left[
Vous devez également avoir installé ces paquets:
* [ape](https://cran.r-project.org/package=ape)
* [gclus](https://cran.r-project.org/package=gclus)
* [vegan](https://cran.r-project.org/package=vegan)
* [GGally](https://cran.r-project.org/package=GGally)
* [PlaneGeometry](https://cran.r-project.org/package=PlaneGeometry)
* [remotes](https://cran.r-project.org/package=remotes)

]

.pull-right[


Pour les installer à partir de CRAN, roulez:


```r
install.packages(c("ape", 
                   "gclus", 
                   "vegan",
                   "GGally", 
                   "PlaneGeometry", 
                   "remotes"))
```
]


&lt;br&gt;

.pull-left2[
Au cours de cet atelier, il y aura une série de **défis** que vous pouvez reconnaître par ce cube de Rubix.

**Pendant ces défis, n'hésitez pas à collaborer!**
]

.pull-right2[

.center[
![:scale 45%](images/rubicub.png)

]
]

---
# Objectifs d'apprentissage

**Apprendre les bases de l'analyse multivariée pour découvrir des patrons dans les données de composition des communautés.

**2.** Utiliser `R` pour effectuer une ordination sans contrainte.

**3.** Choisir les mesures de distance et les transformations appropriées pour effectuer une analyse multivariée.

* Analyse de regroupement
* Analyse en composantes principales (ACP)
* Analyse en coordonnées principales (ACoP)
* Échelle multidimensionnelle non métrique (PMnM).

**4.** Utilisez `R` pour créer un dendrogramme.

---

class: inverse, center, middle

# 1. Introduction
## Qu'est-ce que l'ordination?

---
# Une dimension

Imaginons que nous voulons mieux comprendre cette réponse pour différentes espèces d'algues impliquées dans la densité de l'efflorescence algale ?

.center[![:scale 70%](images/algalBloom.png)]


---
# Deux dimensions

.center[![:scale 70%](images/2dim.png)]

---
# Trois dimensions

.center[
![:scale 70%](images/3dim.png)]


---
# 4, 5, 6, ou plus de dimensions

.center[![:scale 70%](images/4dim.png)]

&lt;br&gt;

.center[ **?** ]

???

En écologie, plusieurs descripteurs sont généralement observés pour chaque objet étudié. Dans la plupart des cas, les écologistes sont intéressés par la caractérisation des principales tendances de variation des objets par rapport à tous les descripteurs, et pas seulement à quelques-uns d'entre eux. L'examen des diagrammes de dispersion des objets par rapport à toutes les paires possibles de descripteurs est une approche fastidieuse, qui n'apporte généralement pas beaucoup de lumière sur le problème à résoudre.
Même avec une approche multivariée, il n'est pas possible de dessiner un tel diagramme sur un papier à plus de trois dimensions, même s'il s'agit d'une construction mathématique parfaitement valable.

---

# Ordination en espace réduit

.center[![:scale 70%](images/Ord1.png)]

- L'ordination est la représentation d'objets (sites, stations, relevés, etc.) sous forme de données ponctuelles le long d'un ou plusieurs axes de référence.

- L'ordination répond à la problématique de la projection de la dispersion des objets écologiques, qui sont situés dans l'espace multidimensionnel des descripteurs, dans un plus petit nombre de dimensions. 

- Espace réduit : espace dont la dimensionnalité est réduite par rapport à l'ensemble des données d'origine.

???

Ref: https://doi.org/10.1016/B978-0-444-53868-0.50009-5

---

# Ordination en espace réduit


.center[![:scale 70%](images/Ord3.png)]

- Pour les besoins d'analyses, les écologistes projettent la dispersion multidimensionnelle de l'espace multidimensionnel en un nombre plus restreint de dimensions de descripteurs. 

- Les axes représentent une grande fraction de la variabilité de la matrice de données multidimensionnelle.

???
Proposition de scénario : Pour les besoins de leur étude, les écologistes projettent généralement le diagramme de dispersion multidimensionnel sur des graphiques bivariés dont les axes sont connus pour leur intérêt particulier. Les axes de ces graphiques sont choisis pour représenter une grande partie de la variabilité de la matrice de données multidimensionnelle, dans un espace de dimensionnalité réduite (c'est-à-dire plus faible) par rapport à l'ensemble de données original. Les méthodes d'ordination dans un espace réduit permettent également d'obtenir des informations quantitatives sur la qualité des projections, et d'étudier les relations entre les descripteurs ainsi qu'entre les objets.

---
exclude: true

# Méthodes pour la recherche scientifique

- **Questions / Hypothèses**
- **Design expérimental**
- **Collecte de données**
- **Transformation / Distance**
- **Analyses**
- **Rédaction**
- **Communication**

---
class: inverse, center, middle
# 2. Exploration des données

---
# Données de poissons de la rivière Doubs

.pull-left[

Données de Verneaux (1973) :
- caractérisation des communautés de poissons
- 27 espèces
- 30 sites
- 11 variables environnementales

]

.pull.right[
![:scale 50%](images/DoubsRiver.png)
]

---
# Données de poissons de la rivière Doubs

Chargement des données espèces (`doubsspe.csv`)


```r
spe &lt;- read.csv("data/doubsspe.csv", row.names = 1) # cette ligne dépend d'où vous avez enregistré les données
spe &lt;-  spe[-8,] # supprimer le site sans données

# pour des plus grands jeux de données:
row_sub = apply(spe, 1, function(row) any(row !=0 ))
# trouver les lignes où il n'y a aucun zéro
spe &lt;- spe[row_sub,]
# garder les lignes qui n'ont aucun zéro  
```

Chargement des données environnementales (`doubsenv.csv`)


```r
env &lt;- read.csv("data/doubsenv.csv", row.names = 1)
env &lt;- env[-8,] # supprimer le site vide
```

.alert[Attention, n'exécuter qu'une seule fois]


---
# Exporation des données

Explorer le contenu des données espèces :


```r
names(spe) # noms des objets
dim(spe) # dimensions
str(spe) # structure des objets
summary(spe) # résumé statistique
head(spe) # 6 premières lignes
```


```
#   CHA TRU VAI LOC OMB BLA HOT TOX VAN CHE BAR SPI GOU BRO PER BOU PSO ROT CAR
# 1   0   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
# 2   0   5   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
# 3   0   5   5   5   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
# 4   0   4   5   5   0   0   0   0   0   1   0   0   1   2   2   0   0   0   0
# 5   0   2   3   2   0   0   0   0   5   2   0   0   2   4   4   0   0   2   0
# 6   0   3   4   5   0   0   0   0   1   2   0   0   1   1   1   0   0   0   0
#   TAN BCO PCH GRE GAR BBO ABL ANG
# 1   0   0   0   0   0   0   0   0
# 2   0   0   0   0   0   0   0   0
# 3   0   0   0   0   0   0   0   0
# 4   1   0   0   0   0   0   0   0
# 5   3   0   0   0   5   0   0   0
# 6   2   0   0   0   1   0   0   0
```

---
# Fréquences des espèces

Observer la distribution de fréquence des espèces :


```r
ab &lt;- table(unlist(spe))
barplot(ab, las = 1, col = grey(5:0/5),
        xlab = "Abondance des classes", ylab = "Fréquence")
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /&gt;

.alert[Notez la proportion de 0]

---
# Fréquences des espèces

Combien de zéros?


```r
sum(spe == 0)
# [1] 408
```

Quelle proportion de zéros?


```r
sum(spe == 0)/(nrow(spe)*ncol(spe))
# [1] 0.5210728
```

---
# Richesse totale en espèce

Observer le nombre d'espèces présentes dans chaque site :


```r
site.pre &lt;- rowSums(spe &gt; 0)
barplot(site.pre, main = "Richesse spécifique",
        xlab = "Sites", ylab = "Nombre d'espèces",
        col = "grey ", las = 1)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-9-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# Comprenez vos données!

.center[...pour choisir la transformation et la distance appropriée]


- Y-a-t-il beaucoup de zéros?

- Que veulent-ils dire?


.alert[Une mesure de 0 (e.g 0mg/L, 0°C) n'est pas équivalent à un 0 représentant une absence d'observation.]


---
# Avant de transformer vos données de composition des communautés...

.alert[Considérations importantes:]

--
- abondances/comptes/présence-absence relatives?

--
- distributions asymmétriques ?

--
- beaucoup d'espèces rares?

--
- surabondance d'espèces dominantes?

--
- problème de double Zéro?

---
# Transformer les données de composition des communautés

.center[
![](images/trans1.png)]

---
# Transformer les données de composition des communautés

## Exemples

Transformer des comptes en présence - absence

```r
library(vegan)
spec.pa &lt;- decostand(spe, method = "pa")
```

Réduire le poids des espèces rares


```r
spec.hel &lt;- decostand(spe, method = "hellinger")
spec.chi &lt;- decostand(spe, method = "chi.square")
```

Réduire le poids des espèces abondantes


```r
spe.pa &lt;- decostand(spe, method = "log")
```

---
# Données sur l'environnement


```r
names(env) # Names of objects
dim(env) # dimensions
str(env) # structure of objects
summary(env) # summary statistics
head(env) # first 6 rows
```


```r
head(env) # first 6 rows
#    das alt  pen  deb  pH dur  pho  nit  amm  oxy dbo
# 1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7
# 2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9
# 3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5
# 4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3
# 5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2
# 6 32.4 846  3.2 2.86 7.9  60 0.20 0.15 0.00 10.2 5.3
```

Explorer la colinéarité en visualisant les corrélations entre les variables:


```r
library(GGally)
ggpairs(env, main="Bivariate Plots of the Environmental Data")
```

---
# Données sur l'environnement

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-16-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# Standardisation

Standardiser les variables environnementales est indispensable car il est impossible de comparer des variables d'unités différentes :


```r
## ?decostand
env.z &lt;- decostand(env, method = "standardize")
```

&lt;Br&gt;
Cette fonction centre-réduit les données pour assurer la fiabilité des analyses:


```r
apply(env.z, 2, mean)
#           das           alt           pen           deb            pH 
# -7.959539e-17 -4.795165e-17  2.494600e-17 -7.323225e-17 -1.730430e-15 
#           dur           pho           nit           amm           oxy 
# -2.028505e-16  4.445790e-17  2.875893e-17  2.754434e-17 -4.038167e-16 
#           dbo 
#  9.829975e-17
apply(env.z, 2, sd)
# das alt pen deb  pH dur pho nit amm oxy dbo 
#   1   1   1   1   1   1   1   1   1   1   1
```



---
class: inverse, center, middle
# 3. Similarité / Dissimilarité


---
# Mesure d'association

L'algébre matricielle est au coeur de plusieurs méthodes d'analyses multivariées

$$
`\begin{bmatrix}
    a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; \dots  &amp; a_{1,n} \\
    a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; \dots  &amp; a_{2,n} \\
    a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; \dots  &amp; a_{3,n} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m,1} &amp; a_{m,2} &amp; a_{m,3} &amp; \dots  &amp; a_{m,n}
\end{bmatrix}`
$$

- Explorer différentes mesures de distance entre objets permet de mieux comprendre le fonctionnement de l'ordination

???
.center[![](images/MatrixAlgebra.png)]
---
# Au-délà de la 1ère dimension

.pull-left[

- Les jeux de données écologiques correspondent souvent à de grandes matrices.

- L'ordination calcule les relations entre espèces, ou entre objets.

- Ces relations peuvent être simplifiées par des mesures de dissimilarités.
]

.pull-right[

![:scale 40%](images/PCAMatrix.png)

![:scale 40%](images/distMes.png)

![:scale 40%](images/distMat.png)
]


---
# Similarité / Dissimilarité

- Utile pour comprendre vos données
- Certains types d'ordination ou de groupement nécessitent des mesures appropriées

.center[
Similarité: S = 1 - D
Distance: D = 1-S]


![](images/similarity.png)

---
# Mesures de distance des communautés

.pull-left[
- **Euclidienne**  
  `\(d_{jk} = \sqrt(\sum((x_{ij}-x_{ik})^2))\)`  
  Utilisé en PCA et RDA.
  &lt;br&gt;&lt;br&gt;
- **Manhattan**
  `\(d_{jk} = \sum(abs(x_{ij} - x_{ik}))\)`  
  Similaire à l'Euclidienne, mais sensible au problème de doubles zéros.
  &lt;br&gt;&lt;br&gt;
- **Corde**  
  Distance Euclidienne calculée pour une matrix standardisée par rangées.
  &lt;br&gt;&lt;br&gt;
]

.pull-right[
- **Hellinger**  
  Racines carrées de probabilités conditionnelles.  
  S'exécute bien en ordination linéaire.
  &lt;br&gt;&lt;br&gt;
- **Khi-carré**  
  `\(d_{jk} = \sum(\frac{(x_{ij}-x_{ik})^2}{(x_{ij}+x_{ik})} ) / 2\)`   
  Utilisé en CA.
  &lt;br&gt;&lt;br&gt;
- **Bray-Curtis** (Semimétrique) 
  `\(d_{jk} = \frac{\sum(abs(x_{ij}-x_{ik}))}{(\sum (x_{ij}+x_{ik}))}\)`  
  Utilisé en PCoA et NMDS.
]
&lt;br&gt;
.center[

*Où `\(x_{ij}\)` `\(x_{ik}\)` sont le nombre d'espèces (colonnes) `\(i\)` dans les sites (rangées) `\(j\)` et `\(k\)`.*]

--

.alert[Chacune de ces distances peut être appropriée pour différentes situations.]

???
Notes: Ref: 
https://sites.google.com/site/mb3gustame/reference/dissimilarity
Euclidienne : Une métrique simple et symétrique utilisant la formule de Pythagore. Plus il y a de variables dans un ensemble de données, plus on peut s'attendre à ce que les distances euclidiennes soient grandes. En plus, les doubles zéros entraînent une diminution des distances. Cette propriété rend la distance euclidienne inadaptée à de nombreux ensembles de données écologiques et des transformations à motivation écologique doivent être envisagées.  L'analyse en composantes principales et l'analyse de la redondance ordonnent les objets en utilisant les distances euclidiennes.

---
# Comparaison des sites de la rivière Doubs

La fonction `vegdist()` comprend les mesures de distances communes :


```r
?vegdist
```
Comment la composition des communautés diffère-t-elle entre les 30 sites de la rivière Doubs?


```r
spe.db.pa &lt;- vegdist(spe, method = "bray")
spe.db &lt;- as.matrix(spe.db.pa)

#         1         2         3         4         5         6         7         9
# 1  0.0000000 0.6000000 0.6842105 0.7500000 0.8918919 0.7500000 0.6842105 1.0000000
# 2  0.6000000 0.0000000 0.1428571 0.3333333 0.6956522 0.3939394 0.1428571 0.6923077
# 3  0.6842105 0.1428571 0.0000000 0.1891892 0.6800000 0.2972973 0.1250000 0.7333333
#...
```

--
&lt;br&gt;  
- La diagonale est composée de zéros (chaque site est comparée à lui-même).  
- Sites 3 et 7 sont les plus similaires (*plus petite distance*).  
- Sites 1 et 9 sont les plus dissimilaires (1 = *complètement* dissimilaire).

---
exclude: true

# Comparaison des sites

.center[![](images/Doubs1.png)]

---
exclude: true 

# Comparaison des sites

.center[![](images/Doubs2.png)]

---
# Visualisation d'une matrice de distances





```r
# Le code pour créer la fonction coldiss() est dans le script de l'atelier.
coldiss(spe.db.pa)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-22-1.png" width="720" style="display: block; margin: auto;" /&gt;


---
# Défi #1 ![:cube]()

&lt;br/&gt;

Discuter en groupes:

&lt;br/&gt;

.center[**Comment savoir si deux objets caractérisés par des données multidimensionnelles sont similaires?**]

&lt;br/&gt;

- Faites une liste de vos suggestions


---
# Et qu'en est-il de l'ordination?

Avec des méthodes d'ordination, nous ordonnons vos objets (sites) en fonction de leur similarité

- Plus les sites sont similaires, plus ils sont proches dans l'espace d'ordination (plus petites distances)

- En écologie, on calcule habituellement la similarité entre sites en fonction de leur composition en espèces ou de leur conditions environnementales.


---
# Analyse schématique des analyses multivariées

.center[![:scale 120%](images/Schema2.png)]

???
Note: re-added the picture, rename it as Schema 2.

---
# Groupement

- Permet de mettre en lumière des structures dans les données en partitionnant les objets

- Les résultats sont représentés sous forme de dendrogramme (arbre)

- Pas une méthode statistique!

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-23-1.png" width="720" style="display: block; margin: auto;" /&gt;

???
.center[
![:scale 80%](images/cluster1.png)]

---
# Aperçu de 3 méthodes hiérarchiques

&lt;br&gt;

**1.** Groupement agglomératif à liens simples ("single").

&lt;br&gt;

**2.** Liens complets, groupement agglomératif ("complete")

&lt;br&gt;

**3.** Groupement à variance minimale de Ward ("ward.D2")

&lt;br&gt;

Les éléments de rang(s) inférieur(s) sont imbriqués dans des clusters de rang supérieur. Par exemple, les espèces sont regroupées par genre, famille, ordre, et ainsi de suite.

???
Dans le clustering agglomératif à liens simples (également appelé tri par plus proches voisins), les objets à l'emplacement de l'objet sont classés par ordre de priorité et les objets les plus proches s'agglomèrent.
Ceci génère souvent de longues et fines grappes ou chaînes d'objets. À l'inverse, dans le clustering agglomératif à liens complets, un objet ne s'agglomère à un groupe que lorsqu'il est lié à l'élément le plus éloigné du groupe, qui le lie à son tour à tous les membres de ce groupe. Il formera de nombreux petits groupes séparés, et est plus approprié pour rechercher des contrastes, des discontinuités dans les données.
Le clustering à variance minimale de Ward diffère de ces deux méthodes en ce que
elle regroupe les objets en groupes en utilisant le critère des moindres carrés
(similaire aux modèles linéaires). Son dendogramme montre par défaut les distances au carré. Pour comparer avec les autres méthodes, calculez d'abord la racine carrée des distances. 

---
# Groupement hiérarchique

À partir d'une matrice de distances, on classe les objets en ordre croissant

![](images/Hierachic1.png)


---
# Groupement à liens simples

.pull-left[

![:scale 50%](images/singleClust1.png)
--

]

.pull-right[

- Les deux objets les plus proches se regroupent

- Ensuite les deux objets les plus proches suivants

- et ainsi de suite.

![](images/singleClust2.png)

]

---
# Groupement à liens complet

.pull-left[

![:scale 50%](images/compleClust1.png)

]

.pull-right[

- Les deux objets les plus proches se regroupent

- Ensuite les groupes se lient à la distance à laquelle les objets qu'ils contiennent sont tous liés

![](images/compleClust2.png)
]


---
# Comparaison

Créer une matrice de distance à partir des données de la rivière Doubs transformées Hellinger et faire le groupement à liens simples :


```r
spe.dhe1 &lt;- vegdist(spec.hel, method = "euclidean")
spe.dhe1.single &lt;- hclust(spe.dhe1, method = "single")
plot(spe.dhe1.single)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-24-1.png" width="504" style="display: block; margin: auto;" /&gt;

---
# Comparaison


```r
spe.dhe1 &lt;- vegdist(spec.hel, method = "euclidean")
spe.dhe1.complete &lt;- hclust(spe.dhe1, method = "complete")
plot(spe.dhe1.single, main="Single linkage clustering", hang =-1)
plot(spe.dhe1.complete, main="Complete linkage clustering", hang=-1)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-25-1.png" width="1080" style="display: block; margin: auto;" /&gt;

.pull-left[

**Liens simples :**

Les objets ont tendance à s'enchaîner (e.g. 19,29,30,26).
]

.pull-right[

**Liens complets :**  

Les groupes sont plus distincts.
]

---
# Groupement de Ward

Le groupement de Ward utilise la méthode des moindres carrés pour lier les objets.
- Les groupes fusionnent de façon à minimiser la variance intragroupe.
- C'est-à-dire qu'à chaque étape, la paire de groupes qui résulte à la plus petite augmentation de la somme des carrés des écarts intra-groupes est celle qui fusionne.


---
# Groupement de Ward

Faire le groupement de Ward et dessiner le dendrogramme en utilisant la racine carrée des distances :


```r
spe.dhel.ward &lt;- hclust(spe.dhe1, method = "ward.D2")
spe.dhel.ward$height &lt;- sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang = -1) # hang = -1 aligns objects at the same level
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-26-1.png" width="648" style="display: block; margin: auto;" /&gt;

---
# Groupement de Ward

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-27-1.png" width="648" style="display: block; margin: auto;" /&gt;

Les objets ont tendance à former des groupes plus homogènes (en termes du nombdre d'objets inclus dans chaque groupe).

---
# Comment choisir la bonne méthode ?

- Choisir une méthode appropriée dépend de votre objectif.
  - Démontrer des gradients? des contrastes?
- Si plus d'une méthode semble adéquate, comparer les dendrogrammes.
- Encore une fois : ceci **n'est pas** une méthode statistique! 

Mais, le groupement permet de:
- Déterminer un nombre de groupes optimal.
- Faire des tests statistiques sur le groupement résultant.
- Combiner le groupement à l'ordination pour distinguer des groupes de sites.

---

#### Et maintenant ?

Alors que l'**analyse de regroupements** remonte les *discontinuités* dans un ensemble de données, l'**ordination** extrait les principales tendances sous la forme d'axes continus.

Nous allons examiner maintenant trois types de **méthodes d'ordination sans contrainte**...

--

.right[...**attendez**, qu'est-ce qu'on entend par **ordination sans contrainte** ? *Quelqu'un* ?]

--
&lt;br&gt;
.center[*Si personne ne s'exprime, choisissez une personne "volontaire" !*]

--

**Les ordinations non contraintes** évaluent les relations chez un seul ensemble de variables. *Aucune tentative* n'est faite pour définir la relation entre un ensemble de variables indépendantes et une ou plusieurs variables dépendantes.

--

L'interprétation des effets potentiels d'autres facteurs sur les patrons observés ne peut se faire qu'indirectement, car ces facteurs ne sont *pas* explicitement inclus dans les analyses !

--

Ici, nous allons explorer :
.small[
.center[
.pull-left2[
**A**nalyse des **C**omposants **P**rincipales 

**A**nalyse des **Co**ordonnées **P**rincipales
]

.pull-right2[
**É**chelonnement **M**ultidimensionnel **N**on-**M**étrique
]
]
]


---
### Mais d'abord, faisons un *récapitulatif*...

Nous avons déjà compris la signification de **variance** et **moyenne**, et comment les calculer :


.pull-left[
`$$\sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$`
]

.pull-right[
`$$\mu_x = \frac{1}{n} \sum_{i=i}^{n} x_{i}$$`
]

Elles sont très utiles pour comprendre le *centre* et la *dispersion* d'une variable ou d'une dimension donnée.

Néanmoins, nous sommes souvent intéressés **par plus d'une dimension**, et nous voulons mesurer dans quelle mesure chaque dimension varie de la moyenne *par rapport aux autres*.

--

La **covariance** est une telle mesure, qui peut décrire comment **deux dimensions co-varient** :

.pull-left[
`$$var_x = \sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$`
]

.pull-right[
`$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$`
]

---

### Mais d'abord, faisons un *récapitulatif*...

Intuitivement, nous pouvons mesurer la **covariance** entre plus de deux variables. Disons, entre les variables `\(x\)`, `\(y\)`, et `\(z\)` :

.pull-left[
`$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$`
]

.pull-right[
`$$cov_{x,z}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(z_{i}-\bar{z})}{N-1}$$`
]

`$$cov_{z,y}=\frac{\sum_{i=1}^{N}(z_{i}-\bar{z})(y_{i}-\bar{y})}{N-1}$$`
Nous pouvons représenter ces calculs dans une **matrice de covariance** :

`$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; cov_{y,y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; cov_{z,z}   
\end{array} \right]$$`

--

**QUIZ TIME**: *Que sont les diagonales ?* *Et, que se passe-t-il si les variables sont indépendantes ?*

---

##### Toujours en train de *récapituler*...

.center[***Que sont les diagonales ?***]

Si, `\(cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}\)`, alors:

`$$cov_{x,x}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(x_{i}-\bar{x})}{N} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2} {N} = var_x$$`
--

Afin que :

`$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; cov_{y,y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; cov_{z,z}   
\end{array} \right] = \left[ \begin{array}{ccc} 
var_{x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; var_{y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; var_{z}   
\end{array} \right]$$`

 &lt;br&gt;
 
.center[La covariance d'une variable avec elle-même est sa *variance* !]

---

##### Toujours en train de *récapituler*...

.center[***Que se passe-t-il si les variables sont indépendantes ?***]

.pull-left[

```r
x &lt;- rnorm(5000, mean = 0, sd = 1)
y &lt;- rnorm(5000, mean = 0, sd = 1)
z &lt;- rnorm(5000, mean = 0, sd = 1)

xyz &lt;- data.frame(x, y, z)

GGally::ggpairs(xyz) 
```

&lt;img src="workshop09-pres-fr_files/figure-html/xyz-norm-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

```r
cov(xyz)
```


```
#          x        y        z
# x  1.00384  0.01152 -0.00182
# y  0.01152  0.98187 -0.01086
# z -0.00182 -0.01086  0.96051
```

Si les variables sont parfaitement indépendantes (ou non corrélées), la matrice de covariance `\(C(x, y, z)\)` est :

`$${C(x, y, z)} =  \left[ \begin{array}{ccc} 
var_{x} &amp; 0 &amp; 0 \\
0 &amp; var_{y} &amp; 0 \\
0 &amp; 0 &amp; var_{z}   
\end{array} \right]$$`

*i*.*e*. une covariance plus proche de `\(1\)` signifie que les variables sont *colinéaires*.

Et ici, `\(var_{x} = var_{y} = var_{z} = 1\)`.

]

---
## Transformations linéaires

Nous sommes souvent intéressés à l'observation des variables d'autres *formes*. 

Pour cela, nous créons une nouvelle variable, disons `\(x_{new}\)`, en utilisant des constantes pour modifier la variable originale `\(x\)`. Par exemple :

--

.pull-left[
On peut transformer des distances mesurées en kilomètres `\(d_{km}\)` en miles :
`$$d_{mi} = 0.62 \times d_{km}$$`
]

.pull-right[
On peut aussi transformer des degrés Fahrenheit en degrés Celsius : 

`$$T_{C} = 0.5556\times T_{Fahr} - 17.778$$`
]

Ces exemples sont des **transformations linéaires** car les variables transformées sont liées linéairement aux variables d'origine et les formes de la distribution ne sont pas modifiées.

--

Deux types de transformations sont très importantes pour nous :

.pull-left[
**Centrage**, qui soustrait les valeurs d'un prédicteur de la moyenne :

`$$x' = x_i - \bar{x}$$`
]

.pull-right[
**Réduction**, qui divise chaque valeur d'une variable par son écart-type:

`$$x'' = \frac{x_i}{\sigma_x}$$`
]

??? 

Le centrage est essentiellement une technique où la moyenne des variables indépendantes est soustraite de toutes les valeurs. Cela signifie que toutes les variables indépendantes ont une moyenne nulle. La réduction est complementaire au centrage. Les variables prédictives sont divisées par leur écart type.

Le présentateur doit mentionner ici que le centrage amène la moyenne à zéro et la mise à l'échelle amène l'écart type à une unité. Ils doivent également mentionner que les variables deviennent comparables lors de la mise à l'échelle, car leur unité est perdue.

---

### Décomposition des matrices en éléments propres

**Les matrices carrées**, telles que la **matrice de covariance**, peuvent être décomposées en *valeurs propres* et en *vecteurs propres*.

Pour une matrice carrée, `\(A_{n \times n}\)`, un vecteur `\(v\)` est un *vecteur propre* de `\(A\)` s'il y a un *scalaire*, `\(\lambda\)`, pour lequel :

.center[ 
`\(A_{n \times n} v_{n \times 1} = \lambda  v_{n \times 1}\)`, or `\(\left(\begin{matrix}a_{11}&amp;\cdots&amp;a_{1n}\\\vdots&amp;\ddots&amp;\vdots\\a_{1n}&amp;\cdots&amp;a_{nn}\\\end{matrix}\right)\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)=\lambda\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)\)` 
]

la valeur de `\(\lambda\)` étant la *valeur propre* correspondante.

--

C'est-à-dire que la matrice `\(A\)` *étire* effectivement le vecteur propre `\(v\)` par la quantité spécifiée par la valeur propre (*scalaire*) `\(\lambda\)`.

Un *vecteur propre* est un vecteur dont la direction reste inchangée lorsqu'on lui applique une **transformation linéaire**.

&lt;br&gt;

.center[Attendez ! Qu'entendons-nous par *direction inchangée* ?]

---

### Décomposition des matrices en éléments propres

.center[Attendez ! Qu'entendons-nous par *direction inchangée* ?]

&lt;br&gt;

Représentons cela avec cet exemple simple.

Nous pouvons transformer un carré en un parallélogramme à l'aide d'une **transformation affine** à axe unique.

--

.pull-left[
Soit `\(S\)` le carré de vertices `\((0,0),\,(1,0),\,(1,1),\,(1,0)\)` qui sera transformé par cisaillement en parallélogramme `\(P\)` de vertices `\((0,0),\,(1,0),\,(1,1.57),\,(1,0.57)\)`.

Nous pouvons voir qu'après la transformation linéaire, la flèche violette n'a pas changé de direction, c'est-à-dire qu'elle est un *vecteur propre* de `\(S\)`.

En revanche, la flèche rouge a changé de direction, et *n'est donc pas* un *vecteur propre* de `\(S\)`.
]

.pull-right[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-28-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---

##### Décomposition des matrices en éléments propres : implications

.center[
*Suivons avec la torture algébrique !* 
]

### Orthogonalité

Une propriété *fabuleuse* et *simple* des matrices *symétriques* que nous pouvons expliquer ici !

.pull-left[
Supposons que `\(x\)` soit un vecteur propre de `\(A\)` correspondant à la valeur propre `\(λ_1\)` et `\(y\)` un vecteur propre de `\(A\)` correspondant à la valeur propre `\(λ_2\)`, avec `\(λ_1≠λ_2\)`.

`$$Ax=\lambda_1x \\
Ay=\lambda_2y$$`

Multiplions chacun d'eux par l'autre *vecteur propre* transposé.
`$$y^{\intercal}Ax=\lambda_1y^{\intercal}x \\ x^{\intercal}A^{\intercal}y=\lambda_2x^{\intercal}y$$`

]

--

.pull-right[

Maintenant, soustrayons la deuxième équation de la première et utilisons la commutativité du produit scalaire :

`\(y^{\intercal}Ax-x^{\intercal}A^{\intercal}y=\lambda_1y^{\intercal}x - \lambda_2x^{\intercal}y \\ 0 = (\lambda_1 - \lambda_2)y^{\intercal}x\)`

Parce que nous savons que `\(\lambda_1-\lambda_2\neq 0\)`, alors 
`\(y^{\intercal}x = 0\)`, *c-*.*à*.*-d*., `\(\mathbf{x}\perp\mathbf{y}\)`, *c-*.*à*.*-d*. sont **orthogonaux** !

*Alors, que révèle la décomposition de la matrice de variance-covariance en elements propres ?*
]

???

L'explication de cette partie est très utile et assez simple, afin que chacun puisse comprendre ce qu'est l'orthogonalité. Il s'agit d'opérations simples d'équations et de soustractions.  

---
##### Décomposition des matrices en éléments propres : implications

.center[
*Suivons avec la torture algébrique !* 
]

### Maximisation

.pull-left[
Si `\(v_i' v_i = 1\)`, alors `\(Av_i=\lambda_iv_i\)` peut être écrite comme :
`$$v_i' A v_i = \lambda_i$$`
En effet, `\(v' A v\)` 
est la variance d'une combinaison linéaire avec des poids en `\(v\)`, *i*.*e*. `\(\text{Var}(v_i'\,A)=v_i'\,\text{Var}(A)\,v_i\)`.

*Donc, on peut relier les points !*

]

--

.pull-right[
Rappelez-vous que les *valeurs propres* de notre *matrice de variance-covariance* `\(A\)` sont directement liées à la variance !

Pour trouver un vecteur `\(v\)` qui maximise la variance, `\(v' A v\)`, il suffit de choisir le *vecteur propre* correspondant à la plus grande *valeur propre* `\(\lambda_i\)` !

De sorte que la variance maximale soit de `\(\lambda_1\)` !
]

--

La *variance expliquée* de chaque *vecteur propre* obéit à l'ordre : `\(\lambda_1 &gt; \lambda_2 &gt; \dots &gt; \lambda_k\)`.

Cela nous permet de condenser un plus grand nombre de variables originales en un ensemble plus petit de vecteurs sélectionnés avec une perte minimale d'informations (c'est-à-dire **une réduction de la dimensionnalité**).

---
# Méthodes d'ordination sans contrainte

Ceci est un bon point de départ pour nous mettre sur la voie des **méthodes d'ordination sans contrainte** que nous allons étudier aujourd'hui !

.pull-left[
Ils nous permettent de :
- Évaluer les relations *dans* un ensemble de variables (espèces ou variables environnementales);
- Trouver les composantes clés de la variation entre les échantillons, les sites, les espèces ;
- Réduire le nombre de dimensions dans les données multivariées tout en limitant la perte substantielle d'informations ;
- Créer de nouvelles variables à utiliser dans des analyses ultérieures.
]

.pull-right[
Ici, nous apprendrons :

1. **P**rincipal **C**omponent **A**nalysis;

2. **P**rincipal **Co**ordinate **A**nalysis;

3. **N**on-Metric **M**ulti**d**imensional **S**caling;
]
---
# Analyse en Composantes Principales

.small[
L'analyse en composantes principales (ACP) est une technique de réduction de la dimensionnalité *linéaire*, c'est-à-dire qu'elle réduit les données fortement corrélées.

En bref, l'ACP transforme *linéairement* des variables originales vers des nouvelles variables contenant des **composantes principales**, qui expliquent la plupart de la variance dans l'ensemble de données, c'est-à-dire qui maximisent la séparation entre les données.
]

--
.pull-left[

.small[L'espace des *composants principaux* peut être écrit comme suit :]

`$$Z_p = ∑_{j=1}^p ϕ_j * X_j$$`
]
.small[
où, 
1. `\(Z_p\)` est la composante principale `\(p\)` ;
2. `\(ϕ_j\)` est le vecteur de charge comprenant les `\(j\)` charges pour la composante principale `\(p\)`, c'est-à-dire les coefficients de la combinaison linéaire des variables originales à partir desquelles les composantes principales sont construites ;
3. `\(X_j\)` est le prédicteur normalisé, c'est-à-dire avec une moyenne égale à zéro et un écart-type égal à un.
]

???

C'est-à-dire que la reconstruction des données peut être donnée par une simple combinaison linéaire des composantes.


Les futures versions de ce document devraient inclure une figure (telle qu'une carte thermique) avec les données normalisées originales sur le côté gauche, et sur le côté droit, une carte thermique des chargements fois une carte thermique des composants (qui peut être considérée comme égale à la somme des matrices de rang un) étant égale à la carte thermique des données reconstruites.

---
# Analyse en Composantes Principales

L'ACP peut être calculée d'au moins *quatre* façons différentes.

Pour des raisons de simplicité, nous nous concentrerons ici sur la façon d'obtenir des composantes principales à partir d'une matrice de corrélation.

Nous apprendrons à le faire à partir de rien, puis à utiliser les paquets `R` pour calculer les composantes principales.

---

### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;
]

.pull-right3[
Dans `R`, à partir de zéro !


```r
data(varechem)

str(varechem)
# 'data.frame':	24 obs. of  14 variables:
#  $ N       : num  19.8 13.4 20.2 20.6 23.8 22.8 26.6 24.2 29.8 28.1 ...
#  $ P       : num  42.1 39.1 67.7 60.8 54.5 40.9 36.7 31 73.5 40.5 ...
#  $ K       : num  140 167 207 234 181 ...
#  $ Ca      : num  519 357 973 834 777 ...
#  $ Mg      : num  90 70.7 209.1 127.2 125.8 ...
#  $ S       : num  32.3 35.2 58.1 40.7 39.5 40.8 33.8 27.1 42.5 60.2 ...
#  $ Al      : num  39 88.1 138 15.4 24.2 ...
#  $ Fe      : num  40.9 39 35.4 4.4 3 ...
#  $ Mn      : num  58.1 52.4 32.1 132 50.1 ...
#  $ Zn      : num  4.5 5.4 16.8 10.7 6.6 9.1 7.4 5.2 9.3 9.1 ...
#  $ Mo      : num  0.3 0.3 0.8 0.2 0.3 0.4 0.3 0.3 0.3 0.5 ...
#  $ Baresoil: num  43.9 23.6 21.2 18.7 46 40.5 23 29.8 17.6 29.9 ...
#  $ Humdepth: num  2.2 2.2 2 2.9 3 3.8 2.8 2 3 2.2 ...
#  $ pH      : num  2.7 2.8 3 2.8 2.7 2.7 2.8 2.8 2.8 2.8 ...
```

]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;
]

.pull-right3[
Dans `R`, à partir de zéro !


```r
data(varechem)

# Étape 1 
Y &lt;- varechem[, 1:2]

head(Y)
#       N    P
# 18 19.8 42.1
# 15 13.4 39.1
# 24 20.2 67.7
# 27 20.6 60.8
# 23 23.8 54.5
# 19 22.8 40.9
```

]

---
#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;

2. Normalisation des observations, comme dans `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)` ; ce qui revient à centrer, comme dans `\(y_c = [y_i - \bar{y}]\)`, puis à mettre à l'échelle, comme dans `\(y_s = \frac{y_i}{\sigma_y}\)` ;
]

.pull-right3[
Dans `R`, à partir de zéro !


```r
data(varechem)

# Étape 1 
Y &lt;- varechem[, 1:2]

# Étape 2
Y_std &lt;- as.matrix(scale(Y))

head(Y_std)
#              N          P
# 18 -0.46731082 -0.1993234
# 15 -1.62503567 -0.4000407
# 24 -0.39495301  1.5134640
# 27 -0.32259521  1.0518143
# 23  0.25626722  0.6303080
# 19  0.07537271 -0.2796103

round(apply(Y_std, 2, mean))
# N P 
# 0 0
round(apply(Y_std, 2, sd))
# N P 
# 1 1
```

]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;

2. Normalisation des observations, comme dans `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)` ; ce qui revient à centrer, comme dans `\(y_c = [y_i - \bar{y}]\)`, puis à mettre à l'échelle, comme dans `\(y_s = \frac{y_i}{\sigma_y}\)` ;

3. Calculer la matrice de variance-covariance `\(R = cov(Y_{std})\)` ;
]

.pull-right3[
Dans `R`, à partir de zéro !


```r
data(varechem)

# Étape 1 
Y &lt;- varechem[, 1:2]

# Étape 2
Y_std &lt;- as.matrix(scale(Y))

# Étape 3
(Y_R &lt;- cov(Y_std))
#            N          P
# N  1.0000000 -0.2511603
# P -0.2511603  1.0000000
```
]

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;

2. Normalisation des observations, comme dans `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)` ; ce qui revient à centrer, comme dans `\(y_c = [y_i - \bar{y}]\)`, puis à mettre à l'échelle, comme dans `\(y_s = \frac{y_i}{\sigma_y}\)` ;

3. Calculer la matrice de variance-covariance `\(R = cov(Y_{std})\)` ;

4. Effectuer la décomposition de la matrice de covariance pour obtenir la matrice `\(U\)` des vecteurs propres, contenant les *composantes principales* ;
]

.pull-right3[
Dans `R`, à partir de zéro !


```r
data(varechem)

# Étape 1
Y &lt;- varechem[, 1:2]

# Étape 2
Y_std &lt;- as.matrix(scale(Y))

# Étape 3
Y_R &lt;- cov(Y_std)

# Étape 4
(Eigenvalues &lt;- eigen(Y_R)$values)
# [1] 1.2511603 0.7488397

(Eigenvectors &lt;- eigen(Y_R)$vectors)
#            [,1]       [,2]
# [1,] -0.7071068 -0.7071068
# [2,]  0.7071068 -0.7071068
```
]

---

### Analyse en composantes principales : étape par étape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, à chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[

Nous pouvons représenter les distances des observations au premier vecteur propre (`PC1`, en rouge).

La première composante principale est dessinée de façon à ce que la variation des valeurs le long de sa ligne soit maximale. 

Les flèches sur les composantes principales sont obtenues en multipliant leurs *valeurs propres* par les *vecteurs propres*.
]

.pull-right[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-34-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

???

Dans cette première représentation, nous pouvons observer qu'une première direction (ou la première composante linéaire) est tracée en essayant de maximiser la variance des données.

Les participants peuvent vous demander en quoi cela est différent d'une régression linéaire. L'une des principales différences réside dans la façon dont les carrés d'erreurs sont minimisés perpendiculairement à la ligne droite (90 degrés, ce qui la rend orthogonale), alors que dans la régression linéaire, les carrés d'erreurs sont minimisés dans la direction des ordonnées.

---

### Analyse en composantes principales : étape par étape

Les *vecteurs propres* sont ici les **composantes principales**, et comme nous l'avons vu, à chaque *vecteur propre* correspond une *valeur propre*.

.pull-left[
Nous pouvons alors représenter les distances des observations au deuxième vecteur propre (`PC2`, en orange).

La deuxième composante principale est également dessinée en maximisant la variance des données.

Notez comment les composantes principales sont orthogonales !
]

.pull-right[

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-35-1.png" width="360" style="display: block; margin: auto;" /&gt;

]

--
.pull-left[
*Nous avons représenté les vecteurs propres, c'est-à-dire les composantes principales !* 

*Mais, quelle est l'utilité des valeurs propres ?*
]

???

Ici, la deuxième direction (ou la deuxième composante linéaire) est tracée de manière à ce que la variance des données soit maximisée par rapport à cette deuxième composante.

---

#### Analyse en composantes principales : étape par étape

Nous avons vu que les *valeurs propres* représentent la magnitude (la variance) des composantes principales.

.pull-left[
En fait, la somme de toutes les *valeurs propres* est égale à la somme des variances, qui sont représentées sur la diagonale de la matrice de variance-covariance.
]

.pull-right[

```r
sum(diag(cov(Y_std)))
# [1] 2
sum(eigen(cov(Y_std))$values)
# [1] 2
```
]

--

Intuitivement, on peut obtenir l'influence relative de chaque *vecteur propre* `\(v_{k}\)` (ou `\(\text{PC}_{k}\)`) en divisant leurs valeurs par la somme de toutes les *valeurs propres*.

`$$\text{Variance expliquée de}~v_{k} = \frac{\lambda_{v_k}}{\sum^p_{i=1}{\lambda_{v}}}$$`

En faisant cela, nous pouvons dire que le `\(\text{PC}1\)` explique 63% de la variance dans les données, tandis que `\(\text{PC}2\)` explique 37% de la variance.

--

Enfin, nous pouvons procéder à la dernière étape de notre analyse des composantes principales !

---

#### Analyse en composantes principales : étape par étape

.pull-left3[
1. Point de départ : une matrice `\(Y\)` de `\(n\)` observations et `\(p\)` variables continues normalement distribuées ;

2. Normalisation des observations, comme dans `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)` ; ce qui revient à centrer, comme dans `\(y_c = [y_i - \bar{y}]\)`, puis à mettre à l'échelle, comme dans `\(y_s = \frac{y_i}{\sigma_y}\)` ;

3. Calculer la matrice de variance-covariance `\(R = cov(Y_{std})\)` ;

4. Effectuer la décomposition de la matrice de covariance pour obtenir la matrice `\(U\)` des vecteurs propres, contenant les *composantes principales* ;

5. Obtenir la *matrice des coordonnées* `\(F\)` en multipliant `\(U\)` par la matrice normalisée `\(Y_{std}\)`.
]

.pull-right3[
En `R`, à partir de zéro !


```r
# Étape 1 
Y &lt;- varechem[, 1:2]

# Étape 2
Y_std &lt;- as.matrix(scale(Y))

# Étape 3
Y_R &lt;- cov(Y_std)

# Étape 4 
Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

# Étape 5
F_PrComps &lt;- Y_std %*% Eigenvectors
head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

---

### Analyse en composantes principales : étape par étape

La *matrice des coordonnées*, `\(F\)`, (objet `F_PrComps`) permet de *rotationner* le nouvel espace de données, afin qu'il soit représenté par rapport aux composantes principales.

.pull-left[
.center[ 
`\(\text{N}\)` ~ `\(\text{P}\)` 
]

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-38-1.png" width="252" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.center[ 
`\(\text{PC}1\)` ~ `\(\text{PC}2\)` 
]

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-39-1.png" width="252" style="display: block; margin: auto;" /&gt;
]

???
Les titres des axes ne se font pas imprimer. Je les ai inclus en haut de chaque graphique, mais ce problème n'est pas résolu.

Le présentateur ou la présentatrice devrait souligner la rotation et parler de ce que sont les points. Vous pouvez survoler les points et leur montrer quelle était la position des points dans les graphiques " sans rotation ", et maintenant dans les graphiques " avec rotation ", en soulignant que maintenant, les " nouveaux axes " sont PC1 et PC2.

Cette compréhension sera utile lorsque les participants utiliseront les fonctions PCA implémentées dans R.

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
&lt;br&gt;
`stats::prcomp()`?

```r
PCA_prcomp &lt;- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp &lt;- prcomp(Y_std)

head(PCA_prcomp$x)
#           PC1        PC2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
# 19  0.2510109 -0.1444178
```
]

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
&lt;br&gt;
`stats::princomp()` ?

```r
PCA_princomp &lt;- princomp(Y_std)

head(PCA_princomp$scores)
#        Comp.1     Comp.2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
# 19  0.2510109 -0.1444178
```
]

---

### Analyse en composantes principales : étape par étape

L'ACP peut aussi être calculée en utilisant les fonctions `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, et `ade4::dudi.pca()`.

.pull-left[
.right[Comment notre ACP faite maison se compare à]


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
&lt;br&gt;
`vegan::rda()` ?

```r
PCA_vegan_rda &lt;- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
#           PC1        PC2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
```

`vegan::rda()` utilise des mises à l'échelle alternatives. Vous pouvez étudier la `vignette("decision-vegan")`.
]

???

Dites aux participants que le nom `rda` fait référence à un type différent de technique d'ordination sous contrainte, mais que si nous exécutons `rda()` avec une seule variable, il exécutera une ACP.

---

# Analyse en composantes principales

Nous avons implémenté l'ACP sur un ensemble de données à deux variables pour plus de simplicité.

Avançons et appliquons-la à notre jeu de données sur les espèces de poissons.


Pour cela, nous allons utiliser la fonction `vegan::rda()` sur les données de poissons *transformées par Hellinger* et résumer les résultats :


```r
spe.h.pca &lt;- rda(spec.hel)

# summary(spe.h.pca)
```

---

# Analyse en composantes principales

.pull-left[
Les premières lignes de `summary.rda()` nous renseignent sur la *variance totale* et la *variance sans contrainte* de notre modèle.
]

.pull-right[


```
# [1] "Partitioning of variance:"        "              Inertia Proportion"
# [3] "Total          0.5025          1" "Unconstrained  0.5025          1"
```
]

--

.pull-left2[


```
# [1] "Importance of components:"                                                     
# [2] "                         PC1     PC2     PC3     PC4     PC5     PC6     PC7"  
# [3] "Eigenvalue            0.2580 0.06424 0.04632 0.03850 0.02197 0.01675 0.01472"  
# [4] "Proportion Explained  0.5133 0.12784 0.09218 0.07662 0.04371 0.03334 0.02930"  
# [5] "Cumulative Proportion 0.5133 0.64118 0.73337 0.80999 0.85370 0.88704 0.91634"  
# [6] "                          PC14     PC15     PC16      PC17      PC18      PC19"
# [7] "Eigenvalue            0.001835 0.001455 0.001118 0.0008309 0.0005415 0.0004755"
# [8] "Proportion Explained  0.003651 0.002895 0.002225 0.0016535 0.0010776 0.0009463"
# [9] "Cumulative Proportion 0.988888 0.991783 0.994008 0.9956612 0.9967389 0.9976852"
```

]

.pull-right2[

Viennent ensuite les *valeurs propres*, et leur contribution à la variance.


En fait, si nous additionnons toutes nos *valeurs propres*, nous obtiendrons la quantité de variance sans contrainte expliquée par l'analyse !


```r
sum(spe.h.pca$CA$eig)
# [1] 0.5025103
```

]

???

Puisque nous n'avons pas contraint notre ordination, la variance proportionnelle non contrainte est égale à la variance totale.

Prenez un moment pour expliquer la proportion expliquée, et montrez que la proportion cumulée sera égale à 1 au 27ème PC.

---

# Analyse en composantes principales

Les informations suivantes sont liées à la *mise à l'échelle* (`scaling`), aux *scores d'espèces*, et aux *scores de sites*.


```
#  [1] "Eigenvalue            0.0003680 0.0002765 0.0002253 0.0001429 7.618e-05"
#  [2] "Proportion Explained  0.0007324 0.0005503 0.0004483 0.0002845 1.516e-04"
#  [3] "Cumulative Proportion 0.9984176 0.9989678 0.9994161 0.9997006 9.999e-01"
#  [4] "                          PC25      PC26      PC27"                     
#  [5] "Proportion Explained  9.93e-05 3.036e-05 1.814e-05"                     
#  [6] "Cumulative Proportion 1.00e+00 1.000e+00 1.000e+00"                     
#  [7] "Scaling 2 for species and site scores"                                  
#  [8] "* Species are scaled proportional to eigenvalues"                       
#  [9] "* Sites are unscaled: weighted dispersion equal on all dimensions"      
# [10] "* General scaling constant of scores:  1.93676 "                        
# [11] ""                                                                       
# [12] ""                                                                       
# [13] "Species scores"                                                         
# [14] "BCO -0.20174  0.08807 -0.067086 -0.0529106  0.0737228  0.037312"        
# [15] "PCH -0.14717  0.05829 -0.067311 -0.0458414  0.0501013  0.031605"        
# [16] "GAR -0.35245 -0.14076  0.168014  0.0185946  0.0213462 -0.129788"        
# [17] "BBO -0.24317  0.03679 -0.082731 -0.0384489  0.0939828  0.063369"        
# [18] "ABL -0.42536 -0.26155 -0.054190  0.1021959 -0.0078085  0.044540"        
# [19] "ANG -0.20631  0.11889 -0.062079 -0.0175733  0.0718743 -0.001956"        
# [20] ""                                                                       
# [21] ""                                                                       
# [22] "Site scores (weighted sums of species scores)"
```
]

---

# Analyse en composantes principales

.pull-left[
`Species` fait référence à vos descripteurs (c'est-à-dire les colonnes de votre ensemble de données), qui sont ici les espèces de poissons.

`Scores` font référence à la position de chaque espèce le long des composantes principales.
]

.pull-right[

```
# [1] "Cumulative Proportion 1.00e+00 1.000e+00 1.000e+00"               
# [2] "Scaling 2 for species and site scores"                            
# [3] "* Species are scaled proportional to eigenvalues"                 
# [4] "* Sites are unscaled: weighted dispersion equal on all dimensions"
# [5] "* General scaling constant of scores:  1.93676 "                  
# [6] ""                                                                 
# [7] ""                                                                 
# [8] "Species scores"
```
]

--


.pull-left2[

```
# [1] "PCH -0.14717  0.05829 -0.067311 -0.0458414  0.0501013  0.031605"
# [2] "GAR -0.35245 -0.14076  0.168014  0.0185946  0.0213462 -0.129788"
# [3] "BBO -0.24317  0.03679 -0.082731 -0.0384489  0.0939828  0.063369"
# [4] "ABL -0.42536 -0.26155 -0.054190  0.1021959 -0.0078085  0.044540"
# [5] "ANG -0.20631  0.11889 -0.062079 -0.0175733  0.0718743 -0.001956"
# [6] ""                                                               
# [7] ""                                                               
# [8] "Site scores (weighted sums of species scores)"
```
]

.pull-right2[
`Sites` représentent les lignes de votre jeu de données, qui sont ici les différents sites le long de la rivière *Doubs*.
]

--

&lt;br&gt;

.pull-left[
Cette information peut être obtenue avec la fonction `score()`:
]

.pull-right[


```r
scores(spe.h.pca,
       display = "species" or "sites")
```

]

---
### Analyse en composantes principales : condensation des données

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

--

#### Critère de Kaiser-Guttman

.pull-left[
Nous pouvons sélectionner les composantes principales qui capturent plus de variance que l'explication moyenne de toutes les composantes principales. Nous le faisons en

1. Extraire les *valeurs propres* associées aux composantes principales ;

2. Sélectionner les *valeurs propres* au-dessus de la *valeur propre* moyenne :


```r
ev &lt;- spe.h.pca$CA$eig
# ev[ev &gt; mean(ev)]
```
]

.pull-right[

```r
n &lt;- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-55-1.png" width="720" style="display: block; margin: auto;" /&gt;
]

---
### Analyse en composantes principales : condensation des données

Nous avons 27 composantes principales. Cependant, nous pouvons appliquer des algorithmes pour sélectionner le plus petit nombre de composantes principales qui rendent encore compte d'une grande variance dans les données.

#### Modèle *broken-stick*

.pull-left[
Le modèle *broken-stick* retient les composantes qui expliquent plus de variance que ce qui serait attendu en divisant aléatoirement la variance en `\(p\)` parties.


```r
head(bstick(spe.h.pca))
#        PC1        PC2        PC3        PC4        PC5        PC6 
# 0.07242581 0.05381432 0.04450858 0.03830475 0.03365187 0.02992957
```
]

.pull-right[

```r
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-57-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

---
## Analyse en composantes principales 

Il ne reste plus qu'à discuter de la *mise à l'échelle* et à *visualiser* nos résultats.

Pratiquons et calculons une ACP sur les variables environnementales standardisées pour le même ensemble de données.


```r
env.pca &lt;- rda(env.z)
# summary(env.pca, scaling  = 2)
```

--

Déterminons notre ensemble de *valeurs propres* et leurs *vecteurs propres* correspondants :

.pull-left[

```r
ev &lt;- env.pca$CA$eig
```


```r
ev[ev&gt;mean(ev)]
#      PC1      PC2      PC3 
# 6.097995 2.167126 1.037603
```
]

--

.pull-right[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-61-1.png" width="576" style="display: block; margin: auto;" /&gt;
]

---

# Analyse en composantes principales : `plot()`

L'information calculée par l'ACP peut être représentée par des *biplots*.

Nous pouvons produire un biplot rapide de l'ACP en utilisant la fonction `plot()` en base `R`.


```r
plot(spe.h.pca)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-62-1.png" width="360" style="display: block; margin: auto;" /&gt;


---

# Analyse en composantes principales : `plot()`


`biplot()` de `base` `R` permet une meilleure interprétation.

.pull-left2[

```r
biplot(spe.h.pca)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-63-1.png" width="468" style="display: block; margin: auto;" /&gt;
]

.pull-right2[

Les flèches sont tracées pour montrer la directionnalité et l'angle des descripteurs dans l'ordination.

Les descripteurs situés à 180 degrés les uns des autres sont corrélés négativement ;

Les descripteurs situés à 90 degrés l'un de l'autre ont une corrélation nulle ;

Les descripteurs situés à 0 degré l'un de l'autre sont positivement corrélés.

]

---
# Analyse en composantes principales : *mise à l'échelle*

.small[
*Mise à l'échelle de type 2* (`scaling = 2`, défaut) : les distances entre les objets ne sont pas des approximations des distances euclidiennes ; les angles entre les vecteurs des descripteurs (espèces) reflètent leurs corrélations.
]

.small[
*Mise à l'échelle de type 1* (`scaling = 1`) : tente de préserver la distance euclidienne (dans un espace multidimensionnel) entre les objets (sites) : les angles entre les vecteurs des descripteurs (espèces) ne sont pas pertinents.
]

.pull-left[

```r
biplot(spe.h.pca, scaling = 1)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-64-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
biplot(spe.h.pca, scaling = 2)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-65-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

???
2 : **Meilleur pour l'interprétation des relations entre les descripteurs (espèces)**.


1 : **Meilleur pour l'interprétation des relations entre les objets (sites)!**

---
# Défi # 3 ![:cube]()

En utilisant tout ce que vous avez appris, calculez une ACP sur les données d'abondance des espèces d'acariens.


```r
data(mite)
```

Soyez prêt à discuter et à répondre :
- Quelles sont les composantes principales *les plus pertinentes*, c'est-à-dire les sous-ensembles ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?

---

# Solution #3

Calculer l'ACP sur les données d'espèces transformées par Hellinger.


```r
mite.spe.hel &lt;- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca &lt;- rda(mite.spe.hel)
```

--

.pull-left[
Appliquer le critère de Kaiser-Guttman


```r
ev &lt;- mite.spe.h.pca$CA$eig
ev[ev&gt;mean(ev)]
n &lt;- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, 
       col = "red3", bty = "n")
```
]

.pull-right[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-69-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
# Solution #3


```r
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-70-1.png" width="504" style="display: block; margin: auto;" /&gt;

---
# Analyse des coordonnées principales

La **ACoP** (*PCoA*, en anglais) est similaire dans son esprit à l'ACP, mais elle prend des *dissimilarités* comme données d'entrée ! 

Elle vise à représenter fidèlement les distances avec l'espace dimensionnel le plus bas possible.

Elle commence par le (i) calcul d'une matrice de distance pour les `\(p\)` éléments, puis (ii) le centrage de la matrice par lignes et colonnes, et enfin, la (iii) *decomposition* de la matrice de distance centrée en éléments propres.

--

Pour calculer une ACoP, nous pouvons utiliser les fonctions `cmdscale()` ou `pcoa()` des paquets `stats` et `ape` :


```r
library(ape)
spe.h.pcoa &lt;- pcoa(dist(spec.hel))
summary(spe.h.pcoa)
#            Length Class      Mode     
# correction   2    -none-     character
# note         1    -none-     character
# values       5    data.frame list     
# vectors    783    -none-     numeric  
# trace        1    -none-     numeric
```

---

# Analyse des coordonnées principales


```r
head(spe.h.pcoa$values)
#   Eigenvalues Relative_eig Broken_stick Cumul_eig Cumul_br_stick
# 1   7.2228939   0.51334374   0.14412803 0.5133437      0.1441280
# 2   1.7987449   0.12783995   0.10709099 0.6411837      0.2512190
# 3   1.2970423   0.09218307   0.08857247 0.7333668      0.3397915
# 4   1.0780684   0.07662021   0.07622679 0.8099870      0.4160183
# 5   0.6150273   0.04371107   0.06696753 0.8536980      0.4829858
# 6   0.4691296   0.03334186   0.05956013 0.8870399      0.5425459
```

---

# Analyse des coordonnées principales

Nous pouvons également voir les *vecteurs propres* associés à chaque *valeur propre* contenant les coordonnées dans l'espace euclidien pour chaque site.



```r
head(spe.h.pcoa$vectors)[, 1:5]
#         Axis.1      Axis.2      Axis.3     Axis.4       Axis.5
# 1 -0.509824403 -0.27654372  0.64011383 -0.3393734  0.207330880
# 2 -0.698794880 -0.03935586  0.11324989 -0.2328859 -0.157730682
# 3 -0.640690642  0.01566707  0.03835044 -0.2669706 -0.125293094
# 4 -0.413985947  0.10477084 -0.15728486 -0.2851828 -0.001250382
# 5  0.003083242  0.05284310 -0.32206098 -0.2730693  0.315944703
# 6 -0.295314224  0.05778805 -0.32395301 -0.2262902  0.056493824
```

---

# Analyse des coordonnées principales : `biplot.pcoa()`

Nous pouvons afficher les distances entre les sites en utilisant la fonction `biplot.pcoa()`, ainsi que représenter les espèces associées à chaque site.


```r
biplot.pcoa(spe.h.pcoa, spec.hel)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-74-1.png" width="576" style="display: block; margin: auto;" /&gt;

---

##### Analyse des coordonnées principales : distances non métriques

La ACoP peut également être utilisée pour capturer les informations contenues dans les distances non métriques, telles que la populaire distance de Bray-Curtis. Essayons :


.pull-left[

```r
spe.bray.pcoa &lt;- pcoa(spe.db.pa)
```


```r
spe.bray.pcoa$values$Eigenvalues
#  [1]  3.695331e+00  1.098472e+00  7.104740e-01  4.149729e-01  3.045604e-01
#  [6]  1.917884e-01  1.569703e-01  1.319099e-01  1.294251e-01  8.667896e-02
# [11]  4.615780e-02  3.864487e-02  2.745800e-02  1.306508e-02  7.087896e-03
# [16]  4.039469e-03  1.300594e-03  0.000000e+00 -3.534426e-05 -3.940676e-03
# [21] -8.956051e-03 -1.461149e-02 -1.598905e-02 -2.145686e-02 -3.017013e-02
# [26] -3.432671e-02 -3.760052e-02 -6.037087e-02 -6.880061e-02
```
]

.pull-right[
Notez les valeurs propres négatives ! 

Cela est dû au fait que les distances non métriques ne peuvent pas être représentées dans l'espace euclidien sans corrections (*voir* Legendre &amp; Legendre 2012 pour plus de détails à ce sujet) :


```r
spe.bray.pcoa &lt;- pcoa(spe.db.pa, 
                      correction = "cailliez")
```
]

---
##### Analyse des coordonnées principales : distances non métriques

.pull-left[

Les valeurs propres corrigées sont maintenant sur une nouvelle colonne !


```r
spe.bray.pcoa$values$Corr_eig
#  [1] 5.20461437 1.60465006 1.09152082 0.68985417 0.52129425 0.38710929
#  [7] 0.36447367 0.29671255 0.27559544 0.21600584 0.15419396 0.15378333
# [13] 0.11812808 0.08848541 0.07304055 0.06999353 0.05712927 0.05587583
# [19] 0.05432215 0.04912221 0.04100207 0.03777775 0.03451234 0.02959507
# [25] 0.02436729 0.01902747 0.00284371 0.00000000 0.00000000
```

]

.pull-right[
Utilisez un biplot sans les espèces pour la représenter !


```r
biplot.pcoa(spe.bray.pcoa)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-79-1.png" width="432" style="display: block; margin: auto;" /&gt;

]

---
# Défi #5 ![:cube]()

Calculez une ACoP sur les données d'abondance des espèces d'acariens transformées par Hellinger.

Soyez prêt à répondre :

- Quels sont les *vecteurs propres* et les *valeurs propres* significatifs ?
- Quels groupes de sites pouvez-vous identifier ?
- Quels groupes d'espèces sont liés à ces groupes de sites ?
- Comment les résultats de la ACoP se comparent-ils à ceux de l'ACP ?

---
# Solution #5

- Transformation de Hellinger des données sur les espèces

```r
mite.spe &lt;- mite
mite.spe.hel &lt;- decostand(mite.spe, method = "hellinger")
```

- Calcul de la ACoP


```r
mite.spe.h.pcoa &lt;- pcoa(dist(mite.spe.hel))
```

---
# Solution #5

- Construisez un biplot pour visualiser les données :

```r
biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel)
```

&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-82-1.png" width="432" style="display: block; margin: auto;" /&gt;


---
# Positionnement multidimensionnel non-métrique

- Dans l'ACP et la ACoP, les objets sont ordonnés selon un petit nombre de dimensions (généralement &gt; 2) ;

- Les biplots 2D peuvent ne pas représenter toute la variation au sein de l'ensemble de données ;

- Parfois, nous cherchons à représenter les données dans un plus petit nombre de dimensions spécifié ;

- Comment pouvons-nous tracer l'espace d'ordination pour représenter la plus grande variation possible dans les données ?

--

Nous pouvons essayer d'utiliser le *positionnement multidimensionnel non-métrique*!

* Le PMnM (en anglais, *nMDS* de *non-metric multidimensional scaling*) est la contrepartie non-métrique de ACoP ;
* Il utilise un algorithme d'optimisation itératif pour trouver la meilleure représentation des distances dans un espace réduit ;

---
# Positionnement multidimensionnel non-métrique

- Le PMnM (ou nMDS) applique une procédure itérative qui tente de positionner les objets dans le nombre de dimensions demandé de manière à minimiser une fonction de contrainte (échelonnée de `\(0\)` à `\(1\)`), qui mesure la qualité de l'ajustement de la distance dans la configuration de l'espace réduit.

- Par conséquent, plus la valeur de stress est faible, meilleure est la représentation des objets dans l'espace d'ordination.

- nMDS est implémenté dans `vegan` comme `metaMDS()` où :
  - `distance` spécifie la métrique de distance à utiliser ;
  - `k` spécifie le nombre de dimensions.


```r
spe.nmds &lt;- metaMDS(spe, distance = 'bray', k = 2)
```



---
# Positionnement multidimensionnel non-métrique: *qualité de l'ajustement*

Le diagramme de *Shepard* et les valeurs de contraintes peuvent être obtenus avec `stressplot()` :


```r
spe.nmds$stress # [1] 0.07376229
stressplot(spe.nmds, main = "Shepard plot")
```

.pull-left[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-86-1.png" width="360" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;Br&gt;&lt;Br&gt;
Le graphique de Shepard identifie une forte corrélation entre la dissimilarité observée et la distance d'ordination `\((R^2 &gt; 0.95)\)`, ce qui met en évidence la qualité élevée de l'ajustement du PMnM.

]

---
# Positionnement multidimensionnel non-métrique : `biplot()`

Construire un biplot


```r
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```

---
##### Positionnement multidimensionnel non-métrique : `biplot()`

.pull-left[

Le biplot du PMnM montre un groupe de sites fermés caractérisés par les espèces BLA, TRU, VAI, LOC, CHA et OMB, tandis que les autres espèces forment un groupe de sites dans la partie supérieure droite du graphique. 

Quatre sites dans la partie inférieure du graphique sont fortement différents des autres.
]

.pull-right[
&lt;img src="workshop09-pres-fr_files/figure-html/unnamed-chunk-88-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
# Défi #6 ![:cube]()

&lt;br&gt;

Exécutez le PMnM de l'abondance des espèces d'acariens en 2 dimensions basé sur une distance de Bray-Curtis.

Évaluez la qualité de l'ajustement de l'ordination et interprétez le biplot.

---
# Solution #6

.pull-left[
![](images/SheplSol6.png)
]

.pull-right[
La corrélation entre la dissimilarité observée et la distance d'ordination `\((R^2 &gt; 0,91)\)` et la valeur de contrainte relativement faible, montrant ensemble une bonne précision de l'ordination NMDS.
]

---
# Solution #6

.pull-left[
![](images/NMDSSol61.png)
]

.pull-right[
Aucun groupe de sites ne peut être défini précisément à partir du biplot NMDS montrant que la plupart des espèces sont présentes dans la plupart des sites, c'est-à-dire que quelques sites abritent des communautés spécifiques.
]

---
# Conclusion

.alert[De nombreuses techniques d'ordination existent, mais leur spécificité doit guider vos choix sur les méthodes à utiliser]

| | Distance préservée | Variables | Nombre maximal d'axes |
|---|---------|--------------|------|
|PCA| Euclidienne | Données quantitatives, relations linéaires | p|
|CA| Chi2 | Non négatif, données quantitatives homogènes, données binaires | p-1 |
|PCoA| Définie par l'utilisateur | Données quantitatives, semi-quantitatives, mixtes| p-1|
|NMDS| Définie par l'utilisateur | Données quantitatives, semi-quantitatives et mixtes | Définie par l'utilisateur |

---
# C'est l'heure du quiz !

.alert[Que signifie ACP?]

--

Analyse en composantes principales

--

.alert[Laquelle de ces méthodes est la meilleure pour visualiser les *distances* entre composition des communautés de différents sites?]

--

Analyse en coordonnées principales (ACoP)

--

.alert[Que représente une valeur propre dans une ACP ?]

--

La proportion de variance capturée par une composante principale

---
# C'est l'heure du quiz !

Trouvez l'erreur!

![:scale 90%](images/Chall7.png)

--
.alert[
- Données non centrées, beurk!
]

---
# C'est l'heure du quiz !

Trouvez l'erreur!

![:scale 90%](images/Chall7_2.png)

--

.alert[
- Les 2 premiers axes capturent 100% de la variation
]

---
class: inverse, center, bottom

# Merci pour votre participation à cet atelier!

![:scale 50%](images/qcbs_logo.png)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="qcbsR-macros.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": true,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
