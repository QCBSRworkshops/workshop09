<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Workshop 9: Multivariate Analyses</title>
    <meta charset="utf-8" />
    <meta name="author" content="Québec Centre for Biodiversity Science" />
    <link href="assets/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script src="assets/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="assets/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="assets/xaringanExtra-clipboard-0.2.6/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fas fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"<i class=\"fa fa-times-circle\" style=\"color: #F94144\"><\/i>"})</script>
    <link href="assets/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
    <link href="assets/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
    <link rel="stylesheet" href="qcbsR.css" type="text/css" />
    <link rel="stylesheet" href="qcbsR-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Workshop 9: Multivariate Analyses
## QCBS R Workshop Series
### Québec Centre for Biodiversity Science

---

class: inverse, center, middle





# About this workshop
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=repo&amp;message=dev&amp;color=6f42c1&amp;logo=github)](https://github.com/QCBSRworkshops/workshop09)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=wiki&amp;message=09&amp;logo=wikipedia)](https://wiki.qcbs.ca/r_workshop9)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=09&amp;color=red&amp;logo=html5)](https://qcbsrworkshops.github.io/workshop09/workshop09-en/workshop09-en.html)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=Slides&amp;message=09&amp;color=red&amp;logo=adobe-acrobat-reader)](https://qcbsrworkshops.github.io/workshop09/workshop09-en/workshop09-en.pdf)
[![badge](https://img.shields.io/static/v1?style=for-the-badge&amp;label=script&amp;message=09&amp;color=2a50b8&amp;logo=r)](https://qcbsrworkshops.github.io/workshop09/workshop09-en/workshop09-en.R)


---

&lt;p style="font-size:75%"&gt;

.center[
**Contributors to the development of this workshop**

by modifying and improving its content as part of the &lt;br&gt; *Le*arning *a*nd *D*evelopment *A*ward
]


.pull-left[
.right[

 **2022** - **2021** - **2020**



]
]

.pull-right[
.left[
**2019** - **2018** - **2017**



**2016** - **2015** - **2014**



]

]
&lt;/p&gt;

---

# Required material

To follow this workshop, you are required to have downloaded and installed the earliest [RStudio](https://rstudio.com/products/rstudio/download/#download) and [R](https://cran.rstudio.com/) versions.

.pull-left[
You must also use these packages:
* [ape](https://cran.r-project.org/package=ape)
* [gclus](https://cran.r-project.org/package=gclus)
* [vegan](https://cran.r-project.org/package=vegan)
* [GGally](https://cran.r-project.org/package=GGally)
* [PlaneGeometry](https://cran.r-project.org/package=PlaneGeometry)
* [remotes](https://cran.r-project.org/package=remotes)

]

.pull-right[


To install them from CRAN, run:


```r
install.packages(c("ape", 
                   "gclus", 
                   "vegan",
                   "GGally", 
                   "PlaneGeometry", 
                   "remotes"))
```

]

&lt;br&gt;

.pull-left2[
Throughout this workshop, there will be a series of **challenges** that you can recognize by this rubix cube.

**During these challenges, do not hesitate to collaborate!**
]

.pull-right2[

.center[
![:scale 45%](images/rubicub.png)
]
]


---
# Learning objectives

**1.** Learn the basics of multivariate analysis to reveal patterns in community composition data

**2.** Use `R` to perform an unconstrained ordination

**3.**    Choose appropriate distance metrics and transformations to perform multivariate analysis

* Clustering analysis
* Principal Component Analysis (PCA)
* Correspondence Analysis (CA)
* Principal Coordinate Analysis (PCoA)
* Non-Metric MultiDimensional Scaling (NMDS)

**4.** Use `R` to create dendrogram


---

class: inverse, center, middle

# 1. Introduction

## What is ordination?


---
# One Dimension

What if we are interested in this response for different species of algae involved in the algal bloom density?

.center[![:scale 70%](images/algalBloom.png)]


---
# Two Dimensions

.center[![:scale 70%](images/2dim.png)]

---
# Three Dimensions

.center[
![:scale 70%](images/3dim.png)]


---
# 4,5,6, or more Dimensions

.center[![:scale 70%](images/4dim.png)]

- It is not possible to draw a diagram with more than 3 dimensions

???
In ecology, several descriptors are usually observed for each object under study. In most instances, ecologists are interested in characterizing the main trends of variation of the objects with respect to all descriptors, not only a few of them. Looking at scatter plots of the objects with respect to all possible pairs of descriptors is a tedious approach, which generally does not shed much light on the problem at hand.
Even with multivariate approach, it is not possible to draw such a diagram on paper with more than three dimensions, however, even though it is a perfectly valid mathematical construct.
---
# Ordination in reduced space

.center[![:scale 70%](images/Ord1.png)]

- Ordination is the representation of objects (sites, stations, readings, etc.) as point data along one or several reference axes.

- Ordination addresses the question of how to project the scatter of ecological objects, which are located in the multidimensional space of descriptors, into a smaller number of dimensions. 

- Reduced space: a space with reduced dimensionality relative to the original dataset.

???
Ref: https://doi.org/10.1016/B978-0-444-53868-0.50009-5



---

# Ordination in reduced space


.center[![:scale 70%](images/Ord3.png)]

- For the purpose of analysis, ecologists project the multidimensional scatter from multidimensional space into a smaller number of dimensions of descriptors. 

- Axes represent a large fraction of the variability of the multidimensional data matrix.


???
Note: Replaced the image to fix the typo "objects"
Suggested script: For the purpose of analysis, ecologists usually project the multidimensional scatter diagram onto bivariate graphs whose axes are known to be of particular interest. The axes of these graphs are chosen to represent a large fraction of the variability of the multidimensional data matrix, in a space with reduced (i.e. lower) dimensionality relative to the original data set. Methods for ordination in reduced space also allow one to derive quantitative information on the quality of the projections, and study the relationships among descriptors as well as among objects.
---
exclude: true

# Methods for scientific research

.pull-right[![:scale 100%](images/SciMed.png)]

- **Questions / Hypothesis**
- **Experimental design**
- **Data Collection**
- **Transformation / Distance**
- **Analysis**
- **Redaction**
- **Communication**
---
class: inverse, center, middle
# 2. Exploring data

---
# Doubs River Fish Dataset

.pull-left[

Verneaux (1973) dataset:
- characterization of fish communities
- 27 different species
- 30 different sites
- 11 environmental variables

]

.pull.right[

![:scale 50%](images/DoubsRiver.png)

]

---
# Doubs River Fish Dataset

Load the Doubs River species data (Doubs.Spe.csv)


```r
spe &lt;- read.csv("data/doubsspe.csv", row.names = 1) # this line will vary depending on where you saved the dataset
spe &lt;-  spe[-8,] # remove site with no data
# for larger datasets:
row_sub = apply(spe, 1, function(row) any(row !=0 ))
# find rows where there are not any values that equal zero
spe &lt;- spe[row_sub,]
# only keep rows that have no zeros  
```

Load the Doubs River environmental data (Doubs.Env.csv)


```r
env &lt;- read.csv("data/doubsenv.csv", row.names = 1)
env &lt;- env[-8,] # remove site with no data
```

.alert[Proceed with caution, only execute once]

---
# Expore Doubs Dataset

Explore the content of the fish community dataset


```r
names(spe) # Names of objects
dim(spe) # dimensions
str(spe) # structure of objects
summary(spe) # summary statistics
head(spe) # first 6 rows
```


```
#   CHA TRU VAI LOC OMB BLA HOT TOX VAN CHE BAR SPI GOU BRO PER BOU PSO ROT CAR
# 1   0   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
# 2   0   5   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
# 3   0   5   5   5   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
# 4   0   4   5   5   0   0   0   0   0   1   0   0   1   2   2   0   0   0   0
# 5   0   2   3   2   0   0   0   0   5   2   0   0   2   4   4   0   0   2   0
# 6   0   3   4   5   0   0   0   0   1   2   0   0   1   1   1   0   0   0   0
#   TAN BCO PCH GRE GAR BBO ABL ANG
# 1   0   0   0   0   0   0   0   0
# 2   0   0   0   0   0   0   0   0
# 3   0   0   0   0   0   0   0   0
# 4   1   0   0   0   0   0   0   0
# 5   3   0   0   0   5   0   0   0
# 6   2   0   0   0   1   0   0   0
```

---
# Species Frequencies

Take a look at the distribution of species frequencies


```r
ab &lt;- table(unlist(spe))
barplot(ab, las = 1, col = grey(5:0/5),
        xlab = "Abundance class", ylab = "Frequency")
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /&gt;

.alert[Note the proportion of 0s]

---
# Species Frequencies

How many zeros?


```r
sum(spe == 0)
# [1] 408
```

What proportion of zeros?


```r
sum(spe == 0)/(nrow(spe)*ncol(spe))
# [1] 0.5210728
```

---
# Total Species Richness

Visualize how many species are present at each site:


```r
site.pre &lt;- rowSums(spe &gt; 0)
barplot(site.pre, main = "Species richness",
        xlab = "Sites", ylab = "Number of species",
        col = "grey ", las = 1)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-9-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# Understand your data!

####To choose the appropiate transformation and distance

- Are there any zeros?

- What do they mean?


.alert[A measured 0 (e.g 0mg/L, 0°C)]
.alert[is not the same as a 0 representing an absence observations]


---
# Before transforming your community data...

.alert[Important considerations:]

--
- relative abundances/counts/presence-absence?

--
- asymmetrical distributions?

--
- many rare species?

--
- overabundance of dominant species?

--
- double Zero problem?

---
# Transforming community data

.center[
![](images/trans1.png)]

---
# Transforming your community data

#### Examples

Transforming counts into presence - absence

```r
library(vegan)
spec.pa &lt;- decostand(spe, method = "pa")
```

Reducing the weight of rare species


```r
spec.hel &lt;- decostand(spe, method = "hellinger")
spec.chi &lt;- decostand(spe, method = "chi.square")
```

Reducing the weight of very abundant species


```r
spe.pa &lt;- decostand(spe,method = "log")
```


---
# Doubs Environmental Data


```r
names(env) # Names of objects
dim(env) # dimensions
str(env) # structure of objects
summary(env) # summary statistics
head(env) # first 6 rows
```


```r
head(env) # first 6 rows
#    das alt  pen  deb  pH dur  pho  nit  amm  oxy dbo
# 1  0.3 934 48.0 0.84 7.9  45 0.01 0.20 0.00 12.2 2.7
# 2  2.2 932  3.0 1.00 8.0  40 0.02 0.20 0.10 10.3 1.9
# 3 10.2 914  3.7 1.80 8.3  52 0.05 0.22 0.05 10.5 3.5
# 4 18.5 854  3.2 2.53 8.0  72 0.10 0.21 0.00 11.0 1.3
# 5 21.5 849  2.3 2.64 8.1  84 0.38 0.52 0.20  8.0 6.2
# 6 32.4 846  3.2 2.86 7.9  60 0.20 0.15 0.00 10.2 5.3
```

Explore colinearity by visualizing correlations between variables.


```r
library(GGally)
ggpairs(env, main="Bivariate Plots of the Environmental Data")
```


---
# Doubs Environmental Data

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-16-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# Standardization

Standardizing environmental variables is crucial as you cannot compare the effects of variables with different units:


```r
## ?decostand
env.z &lt;- decostand(env, method = "standardize")
```

&lt;Br&gt;
This centers and scales the variables to make your downstream analysis more appropriate:


```r
apply(env.z, 2, mean)
#           das           alt           pen           deb            pH 
# -7.959539e-17 -4.795165e-17  2.494600e-17 -7.323225e-17 -1.730430e-15 
#           dur           pho           nit           amm           oxy 
# -2.028505e-16  4.445790e-17  2.875893e-17  2.754434e-17 -4.038167e-16 
#           dbo 
#  9.829975e-17
apply(env.z, 2, sd)
# das alt pen deb  pH dur pho nit amm oxy dbo 
#   1   1   1   1   1   1   1   1   1   1   1
```


---
class: inverse, center, middle
# 3. Similarity / Dissimilarity


---
# Association measures

Matrix algebra is at the heart of all ordinations
$$
`\begin{bmatrix}
    a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; \dots  &amp; a_{1,n} \\
    a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; \dots  &amp; a_{2,n} \\
    a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; \dots  &amp; a_{3,n} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    a_{m,1} &amp; a_{m,2} &amp; a_{m,3} &amp; \dots  &amp; a_{m,n}
\end{bmatrix}`
$$


- Exploring various measures of distance between objects provides some understanding of the engine under the hood

???
.center[![](images/MatrixAlgebra.png)]

---
# Breaking out of 1D

.pull-left[

- As you have seen, ecological datasets can sometimes be very large matrices

- Ordinations compute the relationships between species or between sites

- We can simplify these relationships using methods of dissimilarity
]

.pull-right[

![:scale 40%](images/PCAMatrix.png)

![:scale 40%](images/distMes.png)

![:scale 40%](images/distMat.png)
]


---
# Similarity / Dissimilarity

- Useful to understand your dataset
- Appropriate measure required by some types of ordinations

.center[
Similarity: S = 1 - D
Distance: D = 1-S]



![](images/similarity.png)

---
# Community distance measures

.pull-left[
- **Euclidean**  
  `\(d_{jk} = \sqrt(\sum((x_{ij}-x_{ik})^2))\)`   
  Used in PCA and RDA.
  &lt;br&gt;&lt;br&gt;
- **Manhattan**  
  `\(d_{jk} = \sum(abs(x_{ij} - x_{ik}))\)`  
  Similar to the Euclidean, but sensitive to the double zero problem.
  &lt;br&gt;&lt;br&gt;
- **Chord**  
  Euclidean distance calculated for a row standardised matrix.
]

.pull-right[
- **Hellinger**  
  Square roots of conditional probabilities. Performs well in linear ordination.
  &lt;br&gt;&lt;br&gt;  
- **Chi-square**  
  `\(d_{jk} = \sum(\frac{(x_{ij}-x_{ik})^2}{(x_{ij}+x_{ik})} ) / 2\)`   
  Used in CA
  &lt;br&gt;&lt;br&gt;
- **Bray-Curtis** (Semimetric)  
  `\(d_{jk} = \frac{\sum(abs(x_{ij}-x_{ik}))}{(\sum (x_{ij}+x_{ik}))}\)`  
  Used in PCoA and NMDS.
]
&lt;br&gt;&lt;br&gt;
.center[     

*Where `\(x_{ij}\)` and `\(x_{ik}\)` refer to the quantity of species (column) `\(i\)` in sites (rows) `\(j\)` &amp; `\(k\)`.*]

--

.alert[Each of these will be useful in different situations]

???
Notes: Ref: 
https://sites.google.com/site/mb3gustame/reference/dissimilarity
Euclidean: A simple, symmetrical metric using the Pythagorean formula. The more variables present in a data set, the larger one may expect Euclidean distances to be. Further, double zeros result in decreased distances. This property makes the Euclidean distance unsuitable for many ecological data sets and ecologically-motivated transformations should be considered.  Principal components analysis  and redundancy analysis ordinate objects using Euclidean distances.

---
# Comparing Doubs Sites

The `vegdist()` function contains all common distances


```r
?vegdist
```
--
How different is the community composition across the 30 sites of the Doubs River?


```r
spe.db.pa &lt;- vegdist(spe, method = "bray")
spe.db &lt;- as.matrix(spe.db.pa)

#         1         2         3         4         5         6         7         9
# 1  0.0000000 0.6000000 0.6842105 0.7500000 0.8918919 0.7500000 0.6842105 1.0000000
# 2  0.6000000 0.0000000 0.1428571 0.3333333 0.6956522 0.3939394 0.1428571 0.6923077
# 3  0.6842105 0.1428571 0.0000000 0.1891892 0.6800000 0.2972973 0.1250000 0.7333333
#...
```
--
- Diagonal is zero, because sites are compared to each other.  
- Sites 3 and 7 are the most similar (*smallest distance*).  
- Sites 1 and 9 are the most different (1 = they are *completely* different).  

---
exclude: true

# Comparing Doubs Sites

.center[![](images/Doubs1.png)]

---
exclude: true

# Comparing Doubs Sites

.center[![](images/Doubs2.png)]

---
# Visualization of distance matrices





```r
# the code for the coldiss() function is in the workshop script.
coldiss(spe.db.pa)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-22-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
# Challenge #1 ![:cube]()

&lt;br&gt;

Discuss in breakout rooms:

&lt;br&gt;

.center[**How can we tell how similar objects are when we have multivariate data?**]

&lt;br&gt;

- Make a list of all your suggestions


---

# And what about ordination?


With ordination methods, we order our objects (site) according to their similarity



- The more the sites are similar, the closer they are in the ordination space (smaller distances)


- In Ecology, we usually calculate the similarity between sites according to their species composition or their environmental conditions.


---
# Schematic analysis of multivariate analysis

.center[![:scale 120%](images/Schema2.png)]

???
Note: re-added the picture, rename it as Schema 2.
---
# Clustering

- To highlight structures in the data by partitioning either objects or the descriptors

- Results are represented as dendrograms (trees)

- Not a statistical method
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-23-1.png" width="720" style="display: block; margin: auto;" /&gt;

???
.center[![:scale 80%](images/cluster1_revised.jpg)]

---
# Overview of 3 hierarchical methods

&lt;br&gt;

**1.** Single linkage agglomerative clustering ("single")

&lt;br&gt;

**2.** Complete linkage, agglomerative clustering ("complete")

&lt;br&gt;

**3.** Ward's minimum variance clustering ("ward.D2")

&lt;br&gt;

- Elements of lower ranks are nested within higher ranking clusters. For example, species groups are nested by genus, family, order, and so on.

???
In single linkage agglomerative clustering (also called nearest
neighbour sorting), the objects at the closest distances agglomerate.
which often generates long thin clusters or chains of objects. Conversely, in complete linkage agglomerative clustering, an object agglomerates to a group only when linked to the furthest element of the group, which in turn links it to all members of that group. It  will form many small separate groups, and is more appropriate to look for contrasts, discontinuities in the data.
Ward's minimum variance clustering differ from these two methods in that
it clusters objects into groups using the criterion of least squares
(similar to linear models). Its dendogram shows squared distances by default. To compare with other methods, calculate the sqaure root of the distances first. 

---
# Hierarchical methods


.pull-left[
- A distance matrix is first sorted in increasing distance order
![:scale 200%](images/Hierachic1.png)]

.pull-right[

- The two closest objects merge
- The next two closest objects/clusters merge
- and so on

![](images/singleClust2.png)]

???
Alternative programme:

```r
x &lt;- matrix(abs(rnorm(1:40)),nrow=5,byrow=TRUE)
x.hel &lt;- decostand(x,method="hellinger")
x.dhel &lt;- vegdist(x.hel, method="euclidean")
x.dhel.single &lt;- hclust(x.dhel, method="single")
x.dhel.complete &lt;- hclust(x.dhel, method="complete")
plot &lt;- par(mfrow=c(1,2),mar=c(5,2,4,2))
plot(as.dendrogram(x.dhel.single), horiz=TRUE, main="Single linkage clustering")
plot(as.dendrogram(x.dhel.complete), horiz=TRUE, main = "Complete linkage clustering")
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-24-1.png" width="360" style="display: block; margin: auto;" /&gt;
---
exclude: true

# Single linkage clustering

.pull-left[

![:scale 50%](images/singleClust1.png)

]


.pull-right[

- The two closest objects merge

- The next two closest objects/clusters merge

- and so on

![](images/singleClust2.png)

]

---
# Complete linkage clustering

.pull-left[

![:scale 50%](images/compleClust1.png)

]

.pull-right[

- The objects divided into small groups (1-2, 3-4, 5)

- Connect small groups using the largest distance between their elements 
  * (1-3=0.15, 2-4=0.35, 2-3=0.6, select 0.6 to connect group 1-2 and 3-4)


![](images/compleClust2.png)
]


---
# Comparison

Create a distance matrix from Hellinger transformed Doubs river data and compute the single linkage clustering


```r
spe.dhe1 &lt;- vegdist(spec.hel, method = "euclidean")
spe.dhe1.single &lt;- hclust(spe.dhe1, method = "single")
plot(spe.dhe1.single)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-25-1.png" width="504" style="display: block; margin: auto;" /&gt;


---
# Comparison


```r
spe.dhe1 &lt;- vegdist(spec.hel, method = "euclidean")
spe.dhe1.complete &lt;- hclust(spe.dhe1, method = "complete")
plot(spe.dhe1.single, main="Single linkage clustering", hang =-1)
plot(spe.dhe1.complete, main="Complete linkage clustering", hang=-1)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-26-1.png" width="1080" style="display: block; margin: auto;" /&gt;

.pull-left[

**Single linkage:**

Chains of objects occur (e.g. 19,29,30,26)
]

.pull-right[

**Complete linkage:**
Contrasted groups are formed of objects occur
]

???
![](images/comparison.png)

---
# Ward's minimum variance method

- Uses the criterion of least squares to cluster objects into groups
  - At each step, the pair of clusters merging is the one leading to the minimum increase in total within-group sum of squares

---
# Ward's method

Compute the Ward's minimum variance clustering and plot the dendrogram by using the square root of the distances:


```r
spe.dhel.ward &lt;- hclust(spe.dhe1, method = "ward.D2")
spe.dhel.ward$height &lt;- sqrt(spe.dhel.ward$height)
plot(spe.dhel.ward, hang = -1) # hang = -1 aligns objects at the same level
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-27-1.png" width="360" style="display: block; margin: auto;" /&gt;
- Clusters generated using this method tend to be more spherical and to contain similar number of objects
---
exclude: true
# Ward's method

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-28-1.png" width="648" style="display: block; margin: auto;" /&gt;

Clusters generated using this method tend to contain similar numbers of objects.

---
# How to choose the right method?

- Choosing the 'right' method depends on your objective
  - Do you want to highlight gradients? contrasts?
- If more than on method seems appropriate, compare dendrograms.
- Again: this is **not** an statistical method!


But, clustering allows us to:
- Determine the optimal number of interpretable clusters.
- Compute clustering statistics.
- Combine clustering to ordination to distinguish groups of sites.


---
## Now, what?

While **cluster analysis** looks for *discontinuities* in a dataset, **ordination** extracts the main trends in the form of continuous axes.

From now, we will look into four types of **unconstrained ordination methods**...

--

.right[...**wait**, what do we mean about **unconstrained ordinations**? *Anyone*?]

--
&lt;br&gt;
.center[*If no one speaks out, choose a "volunteer", presenter!*]

--

&lt;br&gt;
**Unconstrained ordinations** assess relationships within a single set of variables. *No attempt* is made to define the relationship between a set of independent variables and one or more dependent variables.

--

In other words, the interpretation of potential effects of other factors that generated observed patterns can only be made indirectly, because those factors are *not* explicitly included in the analyses.

--

Here, we will explore:

.center[
.pull-left[
**P**rincipal **C**omponent **A**nalysis

**P**rincipal **Co**ordinate **A**nalysis
]

.pull-right[
**C**orrespondence **A**nalysis

**N**on-**M**etric Multi**D**imensional **S**caling
]
]

---

### But first, let us *recap*...

We already understand the meaning of **variance** and **mean**, and how to calculate them:

.pull-left[
`$$\sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$`
]

.pull-right[
`$$\mu_x = \frac{1}{n} \sum_{i=i}^{n} x_{i}$$`
]

These are very useful to understand the *centre* and the *dispersion* of a given variable or dimension.

Nevertheless, we are often interested **in more than one dimension**, and want to measure how much of each dimension vary from the mean with *respect to each other*.

--

**Covariance** is such a measure, which can depict how **two dimensions co-vary**:

.pull-left[
`$$var_x = \sigma_x^2 = \frac{\sum_{i=1}^{n}(x_i - \mu)^2} {n}$$`
]

.pull-right[
`$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$`
]

---
### But first, let us *recap*...

Intuitively, we can measure the **covariance** between more than two variables. Let us say, between the variables `\(x\)`, `\(y\)`, and `\(z\)`:

.pull-left[
`$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$`
]

.pull-right[
`$$cov_{x,z}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(z_{i}-\bar{z})}{N-1}$$`
]


`$$cov_{z,y}=\frac{\sum_{i=1}^{N}(z_{i}-\bar{z})(y_{i}-\bar{y})}{N-1}$$`
We can represent these calculations in a **covariance matrix**:

`$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; cov_{y,y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; cov_{z,z}   
\end{array} \right]$$`

--

.center[**QUIZ TIME**

*What are the diagonals?* *And, what happens if variables are independent?*
] 

---
#### Still *recapping*...

.center[***What are the diagonals?***]

If, `\(cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}\)`, then:

`$$cov_{x,x}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(x_{i}-\bar{x})}{N} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})^2} {N} = var_x$$`
--

So, that:

`$${C(x, y, z)} = \left[ \begin{array}{ccc} 
cov_{x,x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; cov_{y,y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; cov_{z,z}   
\end{array} \right] = \left[ \begin{array}{ccc} 
var_{x} &amp; cov_{y,x} &amp; cov_{z,x} \\
cov_{x,y} &amp; var_{y} &amp; cov_{z,y} \\
cov_{x,z} &amp; cov_{y,z} &amp; var_{z}   
\end{array} \right]$$`

 &lt;br&gt;
 
.center[The covariance of a variable with itself is its *variance*!]

---

#### Still *recapping*...

.center[***What happens if the variables are independent?***]

.pull-left[

```r
x &lt;- rnorm(5000, mean = 0, sd = 1)
y &lt;- rnorm(5000, mean = 0, sd = 1)
z &lt;- rnorm(5000, mean = 0, sd = 1)

xyz &lt;- data.frame(x, y, z)

GGally::ggpairs(xyz) 
```

&lt;img src="workshop09-pres-en_files/figure-html/xyz-norm-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

```r
cov(xyz)
```


```
#          x        y       z
# x  0.99690 -0.01100 0.00774
# y -0.01100  1.00951 0.00091
# z  0.00774  0.00091 1.01783
```


If variables are perfectly independent (or uncorrelated), the covariance matrix `\(C(x, y, z)\)` is:

`$${C(x, y, z)} =  \left[ \begin{array}{ccc} 
var_{x} &amp; 0 &amp; 0 \\
0 &amp; var_{y} &amp; 0 \\
0 &amp; 0 &amp; var_{z}   
\end{array} \right]$$`

*i*.*e*. a covariance closer to `\(1\)` means that variables are *colinear*.

And, here, `\(var_{x} = var_{y} = var_{z} = 1\)`.

]

---
## Linear transformations

We are often interested in observing variables in *different ways*. 

In this process, we create a new variable, let us say `\(x_{new}\)`, by multiplying and/or adding the values of the original variable `\(x\)` by constants. For instance:

--

.pull-left[
We can transform a variable of distances measured in kilometres `\(d_{km}\)` into miles, as:
`$$d_{mi} = 0.62 \times d_{km}$$`
]


.pull-right[
We can also transform Fahrenheit degrees to Celsius degrees as: 

`$$T_{C} = 0.5556\times T_{Fahr} - 17.778$$`
]

These are examples of **linear transformations**: the transformed variables are linearly related to the original variabels and the shapes of the distribution are not changed.

--

Two types of transformations are very important for us:

.pull-left[
**Centering**, which subtracts the values of a predictor by the mean:

`$$x' = x_i - \bar{x}$$`
]

.pull-right[
**Scaling**, which divides the predictor variables by their standard deviation:

`$$x'' = \frac{x_i}{\sigma_x}$$`
]

??? 

Centering is basically a technique where the mean of independent variables is subtracted from all the values. It means all independent variables have zero mean. Scaling is similar to centering. Predictor variables are divided by their standard deviation. 

The presenter should mention here that centering brings the mean to zero and scaling brings the standard deviation to one-unit. They also should mention that variables become comparable when scaling, as their unit is lost.

---

## Eigendecomposition

**Square matrices**, such as the **covariance matrix**, can be decomposed into *Eigenvalues* and *Eigenvectors*.

For a square matrix, `\(A_{n \times n}\)`, a vector `\(v\)` is an *Eigenvector* of `\(A\)`, if there is a *scalar*, `\(\lambda\)`, for which:

.center[ 
`\(A_{n \times n} v_{n \times 1} = \lambda  v_{n \times 1}\)`, or `\(\left(\begin{matrix}a_{11}&amp;\cdots&amp;a_{1n}\\\vdots&amp;\ddots&amp;\vdots\\a_{1n}&amp;\cdots&amp;a_{nn}\\\end{matrix}\right)\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)=\lambda\left(\begin{matrix}v_1\\\vdots\\v_n\\\end{matrix}\right)\)` 
]

with the value of `\(\lambda\)` being the corresponding *Eigenvalue*.

--

In other words, the matrix `\(A\)` effectively *stretches* the Eigenvector `\(v\)` by the amount specified by the Eigenvalue (*scalar*) `\(\lambda\)`.

An *Eigenvector* is a vector whose direction remains unchanged when a **linear transformation** is applied to it.

&lt;br&gt;

.center[Wait! What do we mean by *unchanged direction*?]

---
## Eigendecomposition

.center[Wait! What do we mean by *unchanged direction*?]
&lt;br&gt;

Let us represent this with this simple example.

We can transform a square into a parallelogram using a single-axis **shear transformation**. 

--

.pull-left[
Let `\(S\)` be the square with vertices `\((0,0),\,(1,0),\,(1,1),\,(1,0)\)` that will be shear-transformed to the `\(P\)` parallelogram with vertices `\((0,0),\,(1,0),\,(1,1.57),\,(1,0.57)\)`.

We can see that after the linear transformation, the purple arrow has not changed direction, *i.e.* it is an *Eigenvector* of `\(S\)`.

On the other hand, the red arrow changed direction, and thus is *not* an *Eigenvector* of `\(S\)`.
]

.pull-right[
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-29-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
## Eigendecomposition: implications

.center[
*Keep up with the algebra torture!* 
]

### Orthogonality

A *fabulous* and *simple* property of *symmetric* matrices that we can explain here!

.pull-left[
Let us assume that `\(x\)` is an eigenvector of `\(A\)` corresponding to the eigenvalue `\(λ_1\)` and `\(y\)` an eigenvector of `\(A\)` corresponding to the eigenvalue `\(λ_2\)`, with `\(λ_1≠λ_2\)`.

`$$Ax=\lambda_1x \\
Ay=\lambda_2y$$`

Let us multiply each one by the other transposed *Eigenvector*.
`$$y^{\intercal}Ax=\lambda_1y^{\intercal}x \\ x^{\intercal}A^{\intercal}y=\lambda_2x^{\intercal}y$$`

]

--

.pull-right[
Now subtract the second equation from the first one and use the commutativity of the scalar product:

`\(y^{\intercal}Ax-x^{\intercal}A^{\intercal}y=\lambda_1y^{\intercal}x - \lambda_2x^{\intercal}y \\ 0 = (\lambda_1 - \lambda_2)y^{\intercal}x\)`

Because we know that `\(\lambda_1-\lambda_2\neq 0\)`, then 
`\(y^{\intercal}x = 0\)`, *i*.*e*., `\(\mathbf{x}\perp\mathbf{y}\)`, *i*.*e*. are **orthogonal**!

&lt;br&gt;

&lt;br&gt;
*So, what does the Eigendecomposition of a variance-covariance matrix tell us?*
]

???

Hi, Presenter. The explanation of this part is very useful and quite simple, so everyone can understand what orthogonality is. It is a matter of simple equation operations and subtractions. 

---
## Eigendecomposition: implications

.center[
*Keep up with the algebra torture!* 
]

### Maximization

.pull-left[
If `\(v_i' v_i = 1\)`, then `\(Av_i=\lambda_iv_i\)` can be written as:
`$$v_i' A v_i = \lambda_i$$`
In fact, `\(v' A v\)` is the variance of a linear combination with weights in `\(v\)`, *i*.*e*. `\(\text{Var}(v_i'\,A)=v_i'\,\text{Var}(A)\,v_i\)`.

*Hence, we can connect the dots!*

]

--

.pull-right[
Remember that the *Eigenvalues* in our *variance-covariance matrix* `\(A\)` are directly related to the variance!

Thus, to find a vector `\(v\)` that maximizes the variance, `\(v' A v\)`, all we must do is to choose the *Eigenvector* corresponding to the largest *Eigenvalue* `\(\lambda_i\)`!

So that the maximum variance is `\(\lambda_1\)`!
]

--

The *explained variance* of each *Eigenvector* obeys the order: `\(\lambda_1 &gt; \lambda_2 &gt; \dots &gt; \lambda_k\)`.

This allows us to condense a larger number of original variables into a smaller set of selected vectors with minimal loss of information (i.e. ***dimensionallity reduction***).

---
# Unconstrained ordination methods

This is a good startpoint to set us in the direction of the **unconstrained ordination methods** we will study today!

.pull-left[
They allow us to:
- Assess relationships *within* a set of variables (species or environmental variables);

- Find key components of variation among samples, sites, species;

- Reduce the number of dimensions in multivariate data while limiting substantial loss of information;

- Create new variables for use in subsequent analyses.
]

.pull-right[
Here, we will learn:

1. **P**rincipal **C**omponent **A**nalysis;

2. **P**rincipal **Co**ordinate **A**nalysis;

3. **N**on-Metric **M**ulti**d**imensional **S**caling;
]
---
# Principal Component Analysis

The Principal Component Analysis (PCA) is a  *linear*  dimensionality-reduction technique, *i.e.* it reduces strongly correlated data.

In a nutshell, the PCA *linearly* transforms the feature from the original space to a new feature space, containing **principal components** that explain most of the variance in the dataset, i.e. maximize the separation between the data.

--

The *principal component space* can be written as:

`$$Z_p = ∑_{j=1}^p ϕ_j * X_j$$`
where, 

1. `\(Z_p\)` is the principal component `\(p\)`;
2. `\(ϕ_j\)` is the loading vector comprising the `\(j\)` loadings for the `\(p\)` principal component, i.e. the coefficients of the linear combination of the original variables from which the principal components are constructed;
3. `\(X_j\)` is the normalized predictors, i.e. with means equal to zero and standard deviations equal to one.

---
# Principal Component Analysis

PCA can be computed in at least *four* different ways.

For the sake of simplicity, we will focus here on how to obtain principal components from a correlation matrix.

We will learn how to do it from "scratch" and then how to use `R` packages to compute the principal components.


---

# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;
]

.pull-right3[
In `R`, from scratch!


```r
data(varechem)

str(varechem)
# 'data.frame':	24 obs. of  14 variables:
#  $ N       : num  19.8 13.4 20.2 20.6 23.8 22.8 26.6 24.2 29.8 28.1 ...
#  $ P       : num  42.1 39.1 67.7 60.8 54.5 40.9 36.7 31 73.5 40.5 ...
#  $ K       : num  140 167 207 234 181 ...
#  $ Ca      : num  519 357 973 834 777 ...
#  $ Mg      : num  90 70.7 209.1 127.2 125.8 ...
#  $ S       : num  32.3 35.2 58.1 40.7 39.5 40.8 33.8 27.1 42.5 60.2 ...
#  $ Al      : num  39 88.1 138 15.4 24.2 ...
#  $ Fe      : num  40.9 39 35.4 4.4 3 ...
#  $ Mn      : num  58.1 52.4 32.1 132 50.1 ...
#  $ Zn      : num  4.5 5.4 16.8 10.7 6.6 9.1 7.4 5.2 9.3 9.1 ...
#  $ Mo      : num  0.3 0.3 0.8 0.2 0.3 0.4 0.3 0.3 0.3 0.5 ...
#  $ Baresoil: num  43.9 23.6 21.2 18.7 46 40.5 23 29.8 17.6 29.9 ...
#  $ Humdepth: num  2.2 2.2 2 2.9 3 3.8 2.8 2 3 2.2 ...
#  $ pH      : num  2.7 2.8 3 2.8 2.7 2.7 2.8 2.8 2.8 2.8 ...
```

]

---

# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;
]

.pull-right3[
In `R`, from scratch!


```r
data(varechem)

# Step 1 
Y &lt;- varechem[, 1:2]

head(Y)
#       N    P
# 18 19.8 42.1
# 15 13.4 39.1
# 24 20.2 67.7
# 27 20.6 60.8
# 23 23.8 54.5
# 19 22.8 40.9
```

]

---
# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;

2. Standardizing observations, as in `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)`; which is the same as centring, as in `\(y_c = [y_i - \bar{y}]\)`, and then scaling, as in `\(y_s = \frac{y_i}{\sigma_y}\)`;
]

.pull-right3[
In `R`, from scratch!


```r
data(varechem)

# Step 1 
Y &lt;- varechem[, 1:2]

# Step 2
Y_std &lt;- as.matrix(scale(Y))

head(Y_std)
#              N          P
# 18 -0.46731082 -0.1993234
# 15 -1.62503567 -0.4000407
# 24 -0.39495301  1.5134640
# 27 -0.32259521  1.0518143
# 23  0.25626722  0.6303080
# 19  0.07537271 -0.2796103

round(apply(Y_std, 2, mean))
# N P 
# 0 0
round(apply(Y_std, 2, sd))
# N P 
# 1 1
```

]

---

# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;

2. Standardizing observations, as in `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)`; which is the same as centring, as in `\(y_c = [y_i - \bar{y}]\)`, and then scaling, as in `\(y_s = \frac{y_i}{\sigma_y}\)`; 

3. Compute the variance-covariance matrix `\(R = cov(Y_{std})\)`;
]

.pull-right3[
In `R`, from scratch!


```r
data(varechem)

# Step 1 
Y &lt;- varechem[, 1:2]

# Step 2
Y_std &lt;- as.matrix(scale(Y))

# Step 3
(Y_R &lt;- cov(Y_std))
#            N          P
# N  1.0000000 -0.2511603
# P -0.2511603  1.0000000
```
]

---

# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;

2. Standardizing observations, as in `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)`; which is the same as centring, as in `\(y_c = [y_i - \bar{y}]\)`, and then scaling, as in `\(y_s = \frac{y_i}{\sigma_y}\)`; 

3. Compute the variance-covariance matrix `\(R = cov(Y_{std})\)`;

4. Perform the Eigendecomposition of the covariance matrix to obtain the matrix `\(U\)` of Eigenvectors, containing the *Principal Components*;
]

.pull-right3[
In `R`, from scratch!


```r
data(varechem)

# Step 1 
Y &lt;- varechem[, 1:2]

# Step 2
Y_std &lt;- as.matrix(scale(Y))

# Step 3
Y_R &lt;- cov(Y_std)

# Step 4 
(Eigenvalues &lt;- eigen(Y_R)$values)
# [1] 1.2511603 0.7488397

(Eigenvectors &lt;- eigen(Y_R)$vectors)
#            [,1]       [,2]
# [1,] -0.7071068 -0.7071068
# [2,]  0.7071068 -0.7071068
```
]
---
# Principal Component Analysis: step-by-step

The *Eigenvectors* here are the **Principal Components**, and as we have seen, each *Eigenvector* has its corresponding *Eigenvalue*.

.pull-left[

We can represent the distances from the observations to the first Eigenvector (`PC1`, in red).

The first principal component is drawn so that the variation of the values along its line is maximal. 

The arrows on the principal components are obtained by multiplying their *Eigenvalues* by the *Eigenvectors*.
]

.pull-right[
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-35-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
# Principal Component Analysis: step-by-step

The *Eigenvectors* here are the **Principal Components**, and as we have seen, each *Eigenvector* has its corresponding *Eigenvalue*.

.pull-left[
We can then represent the distances from the observations to the second Eigenvector (`PC2`, in orange).

The second principal component is also drawn maximizing the variance of the data.

Note how the principal components are orthogonal!
]

.pull-right[

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-36-1.png" width="360" style="display: block; margin: auto;" /&gt;

]

--
.pull-left[
*We represented the Eigenvectors, i.e. the principal components!* 

*But, what is the use of the Eigenvalues?*
]

---
# Principal Component Analysis: step-by-step

We have seen that the *Eigenvalues* represent the magnitude (the variance) in the principal components.

.pull-left[
In fact, the sum of all *Eigenvalues* is equal to the sum of variances, which are represented on the diagonal of the variance-covariance matrix.
]

.pull-right[

```r
sum(diag(cov(Y_std)))
# [1] 2
sum(eigen(cov(Y_std))$values)
# [1] 2
```
]

--

Intuitively, one can obtain the relative influence of each *Eigenvector* `\(v_{k}\)` (or `\(\text{PC}_{k}\)`)  by dividing their values by the sum of all *Eigenvalues*.

`$$\text{Explained variance of}~v_{k} = \frac{\lambda_{v_k}}{\sum^p_{i=1}{\lambda_{v}}}$$`

By doing this, we can say that the `\(\text{PC}1\)` explains 63% of the variance in the data, while `\(\text{PC}2\)` explains 37% of the variance.

--

Finally, we can proceed to the last step of our computation of principal components!

---

# Principal Component Analysis: step-by-step

.pull-left3[
1. Starting point: a matrix `\(Y\)` of `\(n\)` observations and `\(p\)` normally distributed continuous variables;

2. Standardizing observations, as in `\(Y_{std} = \frac{y_i - \bar{y}}{\sigma_y}\)`; which is the same as centring, as in `\(y_c = [y_i - \bar{y}]\)`, and then scaling, as in `\(y_s = \frac{y_i}{\sigma_y}\)`;

3. Compute the variance-covariance matrix `\(R = cov(Y_{std})\)`;

4. Perform the Eigendecomposition of the covariance matrix to obtain the matrix `\(U\)` of Eigenvectors, containing the *Principal Components*;

5. Obtain the feature space by multiplying `\(U\)` with the standardized matrix `\(Y_{std}\)`, *i*.*e*. the *score matrix* `\(F\)`.
]

.pull-right3[
In `R`, from scratch!


```r
# Step 1 
Y &lt;- varechem[, 1:2]

# Step 2
Y_std &lt;- as.matrix(scale(Y))

# Step 3
Y_R &lt;- cov(Y_std)

# Step 4 
Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

# Step 5
F_PrComps &lt;- Y_std %*% Eigenvectors
head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

---
# Principal Component Analysis: step-by-step

The score matrix, `\(F\)`, (object `F_PrComps`) allows one to *rotate* the new data space, so it is represented in relation to the principal components.

.pull-left[
.center[ 
`\(\text{N}\)` ~ `\(\text{P}\)` 
]

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-39-1.png" width="252" style="display: block; margin: auto;" /&gt;
]

.pull-right[
.center[ 
`\(\text{PC}1\)` ~ `\(\text{PC}2\)` 
]

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-40-1.png" width="252" style="display: block; margin: auto;" /&gt;
]

???
The axis labels are not being printed. I included them at the top of each plot while this is not fixed.

The presenter here should emphasize the rotation, and speak about what the scores are. This understand will be useful when the participants are going to use the PCA functions that are implemented in R.

---
# Principal Component Analysis: step-by-step

PCA can also be computed using the `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, and `ade4::dudi.pca()` functions.

.pull-left[
How our PCA from scratch compares


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
to `stats::prcomp()`?

```r
PCA_prcomp &lt;- prcomp(Y, 
                     center = TRUE, 
                     scale = TRUE)

# or PCA_prcomp &lt;- prcomp(Y_std)

head(PCA_prcomp$x)
#           PC1        PC2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
# 19  0.2510109 -0.1444178
```
]

---
# Principal Component Analysis: step-by-step

PCA can also be computed using the `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, and `ade4::dudi.pca()` functions.

.pull-left[
How our PCA from scratch compares


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
to `stats::princomp()`?

```r
PCA_princomp &lt;- princomp(Y_std)

head(PCA_princomp$scores)
#        Comp.1     Comp.2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
# 19  0.2510109 -0.1444178
```
]

---
# Principal Component Analysis: step-by-step

PCA can also be computed using the `stats::prcomp()`, `stats::princomp()`, `vegan::rda()`, and `ade4::dudi.pca()` functions.

.pull-left[
How our PCA from scratch compares


```r
data(varechem)

Y &lt;- varechem[, 1:2] 
Y_std &lt;- as.matrix(scale(Y))
Y_R &lt;- cov(Y_std)

Eigenvalues &lt;- eigen(Y_R)$values
Eigenvectors &lt;- eigen(Y_R)$vectors

F_PrComps &lt;- Y_std %*% Eigenvectors

head(F_PrComps)
#          [,1]       [,2]
# 18  0.1894957  0.4713816
# 15  0.8662023  1.4319452
# 24  1.3494546 -0.7909067
# 27  0.9718543 -0.5156358
# 23  0.2644868 -0.6269034
# 19 -0.2510109  0.1444178
```
]

.pull-right[
to `vegan::rda()`?

```r
PCA_vegan_rda &lt;- rda(Y_std)

scores(PCA_vegan_rda, 
       display = "sites", 
       scaling = 1,
       choices = seq_len(PCA_vegan_rda$CA$rank),
       const = sqrt(PCA_vegan_rda$tot.chi * (nrow(PCA_vegan_rda$CA$u) - 1)))[1:5, ]
#           PC1        PC2
# 18 -0.1894957 -0.4713816
# 15 -0.8662023 -1.4319452
# 24 -1.3494546  0.7909067
# 27 -0.9718543  0.5156358
# 23 -0.2644868  0.6269034
```

`vegan::rda()` is a bit special. It uses alternative scalings. We will not cover them here, but you can study the `vignette("decision-vegan")`.
]

???

Tell participants that the name `rda` refers to a diferent type of constrained ordination technique, but that if we run `rda()` with just one variable, it will execute a PCA.

---
# Principal Component Analysis

We have implemented PCA on a two-variables dataset, for simplicity.

Let us advance and apply it to our fish species dataset.


For this, we will use the `vegan::rda()` function on the *Hellinger-transformed* fish data and summarise the results:


```r
spe.h.pca &lt;- rda(spec.hel)

# summary(spe.h.pca)
```

---
# Principal Component Analysis

.pull-left[
The first lines of `summary.rda()` tell us about the *Total variance* and *Unconstrained variance* in our model.
]

.pull-right[


```
# [1] "Partitioning of variance:"        "              Inertia Proportion"
# [3] "Total          0.5025          1" "Unconstrained  0.5025          1"
```
]

--

.pull-left2[


```
# [1] "Importance of components:"                                                     
# [2] "                         PC1     PC2     PC3     PC4     PC5     PC6     PC7"  
# [3] "Eigenvalue            0.2580 0.06424 0.04632 0.03850 0.02197 0.01675 0.01472"  
# [4] "Proportion Explained  0.5133 0.12784 0.09218 0.07662 0.04371 0.03334 0.02930"  
# [5] "Cumulative Proportion 0.5133 0.64118 0.73337 0.80999 0.85370 0.88704 0.91634"  
# [6] "                          PC14     PC15     PC16      PC17      PC18      PC19"
# [7] "Eigenvalue            0.001835 0.001455 0.001118 0.0008309 0.0005415 0.0004755"
# [8] "Proportion Explained  0.003651 0.002895 0.002225 0.0016535 0.0010776 0.0009463"
# [9] "Cumulative Proportion 0.988888 0.991783 0.994008 0.9956612 0.9967389 0.9976852"
```

]

.pull-right2[

This is followed by the *Eigenvalues*, and their contribution to the variance.


In fact, if we sum all our *Eigenvalues*, we will obtain the amount of uncostrained variance explained by the analysis!


```r
sum(spe.h.pca$CA$eig)
# [1] 0.5025103
```

]

???

Since we have not constrained our ordination, the proportional unconstrained variance is equal to the total variance.

Take a moment to explain the proportion explained, and show that the cummulative proportion will equal to 1 at the 27th PC.

---
# Principal Component Analysis

The next information is related to the *scaling*, to the *species scores*, and to the *site scores*.


```
#  [1] "Eigenvalue            0.0003680 0.0002765 0.0002253 0.0001429 7.618e-05"
#  [2] "Proportion Explained  0.0007324 0.0005503 0.0004483 0.0002845 1.516e-04"
#  [3] "Cumulative Proportion 0.9984176 0.9989678 0.9994161 0.9997006 9.999e-01"
#  [4] "                          PC25      PC26      PC27"                     
#  [5] "Proportion Explained  9.93e-05 3.036e-05 1.814e-05"                     
#  [6] "Cumulative Proportion 1.00e+00 1.000e+00 1.000e+00"                     
#  [7] "Scaling 2 for species and site scores"                                  
#  [8] "* Species are scaled proportional to eigenvalues"                       
#  [9] "* Sites are unscaled: weighted dispersion equal on all dimensions"      
# [10] "* General scaling constant of scores:  1.93676 "                        
# [11] ""                                                                       
# [12] ""                                                                       
# [13] "Species scores"                                                         
# [14] "BCO -0.20174  0.08807 -0.067086 -0.0529106  0.0737228  0.037312"        
# [15] "PCH -0.14717  0.05829 -0.067311 -0.0458414  0.0501013  0.031605"        
# [16] "GAR -0.35245 -0.14076  0.168014  0.0185946  0.0213462 -0.129788"        
# [17] "BBO -0.24317  0.03679 -0.082731 -0.0384489  0.0939828  0.063369"        
# [18] "ABL -0.42536 -0.26155 -0.054190  0.1021959 -0.0078085  0.044540"        
# [19] "ANG -0.20631  0.11889 -0.062079 -0.0175733  0.0718743 -0.001956"        
# [20] ""                                                                       
# [21] ""                                                                       
# [22] "Site scores (weighted sums of species scores)"
```
]

---
# Principal Component Analysis

.pull-left[
*Species* refer to your descriptors (i.e. the columns in your dataset), which here are the fish species.

*Scores* refer to the position of every species along the principal components.
]

.pull-right[

```
# [1] "Cumulative Proportion 1.00e+00 1.000e+00 1.000e+00"               
# [2] "Scaling 2 for species and site scores"                            
# [3] "* Species are scaled proportional to eigenvalues"                 
# [4] "* Sites are unscaled: weighted dispersion equal on all dimensions"
# [5] "* General scaling constant of scores:  1.93676 "                  
# [6] ""                                                                 
# [7] ""                                                                 
# [8] "Species scores"
```
]

--

&lt;br&gt;

.pull-left2[

```
# [1] "PCH -0.14717  0.05829 -0.067311 -0.0458414  0.0501013  0.031605"
# [2] "GAR -0.35245 -0.14076  0.168014  0.0185946  0.0213462 -0.129788"
# [3] "BBO -0.24317  0.03679 -0.082731 -0.0384489  0.0939828  0.063369"
# [4] "ABL -0.42536 -0.26155 -0.054190  0.1021959 -0.0078085  0.044540"
# [5] "ANG -0.20631  0.11889 -0.062079 -0.0175733  0.0718743 -0.001956"
# [6] ""                                                               
# [7] ""                                                               
# [8] "Site scores (weighted sums of species scores)"
```
]

.pull-right2[
*Sites* represent the rows in your dataset, which here are the different sites along the *Doubs* river.
]

--

&lt;br&gt;

.pull-left[
This information can be obtained with the `score()` function that we used before:
]

.pull-right[


```r
scores(spe.h.pca,
       display = "species" or "sites")
```

]

---
# Principal Component Analysis: condensing data

Here, we have 27 principal components. However, we can apply algorithms to select the lowest number of principal components that still account for a large variance in the data.

--

#### Kaiser-Guttman criterion

.pull-left[
We can select the principal components that capture more variance than the average explanation of all principal components. We do this by:

1. Extracting the *Eigenvalues* associated to the principal components;

2. Subsetting the *Eigenvalues* above the mean *Eigenvalue*:


```r
ev &lt;- spe.h.pca$CA$eig
# ev[ev &gt; mean(ev)]
```
]

.pull-right[

```r
n &lt;- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue",
       lwd = 2, col = "red3" , bty = "n")
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-56-1.png" width="720" style="display: block; margin: auto;" /&gt;
]

---
# Principal Component Analysis: condensing data

Here, we have 27 principal components. However, we can apply algorithms to select the lowest number of principal components that still account for a large variance in the data.

#### Broken-stick model

.pull-left[
The broken-stick model retains components that explain more variance than would be expected by randomly dividing the variance into `\(p\)` parts.


```r
head(bstick(spe.h.pca))
#        PC1        PC2        PC3        PC4        PC5        PC6 
# 0.07242581 0.05381432 0.04450858 0.03830475 0.03365187 0.02992957
```
]

.pull-right[

```r
screeplot(spe.h.pca, 
          bstick = TRUE, type = "lines")
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-58-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

---
# Principal Component Analysis

All that is left is to discuss *scaling* and to *visualize* our results.

Let us practice and compute a PCA on the standardized environmental variables for the same dataset.


```r
env.pca &lt;- rda(env.z)
# summary(env.pca, scaling  = 2)
```

--

Determine our subset of *Eigenvalues* and their corresponding *Eigenvectors*:

.pull-left[

```r
ev &lt;- env.pca$CA$eig
```


```r
ev[ev&gt;mean(ev)]
#      PC1      PC2      PC3 
# 6.097995 2.167126 1.037603
```
]

--

.pull-right[
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-62-1.png" width="576" style="display: block; margin: auto;" /&gt;
]

---
# Principal Component Analysis: `plot()`

The information computed by the PCA can be represented with *biplots*.

We can produce a *quick and dirty* biplot of the PCA using the function `plot()` in base `R`.


```r
plot(spe.h.pca)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-63-1.png" width="360" style="display: block; margin: auto;" /&gt;


---
# Principal Component Analysis: `biplot()`

`biplot()` from `base` `R` allows for a better interpretation.

.pull-left2[

```r
biplot(spe.h.pca)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-64-1.png" width="468" style="display: block; margin: auto;" /&gt;
]

.pull-right2[

The arrows are plotted to show the directionality and angle of the descriptors in the ordination.

Descriptors at 180 degrees of each other are negatively correlated;

Descriptors at 90 degrees of each other have zero correlation;

Descriptors at 0 degrees of each other are positively correlated.
]

---
# Principal Component Analysis: *Scaling*

.small[
*Type 2 scaling* (`default`): distances among objects are not approximations of Euclidean distances; angles between descriptor (species) vectors reflect their correlations.
]

.small[
*Type 1 scaling*: attempts to preserve the Euclidean distance (in multidimensional space)
among objects (sites): the angles among descriptor (species) vector are not meaningful.
]

.pull-left[

```r
biplot(spe.h.pca, scaling = 1)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-65-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
biplot(spe.h.pca, scaling = 2)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-66-1.png" width="324" style="display: block; margin: auto;" /&gt;
]

???
2: **Best for interpreting relationships among descriptors (species)!**


1: **Best for interpreting relationships among objects (sites)!**

---
# Challenge # 3 ![:cube]()

Using everything you have learned, compute a PCA on the mite species abundance data


```r
data(mite)
```

Be ready to discuss and answer:
- What are the *most relevant* principal components, i.e. subset them?
- Which groups of sites can you identify?
- Which groups of species are related to these groups of sites?

---

# Solution #3

Compute PCA on the Hellinger-transformed species data


```r
mite.spe.hel &lt;- decostand(mite, 
                          method = "hellinger")

mite.spe.h.pca &lt;- rda(mite.spe.hel)
```

--

.pull-left[
Apply the Kaiser-Guttman criterion


```r
ev &lt;- mite.spe.h.pca$CA$eig
ev[ev&gt;mean(ev)]
n &lt;- length(ev)
barplot(ev, main = "Eigenvalues", 
        col = "grey", las = 2)
abline(h = mean(ev),
       col = "red3", lwd = 2)
legend("topright", 
       "Average eigenvalue", 
       lwd = 2, 
       col = "red3", bty = "n")
```
]

.pull-right[
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-70-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
# Solution #3


```r
biplot(mite.spe.h.pca, 
       col = c("red3", "grey15"))
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-71-1.png" width="504" style="display: block; margin: auto;" /&gt;

---
# Principal Coordinates Analysis

The **PCoA** is similar in spirit to PCA, but it takes *dissimilarities* as input data! 

It was previously called as *Classical Multidimensional Scaling* (MDS) and it aims at faithfully representing distances with the lowest possible dimensional space.

It begins with the (i) computation of a distance matrix for the `\(p\)` elements, then (ii) the centering of the matrix by rows and columns, and finally, the (iii) *Eigendecomposition* of the centered distance matrix.

--

To compute a PCoA, we can use the `cmdscale()` or the `pcoa()`functions from the `stats` and `ape` packages:

Run a PCoA on the Hellinger distance-transformed fish dataset:


```r
library(ape)
spe.h.pcoa &lt;- pcoa(dist(spec.hel))
summary(spe.h.pcoa)
#            Length Class      Mode     
# correction   2    -none-     character
# note         1    -none-     character
# values       5    data.frame list     
# vectors    783    -none-     numeric  
# trace        1    -none-     numeric
```

---
# Principal Coordinates Analysis


```r
head(spe.h.pcoa$values)
#   Eigenvalues Relative_eig Broken_stick Cumul_eig Cumul_br_stick
# 1   7.2228939   0.51334374   0.14412803 0.5133437      0.1441280
# 2   1.7987449   0.12783995   0.10709099 0.6411837      0.2512190
# 3   1.2970423   0.09218307   0.08857247 0.7333668      0.3397915
# 4   1.0780684   0.07662021   0.07622679 0.8099870      0.4160183
# 5   0.6150273   0.04371107   0.06696753 0.8536980      0.4829858
# 6   0.4691296   0.03334186   0.05956013 0.8870399      0.5425459
```

---
# Principal Coordinates Analysis

We can also see the *Eigenvectors* associated to each *Eigenvalue* containing the coordinates in the Euclidean space for each site.



```r
head(spe.h.pcoa$vectors)[, 1:5]
#         Axis.1      Axis.2      Axis.3     Axis.4       Axis.5
# 1 -0.509824403 -0.27654372  0.64011383 -0.3393734  0.207330880
# 2 -0.698794880 -0.03935586  0.11324989 -0.2328859 -0.157730682
# 3 -0.640690642  0.01566707  0.03835044 -0.2669706 -0.125293094
# 4 -0.413985947  0.10477084 -0.15728486 -0.2851828 -0.001250382
# 5  0.003083242  0.05284310 -0.32206098 -0.2730693  0.315944703
# 6 -0.295314224  0.05778805 -0.32395301 -0.2262902  0.056493824
```

---
# Principal Coordinates Analysis: `biplot.pcoa()`

We can display, the distances between sites using the `biplot.pcoa()` function, as well as represent the species associated to each site.


```r
biplot.pcoa(spe.h.pcoa, spec.hel)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-75-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
### Principal Coordinates Analysis: non-metric distances

PCoA can also be used to capture information contained in non-metric distances, such as the popular Bray-Curtis distance. Let us give it a try:

.pull-left[

```r
spe.bray.pcoa &lt;- pcoa(spe.db.pa)
```


```r
spe.bray.pcoa$values$Eigenvalues
#  [1]  3.695331e+00  1.098472e+00  7.104740e-01  4.149729e-01  3.045604e-01
#  [6]  1.917884e-01  1.569703e-01  1.319099e-01  1.294251e-01  8.667896e-02
# [11]  4.615780e-02  3.864487e-02  2.745800e-02  1.306508e-02  7.087896e-03
# [16]  4.039469e-03  1.300594e-03  0.000000e+00 -3.534426e-05 -3.940676e-03
# [21] -8.956051e-03 -1.461149e-02 -1.598905e-02 -2.145686e-02 -3.017013e-02
# [26] -3.432671e-02 -3.760052e-02 -6.037087e-02 -6.880061e-02
```
]

.pull-right[
Note the negative eigenvalues! 

This is because non-metric distances cannot be represented in Euclidean space without corrections (*see* Legendre &amp; Legendre 2012 for more details on this):


```r
spe.bray.pcoa &lt;- pcoa(spe.db.pa, 
                      correction = "cailliez")
```
]

---
### Principal Coordinates Analysis: non-metric distances

.pull-left[
The corrected Eigenvalues are now on a new column!


```r
spe.bray.pcoa$values$Corr_eig
#  [1] 5.20461437 1.60465006 1.09152082 0.68985417 0.52129425 0.38710929
#  [7] 0.36447367 0.29671255 0.27559544 0.21600584 0.15419396 0.15378333
# [13] 0.11812808 0.08848541 0.07304055 0.06999353 0.05712927 0.05587583
# [19] 0.05432215 0.04912221 0.04100207 0.03777775 0.03451234 0.02959507
# [25] 0.02436729 0.01902747 0.00284371 0.00000000 0.00000000
```

]

.pull-right[
Use a biplot without the species to represent it!


```r
biplot.pcoa(spe.bray.pcoa)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-80-1.png" width="432" style="display: block; margin: auto;" /&gt;

]

---
# Challenge #5 ![:cube]()

Compute a PCoA on the Hellinger-transformed mite species abundance data

Be ready to answer:

- What are the significant *Eigenvectors* and *Eigenvalues*?
- Which groups of sites can you identify?
- Which groups of species are related to these groups of sites
- How do the PCoA results compare with the PCA results?

---
# Solution #5

- Hellinger transform the species data

```r
mite.spe &lt;- mite
mite.spe.hel &lt;- decostand(mite.spe, method = "hellinger")
```

- Compute PCoA


```r
mite.spe.h.pcoa &lt;- pcoa(dist(mite.spe.hel))
```

---
# Solution #5

- Build a biplot to visualize the data:

```r
biplot.pcoa(mite.spe.h.pcoa, mite.spe.hel)
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-83-1.png" width="432" style="display: block; margin: auto;" /&gt;


---
# Non-metric Multidimensional Scaling

- In PCA and PCoA, objects are ordinated in a few number of dimensions (generally &gt; 2);

- 2D-biplots may not represent all the variation within the dataset;

- Sometimes, we aim at representing the data in a specified smaller number of dimensions;

- How can we plot the ordination space to represent the most variation as possible in the data?

--

We can attempt using *non-metric multidimensional scaling*!

* NMDS is the non-metric counterpart of PCoA;
* It uses an iterative optimization algorithm to find the best representation of distances in reduced space;

---
# Non-metric Multidimensional Scaling

- NMDS applies an iterative procedure that tries to position the objects in the requested number of dimensions in such a way as to minimize a stress function (scaled from 0 to 1), which measures the goodness-of-fit of the distance adjustment in the reduced-space configuration.

- Consequently, the lower the stress value, the better the representation of objects in the ordination-space is.

-  nMDS is implemented in `vegan` as `metaMDS()` where:
  - `distance` specifies the distance metric to use;
  - `k` specifies the number of dimensions.


```r
spe.nmds &lt;- metaMDS(spe, distance = 'bray', k = 2)
```



---
#### Non-metric Multidimensional Scaling: *goodness-of-fit*

The *Shepard* diagram and stress values can be obtained from `stressplot()`:


```r
spe.nmds$stress
# [1] 0.07376216
stressplot(spe.nmds, main = "Shepard plot")
```

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-86-1.png" width="360" style="display: block; margin: auto;" /&gt;

The Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance (R2 &gt; 0.95) highlighting a high goodness of fit of the NMDS.

---
# Non-metric Multidimensional Scaling: `biplot()`

Construct the biplot


```r
plot(spe.nmds, type = "none",
     main = paste("NMDS/Bray - Stress =",
                  round(spe.nmds$stress, 3)),
     xlab = c("NMDS1"), ylab = "NMDS2")

points(scores(spe.nmds, display = "sites",
              choiches = c(1,2),
              pch = 21,
              col = "black",
              g = "steelblue",
              cex = 1.2))
text(scores(spe.nmds, display = "species", choices = c(1)),
            scores(spe.nmds, display = "species", choices = c(2)),
            labels = rownames(scores(spe.nmds, display = "species")),
            col = "red", cex = 0.8)
```

---
# Non-metric Multidimensional Scaling: `biplot()`

.pull-left[

The biplot of the NMDS shows a group of closed sites characterized by the species BLA, TRU, VAI, LOC, CHA and OMB,
while the other species form a cluster of sites in the upper right part of the graph. 

Four sites in the lower part of the graph are strongly different from the others.
]

.pull-right[
&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-88-1.png" width="360" style="display: block; margin: auto;" /&gt;
]

---
# Challenge #6 ![:cube]()

&lt;br&gt;

Run the NMDS of the mite species abundances in 2 dimensions based on a Bray-Curtis distance.

Assess the goodness-of-fit of the ordination and interprete the biplot.

---
# Solution #6

.pull-left[
![](images/SheplSol6.png)
]

.pull-right[
The correlation between observed dissimilarity and ordination distance (R2 &gt; 0.91) and the stress value relatively low, showing together a good accuracy of the NMDS ordination
]

---
# Solution #6

.pull-left[
![](images/NMDSSol61.png)
]

.pull-right[
No cluster of sites can be precisely defined from the NMDS biplot showing that most of the species occurred in most of the sites, i.e. a few sites shelter specific communities
]

---
# Conclusion

.alert[Many ordination techniques exist, but their specificity should guide your choices on which methods to use]

|   | Distance preserved | Variables | Maximum number of axis |
|---|---------|--------------|------|
|PCA| Euclidean | Quantitative data, linear relationships | p |
|CA| Chi2 | Non-negative, quantitative homogeneous data, binary data | p-1 |
|PCoA| User defined | Quantitative, semi-quantitative, mixed data| p-1|
|NMDS| User defined | Quantitative, semi-quantitative, mixed data| User defined|

---
# Prime time 4 quiz time

.alert[What does PCA stand for?]

--

Principal Component Analysis

--

.alert[Which one is the best way to visualize the *distances* between the community composition of many sites?]

--

Principal Coordinate Analysis (PCoA)

--

.alert[What does an eigenvalue represent in PCA?]

--

The proportion of variance explained by a principal component

---
# Prime time 4 quiz time

Spot what is sketchy

![:scale 90%](images/Chall7.png)

--
.alert[
- Data non centered, Yikes!
]

---
# Prime time 4 quiz time

Spot what is sketchy

![:scale 90%](images/Chall7_2.png)

--

.alert[
- 2 first PCs explain 100% of the variation!
]

---
class: inverse, center, bottom

# Thank you for attending this workshop!

![:scale 50%](images/qcbs_logo.png)


---

### Principal Component Analysis: *Improved visualization*

We can build more detailed and aesthetic plots:


```r
plot(spe.h.pca, scaling  = 1, 
     type = "none",
     xlab = c("PC1 (%)", 
              round(spe.h.pca$CA$eig[1]/sum(spe.h.pca$CAeig)*100,2)),
     ylab = c("PC2 (%)", round(spe.h.pca$CA$eig[2]/sum(spe.h.pca$CA$eig)*100,2)))
points(scores(spe.h.pca, display = "sites", choices = c(1,2), scaling = 1),
       pch=21, col = "black", bg = "steelblue" , cex  = 1.2)
text(scores(spe.h.pca, display = "species", choices = 1, scaling = 1),
     scores(spe.h.pca, display = "species", choices = 2, scaling = 1),
     labels = rownames(scores(spe.h.pca, display = "species", scaling = 1)),
     col = "red", cex = 0.8)
spe.cs &lt;- scores(spe.h.pca, choices = 1:2, scaling = 1 , display = "sp")
arrows(0, 0, spe.cs[,1], spe.cs[,2], length = 0)
```

---

### Principal Component Analysis: *Improved visualization*

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-90-1.png" width="504" style="display: block; margin: auto;" /&gt;


---
# Correspondence Analysis (CA)

## Euclidean vs Chi&lt;sup&gt;2&lt;/sup&gt; distances

- PCA preserves **euclidean distances** between objects, and thus postulates **linear relationships** between species, and between species and environmental gradients.

- ... but in **some cases, species instead present unimodal responses** to environmental gradients

---

# Principles of CA

- In such cases, CA should be preferred compared to PCA as it preserves **Chi2 distances between sites**... and thus better represents uni modal relationships

---
# How to run a CA?

- CA is implemented in the `vegan` package using the function `cca()`:


```r
spe.ca &lt;- cca(spe[-8,])
# only take columns which rowsums are &gt; than 0.
```


- CA on fish species abundances

---
# CA: R output

- CA results are presented in the same way as PCA results and can be called using:


```r
summary(spe.ca)
# 
# Call:
# cca(X = spe[-8, ]) 
# 
# Partitioning of scaled Chi-square:
#               Inertia Proportion
# Total           1.128          1
# Unconstrained   1.128          1
# 
# Eigenvalues, and their contribution to the scaled Chi-square 
# 
# Importance of components:
#                          CA1    CA2     CA3     CA4     CA5     CA6     CA7
# Eigenvalue            0.6062 0.1423 0.10251 0.07319 0.04912 0.03909 0.03341
# Proportion Explained  0.5374 0.1262 0.09087 0.06488 0.04354 0.03465 0.02962
# Cumulative Proportion 0.5374 0.6635 0.75437 0.81925 0.86279 0.89745 0.92706
#                           CA8     CA9     CA10     CA11     CA12     CA13
# Eigenvalue            0.01709 0.01302 0.010765 0.008141 0.007533 0.005820
# Proportion Explained  0.01515 0.01154 0.009543 0.007217 0.006678 0.005159
# Cumulative Proportion 0.94221 0.95375 0.963294 0.970511 0.977188 0.982347
#                           CA14     CA15     CA16     CA17     CA18     CA19
# Eigenvalue            0.004765 0.004444 0.003262 0.002268 0.001663 0.001376
# Proportion Explained  0.004224 0.003939 0.002892 0.002011 0.001474 0.001219
# Cumulative Proportion 0.986571 0.990510 0.993402 0.995413 0.996887 0.998106
#                            CA20      CA21      CA22      CA23      CA24
# Eigenvalue            0.0010054 0.0004515 0.0002976 0.0001528 0.0001455
# Proportion Explained  0.0008913 0.0004003 0.0002638 0.0001355 0.0001290
# Cumulative Proportion 0.9989973 0.9993976 0.9996614 0.9997969 0.9999258
#                            CA25      CA26
# Eigenvalue            5.671e-05 2.694e-05
# Proportion Explained  5.027e-05 2.388e-05
# Cumulative Proportion 1.000e+00 1.000e+00
# 
# Scaling 2 for species and site scores
# * Species are scaled proportional to eigenvalues
# * Sites are unscaled: weighted dispersion equal on all dimensions
# 
# 
# Species scores
# 
#          CA1       CA2       CA3       CA4      CA5      CA6
# CHA  1.49168  1.385259 -0.433809  0.071078 -0.24616  0.05168
# TRU  1.65415 -0.516649 -0.440526 -0.159605  0.21034 -0.49677
# VAI  1.29689 -0.287854  0.081677  0.039670 -0.04216  0.33505
# LOC  1.01655 -0.318047  0.345002  0.064349  0.07146  0.25276
# OMB  1.54572  1.331279 -1.009750  0.022603 -0.36894  0.38992
# BLA  0.99242  1.488077  0.395856 -0.002786  0.03087 -0.42919
# HOT -0.54197  0.037905 -0.007131  0.189457  0.34781  0.12443
# TOX -0.17940  0.440408  0.666475 -0.157195  0.59702 -0.09090
# VAN  0.01649  0.115139  0.632750  0.027180 -0.28562 -0.25295
# CHE -0.01016 -0.003287  0.087052  0.360355 -0.09299  0.04720
# BAR -0.32772  0.280531  0.087903 -0.134099  0.20162  0.02136
# SPI -0.37673  0.235208  0.296247 -0.251569  0.46512  0.10939
# GOU -0.31588  0.025703  0.089443  0.122259 -0.11758 -0.02693
# BRO -0.25636 -0.209762  0.076551 -0.128801 -0.37914 -0.08203
# PER -0.28362 -0.136097  0.304122 -0.263493 -0.25323 -0.04680
# BOU -0.59482  0.018329 -0.133101 -0.259428  0.15559  0.13556
# PSO -0.57898  0.049761 -0.180659 -0.080819  0.16908  0.02711
# ROT -0.61086 -0.155369 -0.062639 -0.139514 -0.40987 -0.16073
# CAR -0.57134  0.067623 -0.114447 -0.368663  0.11556  0.03287
# TAN -0.38717 -0.126068  0.088482 -0.142953 -0.22493 -0.01394
# BCO -0.69346 -0.064062 -0.326351 -0.337945 -0.02336  0.07199
# PCH -0.72297 -0.058647 -0.458970 -0.536212 -0.09385  0.05842
# GRE -0.68487 -0.073615 -0.386296  0.125350  0.02339  0.01566
# GAR -0.48338 -0.092189  0.075212  0.244705 -0.12215 -0.09479
# BBO -0.70069 -0.077202 -0.373887 -0.141038  0.02316  0.06521
# ABL -0.62469 -0.051667 -0.268265  0.765808  0.17809 -0.07415
# ANG -0.62735 -0.004040 -0.227844 -0.315380  0.09477  0.11331
# 
# 
# Site scores (weighted averages of species scores)
# 
#         CA1      CA2       CA3       CA4      CA5       CA6
# 1   2.72880 -3.63047 -4.297575 -2.180558  4.28251 -12.70718
# 2   2.26939 -2.74567 -0.683635 -0.508114  1.86199  -0.82146
# 3   2.01894 -2.55716  0.004459 -0.347298  1.04225   0.59661
# 4   1.28757 -1.97139  0.649261 -0.366438 -0.69066   0.85344
# 5   0.08260 -0.91883  1.609565 -0.009607 -3.62098  -1.54952
# 6   1.03028 -1.62206  1.049551  0.285131 -0.94420   0.82432
# 7   1.91231 -2.28949  0.346864  0.059722  1.09657  -0.13682
# 10  1.24958 -1.35061  1.947791  1.125968 -0.79586   2.58731
# 11  2.12864 -0.19558 -1.779393  0.246236 -0.32251   1.37538
# 12  2.16190  0.16511 -1.786342  0.125825 -0.17453   1.13375
# 13  2.29162  1.87949 -2.161318 -0.191687 -0.59319   0.15032
# 14  1.85861  2.19432 -1.472445  0.084282 -1.00562   0.53827
# 15  1.33811  1.87052  0.341352  0.449315 -0.93504  -0.62573
# 16  0.70115  1.57032  2.113072 -0.238157  0.62925  -1.83993
# 17  0.29051  0.86531  1.368926 -0.167800  1.64807   0.17059
# 18  0.05651  0.68479  0.965786 -0.005014  1.08092   0.60339
# 19 -0.19970  0.05475  1.064683  0.458458  0.92614   0.60861
# 20 -0.57263 -0.02434  0.359285  0.408580  0.48971   0.14790
# 21 -0.66495 -0.07911 -0.028650 -0.080409  0.05745   0.03087
# 22 -0.71075 -0.04983 -0.163794 -0.180754 -0.05923  -0.05610
# 23 -0.71881 -0.34926 -0.912791  7.297949  0.71786  -1.25267
# 24 -0.83779 -0.26250 -1.470892  4.899937  1.26149  -0.41743
# 25 -0.68011 -0.33390 -0.180281  3.764841 -1.79032  -1.86951
# 26 -0.75038 -0.23767 -0.702769  0.601903 -0.13132   0.06362
# 27 -0.74233 -0.14481 -0.559979 -0.081360 -0.19911   0.05064
# 28 -0.76945 -0.14800 -0.716897 -0.406389 -0.11339   0.18430
# 29 -0.59922  0.12766 -0.437857 -0.646721  0.08057   0.07932
# 30 -0.79734 -0.08325 -0.436074 -1.168606 -0.21617   0.01955
```


---
# CA: Interpretation of results

.pull-left2[
![](images/CAInt.png)
]

.pull-right2[

26 CA axes identified

% CA1 = 51.50%

% CA2 = 12.37%
]


---
# CA: biplots

.center[
![:scale 75%](images/CABiplto.png)]

.small[
The group of sites on the left is characterized by the species *GAR*, *TAN*, *PER*, *ROT*, *PSO*, and *CAR*

The group of sites in the upper right corner is characterized by the species *LOC*, *VAI* and *TRU*
The group of sites in the lower right corner is characterized by the species *BLA*, *CHA*, and *OMB*
]


---
# Challenge #4 ![:cube]()

Using everything you have learned to execute a CA on the mite species abundance data:


```r
mite.spe &lt;- mite
```

- What are the significant axes?
- Which groups of sites can you identify?
- Which groups of species are related to these groups of sites?

---
# Solution #4

- Compute CA:


```r
mite.spe.ca &lt;- cca(mite.spe)
```

- Check significant axes using the Guttman-Kaiser criterion


```r
ev &lt;- mite.spe.ca$CA$eig
ev[ev &gt; mean(ev)]
n &lt;- length(ev)
barplot(ev, main = "Eigenvalues", col = "grey", las = 2)
abline(h = mean(ev), col = "red3", lwd = 2)
legend("topright", "Average eigenvalue", lwd = 2, col = red3, bty = "n")
```

---
# Solution #4

&lt;img src="workshop09-pres-en_files/figure-html/unnamed-chunk-96-1.png" width="720" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="qcbsR-macros.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": true,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
